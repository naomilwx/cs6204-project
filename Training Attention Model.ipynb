{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from models.attention.model import LabelImageAttention, LabelImagePrototypeModel\n",
    "from models.attention.trainer import Trainer\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "\n",
    "from utils.data import get_query_and_support_ids, DatasetConfig\n",
    "from utils.device import get_device\n",
    "from utils.data import get_query_and_support_ids\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT, VINDR_SPLIT2\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "from models.embedding.dataset import Dataset\n",
    "from utils.sampling import MultilabelBalancedRandomSampler\n",
    "\n",
    "\n",
    "configs = {\n",
    "    'vindr1': DatasetConfig('datasets/vindr-cxr-png', 'data/vindr_cxr_split_labels.pkl', 'data/vindr_train_query_set.pkl', VINDR_CXR_LABELS, VINDR_SPLIT, MEAN_STDS['chestmnist']),\n",
    "    'vindr2': DatasetConfig('datasets/vindr-cxr-png', 'data/vindr_cxr_split_labels2.pkl', 'data/vindr_train_query_set2.pkl', VINDR_CXR_LABELS, VINDR_SPLIT2, MEAN_STDS['chestmnist'])\n",
    "}\n",
    "\n",
    "config = configs['vindr2']\n",
    "\n",
    "IMG_PATH = 'datasets/vindr-cxr-png'\n",
    "batch_size = 10*14\n",
    "\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(config.img_info, config.training_split_path)\n",
    "query_dataset = Dataset(config.img_path, config.img_info, query_image_ids, config.label_names_map, config.classes_split_map['train'], mean_std=config.mean_std)\n",
    "query_loader = DataLoader(dataset=query_dataset, batch_size=batch_size, shuffle=True)\n",
    "support_dataset = Dataset(config.img_path, config.img_info, support_image_ids, config.label_names_map, config.classes_split_map['train'], mean_std=config.mean_std)\n",
    "support_loader = DataLoader(dataset=support_dataset, batch_size=batch_size, sampler=MultilabelBalancedRandomSampler(support_dataset.get_class_indicators()))\n",
    "\n",
    "\n",
    "PROJ_SIZE = 512\n",
    "device =  get_device()\n",
    "\n",
    "encoder = torch.load('models/embedding/model/vindr2/imgtext_model_trained1.pth')\n",
    "encoder.text_model.device = device\n",
    "model = LabelImagePrototypeModel(encoder, 8, PROJ_SIZE, num_layers=4, cls_weight=0.1)\n",
    "mtrainer = Trainer(model, support_dataset.class_labels(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naomileow/Documents/school/CS6240/project/models/attention/model.py:52: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  classes = torch.nonzero(label_inds)[:,1] # (Np,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 72.8689956665039\n",
      "Batch 2: loss 72.80627059936523\n",
      "Batch 3: loss 71.58649190266927\n",
      "Batch 4: loss 71.17413139343262\n",
      "Batch 5: loss 70.92964019775391\n",
      "Batch 6: loss 70.54846700032552\n",
      "Batch 7: loss 70.33934020996094\n",
      "Batch 8: loss 70.03028297424316\n",
      "Batch 9: loss 69.64961496988933\n",
      "Batch 10: loss 69.72439651489258\n",
      "Batch 11: loss 69.76450070467862\n",
      "Batch 12: loss 69.60139274597168\n",
      "Batch 13: loss 69.37254450871394\n",
      "Batch 14: loss 69.36271340506417\n",
      "Batch 15: loss 69.18506266276042\n",
      "Batch 16: loss 69.04267692565918\n",
      "Batch 17: loss 69.0490789974437\n",
      "Batch 18: loss 69.01732677883572\n",
      "Batch 19: loss 68.91317347476357\n",
      "Batch 20: loss 68.88681983947754\n",
      "Batch 21: loss 68.79013897123791\n",
      "Batch 22: loss 68.6995058926669\n",
      "Batch 23: loss 68.62465236497962\n",
      "Batch 24: loss 68.50263595581055\n",
      "Batch 25: loss 68.34710876464844\n",
      "Batch 26: loss 68.30223054152269\n",
      "Batch 27: loss 68.21710261592159\n",
      "Batch 28: loss 68.03733021872384\n",
      "Batch 29: loss 67.97193237830852\n",
      "Batch 30: loss 67.89395001729329\n",
      "Batch 31: loss 67.86394242317446\n",
      "Batch 32: loss 67.8587611913681\n",
      "Batch 33: loss 67.82326750321822\n",
      "Batch 34: loss 67.73711384043975\n",
      "Batch 35: loss 67.72804685320173\n",
      "Batch 36: loss 67.67746427324083\n",
      "Batch 37: loss 67.72636155824404\n",
      "Batch 38: loss 67.6637652547736\n",
      "Batch 39: loss 67.61318862132536\n",
      "Batch 40: loss 67.54040517807007\n",
      "Batch 41: loss 67.53454450281654\n",
      "Batch 42: loss 67.51598730541411\n",
      "Batch 43: loss 67.50214607771053\n",
      "Batch 44: loss 67.4781233180653\n",
      "Batch 45: loss 67.49337861802843\n",
      "Batch 46: loss 67.42406355816385\n",
      "Batch 47: loss 67.40794770261074\n",
      "Batch 48: loss 67.41893617312114\n",
      "Batch 49: loss 67.34637186478595\n",
      "Batch 50: loss 67.3685385131836\n",
      "Batch 51: loss 67.32130940755208\n",
      "Batch 52: loss 67.2854668543889\n",
      "Batch 53: loss 67.26299357864092\n",
      "Batch 54: loss 67.2113292835377\n",
      "Batch 55: loss 67.20572787198154\n",
      "Batch 56: loss 67.1991525377546\n",
      "Batch 57: loss 67.19221469812226\n",
      "Batch 58: loss 67.15023961560479\n",
      "Batch 59: loss 67.12291290800451\n",
      "Batch 60: loss 67.05484778086344\n",
      "Batch 61: loss 67.07459402866051\n",
      "Batch 62: loss 67.04739496784825\n",
      "Batch 63: loss 67.01050791664728\n",
      "Batch 64: loss 67.00394147634506\n",
      "Batch 65: loss 66.98506217369666\n",
      "Batch 66: loss 66.96353074276087\n",
      "Batch 67: loss 66.91356903759402\n",
      "Batch 68: loss 66.87528279248406\n",
      "Batch 69: loss 66.82617292542389\n",
      "Batch 70: loss 66.8221393585205\n",
      "Batch 71: loss 66.75871325210787\n",
      "Batch 72: loss 66.7296462059021\n",
      "Batch 73: loss 66.70131834892378\n",
      "Batch 74: loss 66.68863105773926\n",
      "Batch 75: loss 66.6666022237142\n",
      "Batch 76: loss 66.66152908927516\n",
      "Batch 77: loss 66.63156885914988\n",
      "Batch 78: loss 66.6355027907934\n",
      "Batch 79: loss 66.61326323883443\n",
      "Batch 80: loss 66.59881615638733\n",
      "Batch 81: loss 66.57474513112763\n",
      "Batch 82: loss 66.57007538400045\n",
      "Batch 83: loss 66.54773873018931\n",
      "Batch 84: loss 66.5200648080735\n",
      "Batch 85: loss 66.48447153428022\n",
      "Batch 86: loss 66.46824570589287\n",
      "Batch 87: loss 66.45276462072614\n",
      "Batch 88: loss 66.4471906748685\n",
      "Batch 89: loss 66.43191421165895\n",
      "Batch 90: loss 66.4428175608317\n",
      "Batch 91: loss 66.44321739280616\n",
      "Batch 92: loss 66.44104994898257\n",
      "Batch 93: loss 66.42884313932029\n",
      "Batch 94: loss 66.390610431103\n",
      "Batch 95: loss 66.37831513254267\n",
      "Batch 96: loss 66.36714375019073\n",
      "Batch 97: loss 66.35656930982452\n",
      "Batch 98: loss 66.33620199865224\n",
      "Batch 99: loss 66.30599906227805\n",
      "Batch 100: loss 66.28770027160644\n",
      "Batch 101: loss 66.28126306817083\n",
      "Batch 102: loss 66.27475880641563\n",
      "Batch 103: loss 66.27637029851525\n",
      "Batch 104: loss 66.10885937828742\n",
      "Epoch 1: Training loss 66.10885937828742\n",
      "Epoch 1: Validation loss 15.42885066986084 | Accuracy 0.6152000000000001 | AUC 0.7109649083073296\n",
      "Batch 1: loss 67.07105255126953\n",
      "Batch 2: loss 65.86874008178711\n",
      "Batch 3: loss 65.51134236653645\n",
      "Batch 4: loss 65.77028274536133\n",
      "Batch 5: loss 65.55352630615235\n",
      "Batch 6: loss 65.41110356648763\n",
      "Batch 7: loss 65.0268690926688\n",
      "Batch 8: loss 65.25006151199341\n",
      "Batch 9: loss 65.29086176554362\n",
      "Batch 10: loss 65.40528755187988\n",
      "Batch 11: loss 65.13898225264116\n",
      "Batch 12: loss 65.09277820587158\n",
      "Batch 13: loss 65.18101530808669\n",
      "Batch 14: loss 65.18344797406878\n",
      "Batch 15: loss 65.22295405069987\n",
      "Batch 16: loss 65.24748969078064\n",
      "Batch 17: loss 65.28918165319106\n",
      "Batch 18: loss 65.17306073506673\n",
      "Batch 19: loss 65.03238878752056\n",
      "Batch 20: loss 64.9917766571045\n",
      "Batch 21: loss 65.00930822463263\n",
      "Batch 22: loss 65.01858763261275\n",
      "Batch 23: loss 65.10695183795431\n",
      "Batch 24: loss 65.15536117553711\n",
      "Batch 25: loss 65.12233306884765\n",
      "Batch 26: loss 65.10259041419395\n",
      "Batch 27: loss 65.13475658275463\n",
      "Batch 28: loss 65.0525940486363\n",
      "Batch 29: loss 65.1014306956324\n",
      "Batch 30: loss 65.0742909749349\n",
      "Batch 31: loss 65.1278821883663\n",
      "Batch 32: loss 65.09440636634827\n",
      "Batch 33: loss 65.01498054735589\n",
      "Batch 34: loss 64.98350760515999\n",
      "Batch 35: loss 64.97225548880441\n",
      "Batch 36: loss 64.97026348114014\n",
      "Batch 37: loss 64.98277313644822\n",
      "Batch 38: loss 65.03132257963482\n",
      "Batch 39: loss 65.0137095329089\n",
      "Batch 40: loss 64.92576141357422\n",
      "Batch 41: loss 64.90960767792492\n",
      "Batch 42: loss 64.93395287649972\n",
      "Batch 43: loss 64.91559352431186\n",
      "Batch 44: loss 64.92469614202327\n",
      "Batch 45: loss 64.94124925401476\n",
      "Batch 46: loss 64.9241477302883\n",
      "Batch 47: loss 64.868216656624\n",
      "Batch 48: loss 64.86802864074707\n",
      "Batch 49: loss 64.87437890500439\n",
      "Batch 50: loss 64.84757621765137\n",
      "Batch 51: loss 64.8415688159419\n",
      "Batch 52: loss 64.84345531463623\n",
      "Batch 53: loss 64.82468903739498\n",
      "Batch 54: loss 64.85989634195964\n",
      "Batch 55: loss 64.9004746870561\n",
      "Batch 56: loss 64.92288562229702\n",
      "Batch 57: loss 64.89927044249417\n",
      "Batch 58: loss 64.89183175974878\n",
      "Batch 59: loss 64.87458296953622\n",
      "Batch 60: loss 64.82663497924804\n",
      "Batch 61: loss 64.83226075719614\n",
      "Batch 62: loss 64.79645427580803\n",
      "Batch 63: loss 64.78588976178851\n",
      "Batch 64: loss 64.75868833065033\n",
      "Batch 65: loss 64.78214064378005\n",
      "Batch 66: loss 64.78360320582534\n",
      "Batch 67: loss 64.7762988645639\n",
      "Batch 68: loss 64.78210942885455\n",
      "Batch 69: loss 64.72311998450238\n",
      "Batch 70: loss 64.70237726484027\n",
      "Batch 71: loss 64.67542282964142\n",
      "Batch 72: loss 64.70183685090807\n",
      "Batch 73: loss 64.67990049597336\n",
      "Batch 74: loss 64.69054309741871\n",
      "Batch 75: loss 64.68558644612631\n",
      "Batch 76: loss 64.69581403230366\n",
      "Batch 77: loss 64.69658135748529\n",
      "Batch 78: loss 64.69134873610277\n",
      "Batch 79: loss 64.66655538051943\n",
      "Batch 80: loss 64.6340895652771\n",
      "Batch 81: loss 64.60872960973668\n",
      "Batch 82: loss 64.5943037823933\n",
      "Batch 83: loss 64.59282353412674\n",
      "Batch 84: loss 64.582962172372\n",
      "Batch 85: loss 64.57824092191808\n",
      "Batch 86: loss 64.55129760919615\n",
      "Batch 87: loss 64.54067677464978\n",
      "Batch 88: loss 64.52682555805553\n",
      "Batch 89: loss 64.50505815998892\n",
      "Batch 90: loss 64.47313944498698\n",
      "Batch 91: loss 64.4769533597506\n",
      "Batch 92: loss 64.4618440296339\n",
      "Batch 93: loss 64.47376624486779\n",
      "Batch 94: loss 64.48761761442144\n",
      "Batch 95: loss 64.46111743324681\n",
      "Batch 96: loss 64.46191616853078\n",
      "Batch 97: loss 64.44519546351482\n",
      "Batch 98: loss 64.44734479943101\n",
      "Batch 99: loss 64.43691091826467\n",
      "Batch 100: loss 64.44375396728516\n",
      "Batch 101: loss 64.44775949610342\n",
      "Batch 102: loss 64.46789049634747\n",
      "Batch 103: loss 64.45197736869738\n",
      "Batch 104: loss 64.29023624425695\n",
      "Epoch 2: Training loss 64.29023624425695\n",
      "Epoch 2: Validation loss 13.695007209777833 | Accuracy 0.7080000000000001 | AUC 0.8419257095628879\n",
      "Batch 1: loss 64.15172576904297\n",
      "Batch 2: loss 64.07865524291992\n",
      "Batch 3: loss 63.738606770833336\n",
      "Batch 4: loss 63.38241004943848\n",
      "Batch 5: loss 63.212623596191406\n",
      "Batch 6: loss 63.250251134236656\n",
      "Batch 7: loss 63.131347111293245\n",
      "Batch 8: loss 63.32011938095093\n",
      "Batch 9: loss 63.652412838406036\n",
      "Batch 10: loss 63.55194625854492\n",
      "Batch 11: loss 63.50030448219993\n",
      "Batch 12: loss 63.44121742248535\n",
      "Batch 13: loss 63.408177889310394\n",
      "Batch 14: loss 63.44052069527762\n",
      "Batch 15: loss 63.38593495686849\n",
      "Batch 16: loss 63.436851978302\n",
      "Batch 17: loss 63.37985072416418\n",
      "Batch 18: loss 63.59014447530111\n",
      "Batch 19: loss 63.55980802837171\n",
      "Batch 20: loss 63.62063980102539\n",
      "Batch 21: loss 63.66890898204985\n",
      "Batch 22: loss 63.53217679804022\n",
      "Batch 23: loss 63.421575463336445\n",
      "Batch 24: loss 63.374594370524086\n",
      "Batch 25: loss 63.45459014892578\n",
      "Batch 26: loss 63.298373442429764\n",
      "Batch 27: loss 63.25083358199508\n",
      "Batch 28: loss 63.33284160069057\n",
      "Batch 29: loss 63.27804696970973\n",
      "Batch 30: loss 63.23785629272461\n",
      "Batch 31: loss 63.22328616726783\n",
      "Batch 32: loss 63.191871762275696\n",
      "Batch 33: loss 63.12547175089518\n",
      "Batch 34: loss 63.09783082849839\n",
      "Batch 35: loss 63.06478685651507\n",
      "Batch 36: loss 63.00375493367513\n",
      "Batch 37: loss 63.00652034862621\n",
      "Batch 38: loss 62.92350176761025\n",
      "Batch 39: loss 62.904960045447716\n",
      "Batch 40: loss 62.87759628295898\n",
      "Batch 41: loss 62.86441365683951\n",
      "Batch 42: loss 62.82099642072405\n",
      "Batch 43: loss 62.850301875624545\n",
      "Batch 44: loss 62.90539134632457\n",
      "Batch 45: loss 62.89453133477105\n",
      "Batch 46: loss 62.862627029418945\n",
      "Batch 47: loss 62.829799327444526\n",
      "Batch 48: loss 62.86326686541239\n",
      "Batch 49: loss 62.85907558519013\n",
      "Batch 50: loss 62.8655298614502\n",
      "Batch 51: loss 62.87427737666111\n",
      "Batch 52: loss 62.82176311199482\n",
      "Batch 53: loss 62.84172259636645\n",
      "Batch 54: loss 62.77306973492658\n",
      "Batch 55: loss 62.79722706187855\n",
      "Batch 56: loss 62.820124762398855\n",
      "Batch 57: loss 62.85457664623595\n",
      "Batch 58: loss 62.85707440869562\n",
      "Batch 59: loss 62.77350726370084\n",
      "Batch 60: loss 62.7755033493042\n",
      "Batch 61: loss 62.754218242207514\n",
      "Batch 62: loss 62.75734378445533\n",
      "Batch 63: loss 62.734741332039\n",
      "Batch 64: loss 62.707076013088226\n",
      "Batch 65: loss 62.67334547776442\n",
      "Batch 66: loss 62.63029820991285\n",
      "Batch 67: loss 62.61239014810591\n",
      "Batch 68: loss 62.59461565578685\n",
      "Batch 69: loss 62.59060646831126\n",
      "Batch 70: loss 62.547336905343194\n",
      "Batch 71: loss 62.52235890778018\n",
      "Batch 72: loss 62.5363630188836\n",
      "Batch 73: loss 62.5164843520073\n",
      "Batch 74: loss 62.46466414992874\n",
      "Batch 75: loss 62.49260965983073\n",
      "Batch 76: loss 62.50454857474879\n",
      "Batch 77: loss 62.46568010999011\n",
      "Batch 78: loss 62.44259927211664\n",
      "Batch 79: loss 62.39947548395471\n",
      "Batch 80: loss 62.39352269172669\n",
      "Batch 81: loss 62.40976606769326\n",
      "Batch 82: loss 62.452108011013124\n",
      "Batch 83: loss 62.47652504243047\n",
      "Batch 84: loss 62.46701258704776\n",
      "Batch 85: loss 62.44799737369313\n",
      "Batch 86: loss 62.43300260499466\n",
      "Batch 87: loss 62.40369937063634\n",
      "Batch 88: loss 62.38783073425293\n",
      "Batch 89: loss 62.37399382001898\n",
      "Batch 90: loss 62.36529943678114\n",
      "Batch 91: loss 62.32656227363335\n",
      "Batch 92: loss 62.318968772888184\n",
      "Batch 93: loss 62.299446393084786\n",
      "Batch 94: loss 62.29422342016342\n",
      "Batch 95: loss 62.30370752434982\n",
      "Batch 96: loss 62.2927295366923\n",
      "Batch 97: loss 62.30071459111479\n",
      "Batch 98: loss 62.27392792215153\n",
      "Batch 99: loss 62.26076195456765\n",
      "Batch 100: loss 62.24757682800293\n",
      "Batch 101: loss 62.23270911037332\n",
      "Batch 102: loss 62.25889665940229\n",
      "Batch 103: loss 62.261417018557054\n",
      "Batch 104: loss 62.11047070683361\n",
      "Epoch 3: Training loss 62.11047070683361\n",
      "Epoch 3: Validation loss 12.538246059417725 | Accuracy 0.8787999999999999 | AUC 0.960371648690046\n",
      "Batch 1: loss 61.35894012451172\n",
      "Batch 2: loss 61.52138710021973\n",
      "Batch 3: loss 61.30335998535156\n",
      "Batch 4: loss 61.37578773498535\n",
      "Batch 5: loss 61.14888916015625\n",
      "Batch 6: loss 60.9964853922526\n",
      "Batch 7: loss 61.26714924403599\n",
      "Batch 8: loss 61.04829263687134\n",
      "Batch 9: loss 61.15975485907661\n",
      "Batch 10: loss 61.28816566467285\n",
      "Batch 11: loss 61.343442396684125\n",
      "Batch 12: loss 61.33804098765055\n",
      "Batch 13: loss 61.35966022198017\n",
      "Batch 14: loss 61.370168685913086\n",
      "Batch 15: loss 61.29857864379883\n",
      "Batch 16: loss 61.14212965965271\n",
      "Batch 17: loss 61.076646468218634\n",
      "Batch 18: loss 60.981434716118706\n",
      "Batch 19: loss 61.009775262129935\n",
      "Batch 20: loss 60.86665859222412\n",
      "Batch 21: loss 60.9677490960984\n",
      "Batch 22: loss 60.90256812355735\n",
      "Batch 23: loss 60.88126837688944\n",
      "Batch 24: loss 60.766623655954994\n",
      "Batch 25: loss 60.74409652709961\n",
      "Batch 26: loss 60.85419493455153\n",
      "Batch 27: loss 60.80032334504304\n",
      "Batch 28: loss 60.773917061941965\n",
      "Batch 29: loss 60.840372545965785\n",
      "Batch 30: loss 60.84626859029134\n",
      "Batch 31: loss 60.84579529300813\n",
      "Batch 32: loss 60.896920680999756\n",
      "Batch 33: loss 60.83253109093869\n",
      "Batch 34: loss 60.81856570524328\n",
      "Batch 35: loss 60.80478809901646\n",
      "Batch 36: loss 60.811203214857315\n",
      "Batch 37: loss 60.776737728634394\n",
      "Batch 38: loss 60.821889877319336\n",
      "Batch 39: loss 60.77380204812074\n",
      "Batch 40: loss 60.76213626861572\n",
      "Batch 41: loss 60.70634730269269\n",
      "Batch 42: loss 60.72801599048433\n",
      "Batch 43: loss 60.722816644712935\n",
      "Batch 44: loss 60.75646071000533\n",
      "Batch 45: loss 60.70219946967231\n",
      "Batch 46: loss 60.693525811900265\n",
      "Batch 47: loss 60.729547297700925\n",
      "Batch 48: loss 60.71557847658793\n",
      "Batch 49: loss 60.67617774496273\n",
      "Batch 50: loss 60.66307411193848\n",
      "Batch 51: loss 60.66013957004921\n",
      "Batch 52: loss 60.700433657719536\n",
      "Batch 53: loss 60.683237111793375\n",
      "Batch 54: loss 60.584212126555265\n",
      "Batch 55: loss 60.5438664523038\n",
      "Batch 56: loss 60.52358559199742\n",
      "Batch 57: loss 60.53632107115629\n",
      "Batch 58: loss 60.52055760087638\n",
      "Batch 59: loss 60.51932506238\n",
      "Batch 60: loss 60.54172128041585\n",
      "Batch 61: loss 60.51844712554431\n",
      "Batch 62: loss 60.560684019519435\n",
      "Batch 63: loss 60.56147469414605\n",
      "Batch 64: loss 60.61747759580612\n",
      "Batch 65: loss 60.62141794057993\n",
      "Batch 66: loss 60.616088520396836\n",
      "Batch 67: loss 60.60970665091899\n",
      "Batch 68: loss 60.57631587982178\n",
      "Batch 69: loss 60.57482794056768\n",
      "Batch 70: loss 60.574066816057474\n",
      "Batch 71: loss 60.55122321760151\n",
      "Batch 72: loss 60.53887918260362\n",
      "Batch 73: loss 60.514808811553536\n",
      "Batch 74: loss 60.48112199113176\n",
      "Batch 75: loss 60.47789235432943\n",
      "Batch 76: loss 60.462531993263646\n",
      "Batch 77: loss 60.418624035723795\n",
      "Batch 78: loss 60.40457921150403\n",
      "Batch 79: loss 60.41933600510223\n",
      "Batch 80: loss 60.41550526618958\n",
      "Batch 81: loss 60.39435323079427\n",
      "Batch 82: loss 60.40222302878775\n",
      "Batch 83: loss 60.40583663388907\n",
      "Batch 84: loss 60.38702851250058\n",
      "Batch 85: loss 60.36569375430837\n",
      "Batch 86: loss 60.36580334153286\n",
      "Batch 87: loss 60.34821942208827\n",
      "Batch 88: loss 60.34743642807007\n",
      "Batch 89: loss 60.34611596954003\n",
      "Batch 90: loss 60.30997144911024\n",
      "Batch 91: loss 60.25317370236575\n",
      "Batch 92: loss 60.24660595603611\n",
      "Batch 93: loss 60.262640101935276\n",
      "Batch 94: loss 60.2441926509776\n",
      "Batch 95: loss 60.23489146985506\n",
      "Batch 96: loss 60.22737526893616\n",
      "Batch 97: loss 60.21970198326504\n",
      "Batch 98: loss 60.19472332389987\n",
      "Batch 99: loss 60.187952215021305\n",
      "Batch 100: loss 60.17711269378662\n",
      "Batch 101: loss 60.195189069993425\n",
      "Batch 102: loss 60.19715623294606\n",
      "Batch 103: loss 60.195865742211204\n",
      "Batch 104: loss 60.048043884868065\n",
      "Epoch 4: Training loss 60.048043884868065\n",
      "Epoch 4: Validation loss 11.995727138519287 | Accuracy 0.9518 | AUC 0.9959282279550389\n",
      "Batch 1: loss 58.67021942138672\n",
      "Batch 2: loss 58.05694389343262\n",
      "Batch 3: loss 58.08469899495443\n",
      "Batch 4: loss 58.2645206451416\n",
      "Batch 5: loss 58.29432373046875\n",
      "Batch 6: loss 58.33967971801758\n",
      "Batch 7: loss 58.60895756312779\n",
      "Batch 8: loss 58.63077735900879\n",
      "Batch 9: loss 58.681190490722656\n",
      "Batch 10: loss 58.76155395507813\n",
      "Batch 11: loss 58.85509490966797\n",
      "Batch 12: loss 58.94226201375326\n",
      "Batch 13: loss 58.88914460402269\n",
      "Batch 14: loss 58.91794531685965\n",
      "Batch 15: loss 58.92866541544596\n",
      "Batch 16: loss 59.003318309783936\n",
      "Batch 17: loss 59.00977325439453\n",
      "Batch 18: loss 59.04408984714084\n",
      "Batch 19: loss 59.08253278230366\n",
      "Batch 20: loss 59.009420776367186\n",
      "Batch 21: loss 58.977156866164435\n",
      "Batch 22: loss 59.01652769608931\n",
      "Batch 23: loss 59.1497335019319\n",
      "Batch 24: loss 59.26596530278524\n",
      "Batch 25: loss 59.176254425048825\n",
      "Batch 26: loss 59.335360747117264\n",
      "Batch 27: loss 59.32269866378219\n",
      "Batch 28: loss 59.35228715624128\n",
      "Batch 29: loss 59.30568918688544\n",
      "Batch 30: loss 59.33763440450033\n",
      "Batch 31: loss 59.379161096388295\n",
      "Batch 32: loss 59.436970472335815\n",
      "Batch 33: loss 59.50015513102213\n",
      "Batch 34: loss 59.52336244022145\n",
      "Batch 35: loss 59.459454563685824\n",
      "Batch 36: loss 59.445990562438965\n",
      "Batch 37: loss 59.47783372208879\n",
      "Batch 38: loss 59.45656896892347\n",
      "Batch 39: loss 59.49216832870092\n",
      "Batch 40: loss 59.48344173431396\n",
      "Batch 41: loss 59.48562287121284\n",
      "Batch 42: loss 59.434915270124165\n",
      "Batch 43: loss 59.42107648627702\n",
      "Batch 44: loss 59.48439953543923\n",
      "Batch 45: loss 59.42176835801866\n",
      "Batch 46: loss 59.398985821267836\n",
      "Batch 47: loss 59.42317159125145\n",
      "Batch 48: loss 59.415085554122925\n",
      "Batch 49: loss 59.4561239748585\n",
      "Batch 50: loss 59.40590599060059\n",
      "Batch 51: loss 59.40791305841184\n",
      "Batch 52: loss 59.3887478021475\n",
      "Batch 53: loss 59.42138643084832\n",
      "Batch 54: loss 59.41530736287435\n",
      "Batch 55: loss 59.35679758245295\n",
      "Batch 56: loss 59.325692244938445\n",
      "Batch 57: loss 59.33916252537777\n",
      "Batch 58: loss 59.326386747689085\n",
      "Batch 59: loss 59.35241996635825\n",
      "Batch 60: loss 59.389499473571775\n",
      "Batch 61: loss 59.35257427027968\n",
      "Batch 62: loss 59.301050370739354\n",
      "Batch 63: loss 59.298492673843626\n",
      "Batch 64: loss 59.3310906291008\n",
      "Batch 65: loss 59.36791493342473\n",
      "Batch 66: loss 59.37409285343055\n",
      "Batch 67: loss 59.40333124416978\n",
      "Batch 68: loss 59.35646876166849\n",
      "Batch 69: loss 59.313902260600656\n",
      "Batch 70: loss 59.30030555725098\n",
      "Batch 71: loss 59.31915095154668\n",
      "Batch 72: loss 59.311385207706024\n",
      "Batch 73: loss 59.29445825864191\n",
      "Batch 74: loss 59.32569251189361\n",
      "Batch 75: loss 59.33777592976888\n",
      "Batch 76: loss 59.32418386559738\n",
      "Batch 77: loss 59.28919992818461\n",
      "Batch 78: loss 59.30341916206555\n",
      "Batch 79: loss 59.30311758306962\n",
      "Batch 80: loss 59.2989278793335\n",
      "Batch 81: loss 59.281152372007014\n",
      "Batch 82: loss 59.27715394555069\n",
      "Batch 83: loss 59.253220156014685\n",
      "Batch 84: loss 59.26115680876232\n",
      "Batch 85: loss 59.248462497486784\n",
      "Batch 86: loss 59.22975677667662\n",
      "Batch 87: loss 59.24936531329977\n",
      "Batch 88: loss 59.2340689572421\n",
      "Batch 89: loss 59.210378882590305\n",
      "Batch 90: loss 59.23438674079047\n",
      "Batch 91: loss 59.23135472391988\n",
      "Batch 92: loss 59.21944054313328\n",
      "Batch 93: loss 59.20517841462166\n",
      "Batch 94: loss 59.198283540441636\n",
      "Batch 95: loss 59.20315652144583\n",
      "Batch 96: loss 59.22791556517283\n",
      "Batch 97: loss 59.20207088509786\n",
      "Batch 98: loss 59.188965622259644\n",
      "Batch 99: loss 59.18609915839301\n",
      "Batch 100: loss 59.206565132141115\n",
      "Batch 101: loss 59.21929376432211\n",
      "Batch 102: loss 59.1991159027698\n",
      "Batch 103: loss 59.19863024961601\n",
      "Batch 104: loss 59.04917276740593\n",
      "Epoch 5: Training loss 59.04917276740593\n",
      "Epoch 5: Validation loss 11.797716445922852 | Accuracy 0.9902000000000001 | AUC 0.9993667318695159\n",
      "Batch 1: loss 57.912479400634766\n",
      "Batch 2: loss 58.00713920593262\n",
      "Batch 3: loss 59.20877456665039\n",
      "Batch 4: loss 59.482069969177246\n",
      "Batch 5: loss 59.273163604736325\n",
      "Batch 6: loss 59.03997357686361\n",
      "Batch 7: loss 58.786125728062224\n",
      "Batch 8: loss 58.66343927383423\n",
      "Batch 9: loss 58.57358508639865\n",
      "Batch 10: loss 58.34455261230469\n",
      "Batch 11: loss 58.5221921747381\n",
      "Batch 12: loss 58.77389748891195\n",
      "Batch 13: loss 58.73132001436674\n",
      "Batch 14: loss 58.70270211356027\n",
      "Batch 15: loss 58.75308634440104\n",
      "Batch 16: loss 58.713576555252075\n",
      "Batch 17: loss 58.768350040211395\n",
      "Batch 18: loss 58.63494873046875\n",
      "Batch 19: loss 58.521622105648646\n",
      "Batch 20: loss 58.58861045837402\n",
      "Batch 21: loss 58.572938828241256\n",
      "Batch 22: loss 58.671904130415484\n",
      "Batch 23: loss 58.81611119145932\n",
      "Batch 24: loss 58.7804209391276\n",
      "Batch 25: loss 58.786431579589845\n",
      "Batch 26: loss 58.78894145672138\n",
      "Batch 27: loss 58.78384936297381\n",
      "Batch 28: loss 58.7460549218314\n",
      "Batch 29: loss 58.72705196512157\n",
      "Batch 30: loss 58.85829099019369\n",
      "Batch 31: loss 58.89369890766759\n",
      "Batch 32: loss 58.84644067287445\n",
      "Batch 33: loss 58.867555444890804\n",
      "Batch 34: loss 58.88704389684341\n",
      "Batch 35: loss 58.996187046595985\n",
      "Batch 36: loss 58.9640777375963\n",
      "Batch 37: loss 58.97362827610325\n",
      "Batch 38: loss 58.965409529836556\n",
      "Batch 39: loss 58.99585195688101\n",
      "Batch 40: loss 58.967058944702146\n",
      "Batch 41: loss 58.91998737614329\n",
      "Batch 42: loss 58.93815040588379\n",
      "Batch 43: loss 58.956474481627\n",
      "Batch 44: loss 58.9788795817982\n",
      "Batch 45: loss 58.93330612182617\n",
      "Batch 46: loss 58.9772019593612\n",
      "Batch 47: loss 58.95495759679916\n",
      "Batch 48: loss 58.97655916213989\n",
      "Batch 49: loss 58.935832821592996\n",
      "Batch 50: loss 58.861393508911135\n",
      "Batch 51: loss 58.87366844626034\n",
      "Batch 52: loss 58.87529255793645\n",
      "Batch 53: loss 58.83598658723651\n",
      "Batch 54: loss 58.84185819272642\n",
      "Batch 55: loss 58.84857406616211\n",
      "Batch 56: loss 58.867657797677175\n",
      "Batch 57: loss 58.8545305352462\n",
      "Batch 58: loss 58.8413463460988\n",
      "Batch 59: loss 58.84326087822348\n",
      "Batch 60: loss 58.841144752502444\n",
      "Batch 61: loss 58.859187892225926\n",
      "Batch 62: loss 58.834805580877486\n",
      "Batch 63: loss 58.82403812711201\n",
      "Batch 64: loss 58.86011028289795\n",
      "Batch 65: loss 58.86056929368239\n",
      "Batch 66: loss 58.83761284568093\n",
      "Batch 67: loss 58.81593334141062\n",
      "Batch 68: loss 58.811029826893524\n",
      "Batch 69: loss 58.77886614592179\n",
      "Batch 70: loss 58.79422678266253\n",
      "Batch 71: loss 58.782474517822266\n",
      "Batch 72: loss 58.774483097924126\n",
      "Batch 73: loss 58.7556078662611\n",
      "Batch 74: loss 58.737770235216296\n",
      "Batch 75: loss 58.76789026896159\n",
      "Batch 76: loss 58.80150047101473\n",
      "Batch 77: loss 58.81174761289126\n",
      "Batch 78: loss 58.79119423108223\n",
      "Batch 79: loss 58.813301521011546\n",
      "Batch 80: loss 58.8450511932373\n",
      "Batch 81: loss 58.856747615484544\n",
      "Batch 82: loss 58.83742672059594\n",
      "Batch 83: loss 58.839789562914746\n",
      "Batch 84: loss 58.83009588150751\n",
      "Batch 85: loss 58.822178919175094\n",
      "Batch 86: loss 58.82971022849859\n",
      "Batch 87: loss 58.821447613595545\n",
      "Batch 88: loss 58.823612386530094\n",
      "Batch 89: loss 58.824599319629456\n",
      "Batch 90: loss 58.815004391140405\n",
      "Batch 91: loss 58.81822627979321\n",
      "Batch 92: loss 58.79448073843251\n",
      "Batch 93: loss 58.80037336452033\n",
      "Batch 94: loss 58.80729752398552\n",
      "Batch 95: loss 58.817181717722036\n",
      "Batch 96: loss 58.81861706574758\n",
      "Batch 97: loss 58.82870294629913\n",
      "Batch 98: loss 58.816663391736085\n",
      "Batch 99: loss 58.819669434518524\n",
      "Batch 100: loss 58.853599014282224\n",
      "Batch 101: loss 58.86921680563747\n",
      "Batch 102: loss 58.86028128979253\n",
      "Batch 103: loss 58.84762665831927\n",
      "Batch 104: loss 58.6979304612378\n",
      "Epoch 6: Training loss 58.6979304612378\n",
      "Epoch 6: Validation loss 11.748415126800538 | Accuracy 0.9976 | AUC 0.9986290667724407\n",
      "Best epoch:  6\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "mtrainer.run_train(6, support_loader, query_loader, lr=2e-5, full_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7689795918367346, 1.0, 13.967295932769776)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, query_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.model.attention, 'models/attention/model/vindr2/full/attention-model8h4l.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.model.encoder, 'models/attention/model/vindr2/full/imgtxt-encoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import AverageMeter, calculate_auc, multilabel_logit_accuracy\n",
    "from torchmetrics.classification import MultilabelRecall, MultilabelSpecificity, MultilabelPrecision\n",
    "from models.attention.model import image_text_logits\n",
    "\n",
    "def run_eval(model, dataloader, device, class_labels):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    loss_meter = AverageMeter()\n",
    "    auc_meter = AverageMeter()\n",
    "    acc_meter = AverageMeter()\n",
    "\n",
    "    specificity = MultilabelSpecificity(num_labels=len(class_labels)).to(device)\n",
    "    spec_meter = AverageMeter()\n",
    "    recall = MultilabelRecall(num_labels=len(class_labels)).to(device)\n",
    "    rec_meter = AverageMeter()\n",
    "    precision = MultilabelPrecision(num_labels=len(class_labels)).to(device)\n",
    "    with torch.no_grad():\n",
    "         for images, class_inds in dataloader:\n",
    "                images, class_inds = images.to(device), class_inds.to(device)\n",
    "\n",
    "                text_embeddings, _, prototypes = model(class_labels, images, class_inds)\n",
    "\n",
    "                logits_per_image = image_text_logits(text_embeddings, prototypes, model.encoder.get_logit_scale())\n",
    "                loss = model.attention.contrastive_loss(prototypes, class_inds).item()\n",
    "                loss += model.attention.classification_loss(logits_per_image, class_inds).item()\n",
    "        \n",
    "                loss_meter.update(loss, len(class_inds))\n",
    "\n",
    "                auc = calculate_auc(logits_per_image, class_inds)\n",
    "                auc_meter.update(auc, len(class_inds))\n",
    "            \n",
    "                acc = multilabel_logit_accuracy(logits_per_image, class_inds)\n",
    "                acc_meter.update(acc, len(class_inds))\n",
    "\n",
    "                spec = specificity(logits_per_image, class_inds)\n",
    "                spec_meter.update(spec.item(), len(class_inds))\n",
    "                rec = recall(logits_per_image, class_inds)\n",
    "                rec_meter.update(rec.item(), len(class_inds))\n",
    "                prec = precision(logits_per_image, class_inds)\n",
    "                print(f\"Loss {loss} | Accuracy {acc} | AUC {auc} | Specificity {spec} | Recall {rec} | Precision {prec}\")\n",
    "            \n",
    "    return acc_meter.average(), auc_meter.average(), loss_meter.average(), spec_meter.average(), rec_meter.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 6.035398006439209 | Accuracy 0.9163265306122449 | AUC 0.9601097442520172 | Specificity 0.8863561153411865 | Recall 1.0 | Precision 0.7190194129943848\n",
      "Loss 6.016775846481323 | Accuracy 0.9020408163265307 | AUC 0.950413918427783 | Specificity 0.8675279021263123 | Recall 1.0 | Precision 0.6775742173194885\n",
      "Loss nan | Accuracy 0.9061224489795918 | AUC 0.9621225231408158 | Specificity 0.8780972361564636 | Recall 1.0 | Precision 0.6373937726020813\n",
      "Loss 5.877500772476196 | Accuracy 0.9051020408163265 | AUC 0.9520963027023325 | Specificity 0.8736120462417603 | Recall 1.0 | Precision 0.6980288028717041\n",
      "Loss 5.879352807998657 | Accuracy 0.9153061224489796 | AUC 0.9580760437786499 | Specificity 0.888192355632782 | Recall 1.0 | Precision 0.6855992674827576\n",
      "Loss 5.763930082321167 | Accuracy 0.9122448979591836 | AUC 0.9568867816190046 | Specificity 0.8859713673591614 | Recall 1.0 | Precision 0.7168556451797485\n",
      "Loss 5.975746512413025 | Accuracy 0.9071428571428571 | AUC 0.9517384316075806 | Specificity 0.8739240765571594 | Recall 1.0 | Precision 0.725176990032196\n",
      "Loss 5.778767108917236 | Accuracy 0.9020408163265307 | AUC 0.9550003321081502 | Specificity 0.8708547353744507 | Recall 1.0 | Precision 0.6752564311027527\n",
      "Loss 6.011700749397278 | Accuracy 0.9081632653061225 | AUC 0.9594566053022875 | Specificity 0.8792134523391724 | Recall 1.0 | Precision 0.7122929096221924\n",
      "Loss 6.004516124725342 | Accuracy 0.9091836734693878 | AUC 0.9571996478153567 | Specificity 0.8750742673873901 | Recall 1.0 | Precision 0.6936899423599243\n",
      "Loss 5.634267091751099 | Accuracy 0.9023199023199023 | AUC 0.9581488193279074 | Specificity 0.8727905750274658 | Recall 1.0 | Precision 0.6700223684310913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9079009322911763, 0.956451851131788, nan, 0.8774896472712135, 1.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = Dataset(config.img_path, config.img_info, config.img_info[config.img_info['meta_split'] == 'test']['image_id'].to_list(), config.label_names_map, config.classes_split_map['test'], mean_std=config.mean_std)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "run_eval(mtrainer.model, test_loader, device, test_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 11.984139442443848 | Accuracy 0.9978571428571429 | AUC 0.999432338137726 | Specificity 0.991599440574646 | Recall 1.0 | Precision 0.9951183795928955\n",
      "Loss 11.039484977722168 | Accuracy 0.9957142857142857 | AUC 0.9983705482192878 | Specificity 0.9866666793823242 | Recall 1.0 | Precision 0.9876745939254761\n",
      "Loss 12.2591233253479 | Accuracy 0.9978571428571429 | AUC 0.995954538069212 | Specificity 0.98682701587677 | Recall 1.0 | Precision 0.9947980046272278\n",
      "Loss 11.675161838531494 | Accuracy 1.0 | AUC 1.0 | Specificity 1.0 | Recall 1.0 | Precision 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9976000000000002,\n",
       " 0.9982520788393433,\n",
       " 11.747195262908935,\n",
       " 0.9902260780334473,\n",
       " 1.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_eval(mtrainer.model, query_loader, device, query_dataset.class_labels())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
