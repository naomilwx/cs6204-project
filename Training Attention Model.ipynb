{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from models.attention.model import LabelImageAttention, LabelImagePrototypeModel\n",
    "from models.attention.trainer import Trainer\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "\n",
    "from utils.device import get_device\n",
    "from utils.data import get_query_and_support_ids\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "from models.embedding.dataset import Dataset\n",
    "from utils.sampling import MultilabelBalancedRandomSampler\n",
    "\n",
    "img_info = pd.read_pickle('data/vindr_cxr_split_labels.pkl')\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(img_info, 'data/vindr_train_query_set.pkl')\n",
    "\n",
    "IMG_PATH = 'datasets/vindr-cxr-png'\n",
    "batch_size = 10*14\n",
    "\n",
    "query_dataset = Dataset(IMG_PATH, img_info, query_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "query_loader = DataLoader(dataset=query_dataset, batch_size=batch_size, shuffle=True)\n",
    "support_dataset = Dataset(IMG_PATH, img_info, support_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "support_loader = DataLoader(dataset=support_dataset, batch_size=batch_size, sampler=MultilabelBalancedRandomSampler(support_dataset.get_class_indicators()))\n",
    "\n",
    "PROJ_SIZE = 512\n",
    "# device = 'cpu'\n",
    "device =  get_device()\n",
    "\n",
    "encoder = torch.load('models/embedding/model/imgtext_model_trained.pth')\n",
    "encoder.text_model.device = device\n",
    "model = LabelImagePrototypeModel(encoder, 8, PROJ_SIZE, num_layers=4, cls_weight=0.5)\n",
    "mtrainer = Trainer(model, support_dataset.class_labels(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naomileow/Documents/school/CS6240/project/models/attention/model.py:48: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  classes = torch.nonzero(label_inds)[:,1] # (Np,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 11.806146621704102\n",
      "Batch 2: loss 12.007411479949951\n",
      "Batch 3: loss 12.039634386698404\n",
      "Batch 4: loss 11.932925462722778\n",
      "Batch 5: loss 11.890310096740723\n",
      "Batch 6: loss 11.853495597839355\n",
      "Batch 7: loss 11.849952697753906\n",
      "Batch 8: loss 11.844443798065186\n",
      "Batch 9: loss 11.81305980682373\n",
      "Batch 10: loss 11.809188938140869\n",
      "Batch 11: loss 11.803815234791148\n",
      "Batch 12: loss 11.796727895736694\n",
      "Batch 13: loss 11.746948608985313\n",
      "Batch 14: loss 11.71504613331386\n",
      "Batch 15: loss 11.693219248453776\n",
      "Batch 16: loss 11.680330693721771\n",
      "Batch 17: loss 11.701137711020078\n",
      "Batch 18: loss 11.681871891021729\n",
      "Batch 19: loss 11.655200908058568\n",
      "Batch 20: loss 11.657014513015747\n",
      "Batch 21: loss 11.635733740670341\n",
      "Batch 22: loss 11.63504713231867\n",
      "Batch 23: loss 11.62240335215693\n",
      "Batch 24: loss 11.595344622929892\n",
      "Batch 25: loss 11.595146560668946\n",
      "Batch 26: loss 11.573901946728046\n",
      "Batch 27: loss 11.571162011888292\n",
      "Batch 28: loss 11.55950699533735\n",
      "Batch 29: loss 11.54445378533725\n",
      "Batch 30: loss 11.531041272481282\n",
      "Batch 31: loss 11.51570689293646\n",
      "Batch 32: loss 11.500483632087708\n",
      "Batch 33: loss 11.500568852280125\n",
      "Batch 34: loss 11.489229931550867\n",
      "Batch 35: loss 11.471573202950614\n",
      "Batch 36: loss 11.465562396579319\n",
      "Batch 37: loss 11.4473677454768\n",
      "Batch 38: loss 11.442781799717954\n",
      "Batch 39: loss 11.435778960203512\n",
      "Batch 40: loss 11.426014494895934\n",
      "Batch 41: loss 11.422851585760348\n",
      "Batch 42: loss 11.419801167079381\n",
      "Batch 43: loss 11.420458416606104\n",
      "Batch 44: loss 11.407158656553788\n",
      "Batch 45: loss 11.403819931877983\n",
      "Batch 46: loss 11.397162126458209\n",
      "Batch 47: loss 11.38669994029593\n",
      "Batch 48: loss 11.38526306549708\n",
      "Batch 49: loss 11.375137601579938\n",
      "Batch 50: loss 11.372301216125487\n",
      "Batch 51: loss 11.37037269741881\n",
      "Batch 52: loss 11.35915547150832\n",
      "Batch 53: loss 11.347861847787533\n",
      "Batch 54: loss 11.341449419657389\n",
      "Batch 55: loss 11.328922965309836\n",
      "Batch 56: loss 11.319555861609322\n",
      "Batch 57: loss 11.309592263740406\n",
      "Batch 58: loss 11.295857199307147\n",
      "Batch 59: loss 11.286049632702843\n",
      "Batch 60: loss 11.279078912734985\n",
      "Batch 61: loss 11.273920575126272\n",
      "Batch 62: loss 11.269032186077487\n",
      "Batch 63: loss 11.259140090336876\n",
      "Batch 64: loss 11.254544347524643\n",
      "Batch 65: loss 11.252709286029523\n",
      "Batch 66: loss 11.2461368820884\n",
      "Batch 67: loss 11.240110425806757\n",
      "Batch 68: loss 11.237371444702148\n",
      "Batch 69: loss 11.237776922143023\n",
      "Batch 70: loss 11.23692808151245\n",
      "Batch 71: loss 11.227839147540886\n",
      "Batch 72: loss 11.221689873271519\n",
      "Batch 73: loss 11.216666300002842\n",
      "Batch 74: loss 11.21088558918721\n",
      "Batch 75: loss 11.204484990437825\n",
      "Batch 76: loss 11.199726782347026\n",
      "Batch 77: loss 11.199597717879655\n",
      "Batch 78: loss 11.196742302332169\n",
      "Batch 79: loss 11.189450710634642\n",
      "Batch 80: loss 11.184234833717346\n",
      "Batch 81: loss 11.181623929812584\n",
      "Batch 82: loss 11.175032464469352\n",
      "Batch 83: loss 11.171555208872599\n",
      "Batch 84: loss 11.170551958538237\n",
      "Batch 85: loss 11.16114466050092\n",
      "Batch 86: loss 11.157428830168968\n",
      "Batch 87: loss 11.155779356243967\n",
      "Batch 88: loss 11.153850371187383\n",
      "Batch 89: loss 11.149316059069687\n",
      "Batch 90: loss 11.145770729912652\n",
      "Batch 91: loss 11.143180910047594\n",
      "Batch 92: loss 11.140289513961129\n",
      "Batch 93: loss 11.13771842628397\n",
      "Batch 94: loss 11.134870732084234\n",
      "Batch 95: loss 11.130922879670795\n",
      "Batch 96: loss 11.122785846392313\n",
      "Batch 97: loss 11.116887417036233\n",
      "Batch 98: loss 11.113746935007523\n",
      "Batch 99: loss 11.109898336005934\n",
      "Batch 100: loss 11.105339860916137\n",
      "Batch 101: loss 11.10178818088947\n",
      "Batch 102: loss 11.099460592456893\n",
      "Batch 103: loss 11.095016525787058\n",
      "Batch 104: loss 11.088741760987501\n",
      "Batch 105: loss 11.086768840608142\n",
      "Batch 106: loss 11.084158888402975\n",
      "Batch 107: loss 11.079319454799188\n",
      "Batch 108: loss 11.076078017552694\n",
      "Batch 109: loss 11.07380150808768\n",
      "Epoch 1: Training loss 11.07380150808768\n",
      "Epoch 1: Validation loss 21.612201499938966 | Accuracy 73.8061224489796 | AUC 0.9484951194063899\n",
      "Batch 1: loss 10.614805221557617\n",
      "Batch 2: loss 10.791171550750732\n",
      "Batch 3: loss 10.692160288492838\n",
      "Batch 4: loss 10.680566310882568\n",
      "Batch 5: loss 10.648024749755859\n",
      "Batch 6: loss 10.639620304107666\n",
      "Batch 7: loss 10.660628046308245\n",
      "Batch 8: loss 10.683809161186218\n",
      "Batch 9: loss 10.679319487677681\n",
      "Batch 10: loss 10.64482831954956\n",
      "Batch 11: loss 10.621078057722611\n",
      "Batch 12: loss 10.612625201543173\n",
      "Batch 13: loss 10.62261911538931\n",
      "Batch 14: loss 10.599873610905238\n",
      "Batch 15: loss 10.63013916015625\n",
      "Batch 16: loss 10.646358370780945\n",
      "Batch 17: loss 10.624699199900908\n",
      "Batch 18: loss 10.618767314487034\n",
      "Batch 19: loss 10.632070039447985\n",
      "Batch 20: loss 10.618704700469971\n",
      "Batch 21: loss 10.610234215146018\n",
      "Batch 22: loss 10.607966769825328\n",
      "Batch 23: loss 10.605315581611965\n",
      "Batch 24: loss 10.60485545794169\n",
      "Batch 25: loss 10.614475555419922\n",
      "Batch 26: loss 10.608543212597187\n",
      "Batch 27: loss 10.60837060433847\n",
      "Batch 28: loss 10.600613253457206\n",
      "Batch 29: loss 10.597956690294989\n",
      "Batch 30: loss 10.59416904449463\n",
      "Batch 31: loss 10.58802253969254\n",
      "Batch 32: loss 10.579752087593079\n",
      "Batch 33: loss 10.573173002763228\n",
      "Batch 34: loss 10.564417109769934\n",
      "Batch 35: loss 10.564616911751884\n",
      "Batch 36: loss 10.55401939815945\n",
      "Batch 37: loss 10.555795824205553\n",
      "Batch 38: loss 10.556635655854878\n",
      "Batch 39: loss 10.560486769064878\n",
      "Batch 40: loss 10.558681297302247\n",
      "Batch 41: loss 10.558566070184476\n",
      "Batch 42: loss 10.555517764318557\n",
      "Batch 43: loss 10.55407122678535\n",
      "Batch 44: loss 10.553834243254228\n",
      "Batch 45: loss 10.551086489359538\n",
      "Batch 46: loss 10.54573044569596\n",
      "Batch 47: loss 10.544012597266663\n",
      "Batch 48: loss 10.534635106722513\n",
      "Batch 49: loss 10.528863634381976\n",
      "Batch 50: loss 10.530335063934325\n",
      "Batch 51: loss 10.527759028416053\n",
      "Batch 52: loss 10.520689634176401\n",
      "Batch 53: loss 10.516308748497153\n",
      "Batch 54: loss 10.515989621480307\n",
      "Batch 55: loss 10.511846195567738\n",
      "Batch 56: loss 10.507002438817706\n",
      "Batch 57: loss 10.503863803127356\n",
      "Batch 58: loss 10.498128907433872\n",
      "Batch 59: loss 10.495037579940536\n",
      "Batch 60: loss 10.494817717870076\n",
      "Batch 61: loss 10.491360382955582\n",
      "Batch 62: loss 10.487214196112848\n",
      "Batch 63: loss 10.48285576653859\n",
      "Batch 64: loss 10.483315870165825\n",
      "Batch 65: loss 10.474961618276742\n",
      "Batch 66: loss 10.471566272504402\n",
      "Batch 67: loss 10.466134455666614\n",
      "Batch 68: loss 10.465759080999037\n",
      "Batch 69: loss 10.46099541152733\n",
      "Batch 70: loss 10.458696515219552\n",
      "Batch 71: loss 10.453175410418444\n",
      "Batch 72: loss 10.453643321990967\n",
      "Batch 73: loss 10.453579053486862\n",
      "Batch 74: loss 10.454169247601483\n",
      "Batch 75: loss 10.45249210357666\n",
      "Batch 76: loss 10.447408738889193\n",
      "Batch 77: loss 10.440125304383116\n",
      "Batch 78: loss 10.440789271623661\n",
      "Batch 79: loss 10.43428969081444\n",
      "Batch 80: loss 10.431620025634766\n",
      "Batch 81: loss 10.429036646713445\n",
      "Batch 82: loss 10.42883244956412\n",
      "Batch 83: loss 10.424072012843856\n",
      "Batch 84: loss 10.419850826263428\n",
      "Batch 85: loss 10.418115458768957\n",
      "Batch 86: loss 10.415016041245572\n",
      "Batch 87: loss 10.409719215042289\n",
      "Batch 88: loss 10.411002560095353\n",
      "Batch 89: loss 10.40600055523133\n",
      "Batch 90: loss 10.402039813995362\n",
      "Batch 91: loss 10.39661778460492\n",
      "Batch 92: loss 10.393939018249512\n",
      "Batch 93: loss 10.38919033542756\n",
      "Batch 94: loss 10.385992222643914\n",
      "Batch 95: loss 10.388611110888029\n",
      "Batch 96: loss 10.385944416125616\n",
      "Batch 97: loss 10.38517685526425\n",
      "Batch 98: loss 10.382671580022695\n",
      "Batch 99: loss 10.379808050213438\n",
      "Batch 100: loss 10.378217315673828\n",
      "Batch 101: loss 10.377389756759795\n",
      "Batch 102: loss 10.372856158836216\n",
      "Batch 103: loss 10.372378284491381\n",
      "Batch 104: loss 10.371519473882822\n",
      "Batch 105: loss 10.370569356282552\n",
      "Batch 106: loss 10.367581907308326\n",
      "Batch 107: loss 10.367363974312756\n",
      "Batch 108: loss 10.363703003636113\n",
      "Batch 109: loss 10.361310626768095\n",
      "Epoch 2: Training loss 10.361310626768095\n",
      "Epoch 2: Validation loss 16.024741840362548 | Accuracy 97.66326530612244 | AUC 0.9990072028322028\n",
      "Best epoch:  2\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "mtrainer.run_train(2, support_loader, query_loader, lr=2e-5, full_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, query_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97.82653061224491, 0.9992765783681979, 14.54466733932495)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.model, query_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.model.attention, 'attention-model8h4l.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
