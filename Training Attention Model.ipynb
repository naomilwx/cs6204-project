{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utils.contrastive_loss import SupConLoss\n",
    "\n",
    "class LabelImageAttention(nn.Module):\n",
    "    def __init__(self, dim_in, n_head, dropout=0.1, num_layers=4, temperature=1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Transformer(dim_in, batch_first=True, nhead=n_head, dropout=dropout, num_decoder_layers=num_layers, num_encoder_layers=num_layers)\n",
    "        self.con_loss = SupConLoss(temperature=temperature)\n",
    "\n",
    "    def forward(self, texts, images, label_inds=None):\n",
    "        # transformer: (N, S, E), (N, T, E) -> (N, T, E)\n",
    "        # texts: (L,D) , images: (N,D,H,W), label_inds: (N, L)\n",
    "        texts = texts.expand(images.shape[0], -1, -1)\n",
    "        images = images.flatten(start_dim=2).permute(0, 2, 1)\n",
    "        mask = None\n",
    "        if label_inds is not None:\n",
    "            mask = 1 - label_inds\n",
    "        # Output: (N, L, D)\n",
    "        # Texts: NxLxD (decode)\n",
    "        # Mask irrelevant labels with tgt_key_padding_mask, set masked positions to True\n",
    "        # Images: Nx(HxW)xD\n",
    "        return self.attn(images, texts, tgt_key_padding_mask=mask)\n",
    "    \n",
    "    def loss(self, results, label_inds):\n",
    "        # results: (N, L, D), labels: (N, L)\n",
    "        classes = torch.nonzero(label_inds)[:,1]\n",
    "        prototypes = results[label_inds.bool()]\n",
    "        return self.con_loss(prototypes, classes)\n",
    "\n",
    "\n",
    "class LabelImagePrototypeModel(nn.Module):\n",
    "    def __init__(self, encoder, n_head, dim_in=512, dropout=0.1, num_layers=4, temperature=1):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.attention = LabelImageAttention(dim_in, n_head, dropout, num_layers, temperature)\n",
    "\n",
    "    def freeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_encoder(self):\n",
    "        self.encoder.img_model.proj.requires_grad = True\n",
    "        self.encoder.text_model.proj.requires_grad = True\n",
    "    \n",
    "    def forward(self, class_labels, images, label_inds):\n",
    "        text_embedding, image_emedding = self.encoder(class_labels, images, False)\n",
    "        prototypes = self.attention(text_embedding, image_emedding, label_inds)\n",
    "        return text_embedding, image_emedding, prototypes\n",
    "    \n",
    "    def attention_loss(self, prototypes, label_inds):\n",
    "        return self.attention.loss(prototypes, label_inds)\n",
    "    \n",
    "    def full_loss(self, text_emb, img_emb, label_inds, prototypes, weight=0.5):\n",
    "       return self.attention(prototypes, label_inds) + weight * self.encoder.loss(text_emb, img_emb, label_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from utils.metrics import AverageMeter, calculate_auc, multilabel_accuracy\n",
    "from utils.prototype import class_prototype_mean\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, class_labels, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.class_labels = class_labels\n",
    "    \n",
    "    def run_train(self, epochs, dataloader, val_dataloader, lr=1e-4, full_training=False):\n",
    "        model = self.model.to(self.device)\n",
    "        best_epoch = None\n",
    "        best_acc = None\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        if full_training:\n",
    "            model.unfreeze_encoder()\n",
    "        else:\n",
    "            model.freeze_encoder()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            loss_meter = AverageMeter()\n",
    "            for i, (images, class_inds) in enumerate(dataloader):\n",
    "                images, class_inds = images.to(self.device), class_inds.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                text_embeddings, image_embeddings, prototypes = model(self.class_labels, images, class_inds)\n",
    "                if full_training:\n",
    "                    loss = model.full_loss(text_embeddings, image_embeddings, class_inds, prototypes)\n",
    "                else:\n",
    "                    loss = model.attention_loss(prototypes, class_inds)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_meter.update(loss.item(), len(class_inds))\n",
    "                print(f\"Batch {i+1}: loss {loss_meter.average()}\")\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Training loss {loss_meter.average()}\")\n",
    "            val_acc, val_auc, val_loss = self.run_eval(model, val_dataloader)\n",
    "            print(f\"Epoch {epoch+1}: Validation loss {val_loss} | Accuracy {val_acc} | AUC {val_auc}\")\n",
    "            \n",
    "            if best_acc is None or val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                self.best_model = copy.deepcopy(model)\n",
    "                best_epoch = epoch\n",
    "        print('Best epoch: ', best_epoch+1)\n",
    "\n",
    "    def run_eval(self, model, dataloader, full_training=False):\n",
    "        model.eval()\n",
    "        model = model.to(self.device)\n",
    "        \n",
    "        loss_meter = AverageMeter()\n",
    "        auc_meter = AverageMeter()\n",
    "        acc_meter = AverageMeter()\n",
    "        with torch.no_grad():\n",
    "            for images, class_inds in dataloader:\n",
    "                images, class_inds = images.to(self.device), class_inds.to(self.device)\n",
    "                text_embeddings, _, prototypes = model(self.class_labels, images, class_inds)\n",
    "                class_prototypes = class_prototype_mean(prototypes)\n",
    "                logits_per_text, logits_per_image = model.encoder.compute_logits(text_embeddings, class_prototypes)\n",
    "\n",
    "                loss = model.attention_loss(prototypes, class_inds).item()\n",
    "                if full_training:\n",
    "                    loss += model.encoder.contrastive_logit_loss(logits_per_text, logits_per_image, class_inds).item()\n",
    "        \n",
    "                loss_meter.update(loss.item(), len(class_inds))\n",
    "\n",
    "                auc = calculate_auc(logits_per_image, class_inds)\n",
    "                auc_meter.update(auc, len(class_inds))\n",
    "                \n",
    "                acc = multilabel_accuracy(logits_per_image, class_inds)\n",
    "                acc_meter.update(acc, len(class_inds))\n",
    "        return acc_meter.average(), auc_meter.average(), loss_meter.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "from utils.device import get_device\n",
    "from utils.data import get_query_and_support_ids\n",
    "from models.embedding.dataset import Dataset\n",
    "\n",
    "img_info = pd.read_pickle('data/vindr_cxr_split_labels.pkl')\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(img_info, 'data/vindr_train_query_set.pkl')\n",
    "\n",
    "IMG_PATH = 'datasets/vindr-cxr-png'\n",
    "batch_size = 10*14\n",
    "query_dataset = Dataset(IMG_PATH, img_info, query_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "query_loader = DataLoader(dataset=query_dataset, batch_size=batch_size, shuffle=True)\n",
    "support_dataset = Dataset(IMG_PATH, img_info, support_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "support_loader = DataLoader(dataset=support_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "PROJ_SIZE = 512\n",
    "device = get_device()\n",
    "\n",
    "encoder = torch.load('imgtext_model_trained.pth')\n",
    "model = LabelImagePrototypeModel(encoder, 8, PROJ_SIZE)\n",
    "mtrainer = Trainer(model, support_dataset.class_labels(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrainer.run_train(5, support_loader, query_loader, lr=1e-4, full_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, query_loader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
