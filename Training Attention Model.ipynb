{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utils.contrastive_loss import SupConLoss\n",
    "\n",
    "def image_text_logits(text_embeddings, prototypes, scale=1):\n",
    "    # text_embeddings: (14, 512) x prototypes: (140, 14, 512) -> (140, 14)\n",
    "    fac = text_embeddings.unsqueeze(0).expand_as(prototypes)\n",
    "\n",
    "    return (fac * prototypes).sum(axis=2) * scale\n",
    "\n",
    "class LabelImageAttention(nn.Module):\n",
    "    def __init__(self, dim_in, n_head, dropout=0.1, num_layers=6, temperature=1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Transformer(dim_in, batch_first=True, nhead=n_head, dropout=dropout, num_decoder_layers=num_layers, num_encoder_layers=num_layers)\n",
    "        self.con_loss = SupConLoss(temperature=temperature, contrast_mode='one')\n",
    "        self.class_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, texts, images, label_inds=None):\n",
    "        # transformer: (N, S, E), (N, T, E) -> (N, T, E)\n",
    "        # texts: (L,D) , images: (N,D,H,W), label_inds: (N, L)\n",
    "        texts = texts.repeat(images.shape[0], 1, 1)\n",
    "        images = images.flatten(start_dim=2).permute(0, 2, 1)\n",
    "        mask = None\n",
    "        if label_inds is not None:\n",
    "            mask = (1 - label_inds).bool()\n",
    "        \n",
    "        # Texts: NxLxD (decode)\n",
    "        # Mask irrelevant labels with tgt_key_padding_mask, set masked positions to True\n",
    "        # Images: Nx(HxW)xD\n",
    "        # Output: (N, L, D)\n",
    "        out = self.attn(images, texts, tgt_key_padding_mask=mask)\n",
    "        return out / out.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    def loss(self, text_embeddings, prototypes, label_inds):\n",
    "        logits = image_text_logits(text_embeddings, prototypes)\n",
    "        return self.contrastive_loss(prototypes, label_inds) + self.classification_loss(logits, label_inds)\n",
    "    \n",
    "    def classification_loss(self, logits, label_inds):\n",
    "        return self.class_loss(logits, label_inds.float())\n",
    "    \n",
    "    def contrastive_loss(self, results, label_inds):\n",
    "        # results: (N, L, D), labels: (N, L)\n",
    "        classes = torch.nonzero(label_inds)[:,1] # (Np,)\n",
    "        prototypes = results[label_inds.bool()] # (Np, D)\n",
    "        return self.con_loss(prototypes.unsqueeze(1), classes)\n",
    "\n",
    "\n",
    "class LabelImagePrototypeModel(nn.Module):\n",
    "    def __init__(self, encoder, n_head, dim_in=512, dropout=0.1, num_layers=6, temperature=1):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.attention = LabelImageAttention(dim_in, n_head, dropout, num_layers, temperature)\n",
    "\n",
    "    def freeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_encoder(self):\n",
    "        self.encoder.img_model.proj.requires_grad = True\n",
    "        self.encoder.text_model.proj.requires_grad = True\n",
    "    \n",
    "    def forward(self, class_labels, images, label_inds):\n",
    "        text_embedding, image_emedding = self.encoder(class_labels, images, False)\n",
    "        prototypes = self.attention(text_embedding, image_emedding, label_inds)\n",
    "        return text_embedding, image_emedding, prototypes\n",
    "    \n",
    "    def attention_loss(self, text_embeddings, prototypes, label_inds):\n",
    "        return self.attention.loss(text_embeddings, prototypes, label_inds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from utils.metrics import AverageMeter, calculate_auc, multilabel_accuracy\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, class_labels, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.class_labels = class_labels\n",
    "    \n",
    "    def run_train(self, epochs, dataloader, val_dataloader, lr=1e-4, full_training=False):\n",
    "        model = self.model.to(self.device)\n",
    "        best_epoch = None\n",
    "        best_loss = None\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        if full_training:\n",
    "            model.unfreeze_encoder()\n",
    "        else:\n",
    "            model.freeze_encoder()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            loss_meter = AverageMeter()\n",
    "            for i, (images, class_inds) in enumerate(dataloader):\n",
    "                images, class_inds = images.to(self.device), class_inds.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                text_embeddings, _, prototypes = model(self.class_labels, images, class_inds)\n",
    "                if torch.any(prototypes.isnan()):\n",
    "                    print(prototypes)\n",
    "                    print('has nan')\n",
    "                loss = model.attention_loss(text_embeddings, prototypes, class_inds)\n",
    "                if full_training:\n",
    "                    logits_per_image = image_text_logits(text_embeddings, prototypes, model.encoder.get_logit_scale())\n",
    "                    loss += 0.5*model.encoder.contrastive_logit_loss(logits_per_image.t(), logits_per_image, class_inds)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_meter.update(loss.item(), len(class_inds))\n",
    "                print(f\"Batch {i+1}: loss {loss_meter.average()}\")\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Training loss {loss_meter.average()}\")\n",
    "\n",
    "            val_acc, val_auc, val_loss = self.run_eval(model, val_dataloader)\n",
    "            print(f\"Epoch {epoch+1}: Validation loss {val_loss} | Accuracy {val_acc} | AUC {val_auc}\")\n",
    "\n",
    "            if best_loss is None or val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                self.best_model = copy.deepcopy(model)\n",
    "                best_epoch = epoch\n",
    "        self.model = model\n",
    "        print('Best epoch: ', best_epoch+1)\n",
    "\n",
    "    def run_eval(self, model, dataloader, full_training=False):\n",
    "        model.eval()\n",
    "        model = model.to(self.device)\n",
    "        \n",
    "        loss_meter = AverageMeter()\n",
    "        auc_meter = AverageMeter()\n",
    "        acc_meter = AverageMeter()\n",
    "        with torch.no_grad():\n",
    "            for images, class_inds in dataloader:\n",
    "                images, class_inds = images.to(self.device), class_inds.to(self.device)\n",
    "                text_embeddings, _, prototypes = model(self.class_labels, images, class_inds)\n",
    "\n",
    "                logits_per_image = image_text_logits(text_embeddings, prototypes, model.encoder.get_logit_scale())\n",
    "                loss = model.attention.contrastive_loss(prototypes, class_inds).item()\n",
    "                loss += model.attention.classification_loss(logits_per_image, class_inds).item()\n",
    "                if full_training:\n",
    "                    loss += 0.5*model.encoder.contrastive_logit_loss(logits_per_image.t(), logits_per_image, class_inds).item()\n",
    "        \n",
    "                loss_meter.update(loss, len(class_inds))\n",
    "\n",
    "                auc = calculate_auc(logits_per_image, class_inds)\n",
    "                auc_meter.update(auc, len(class_inds))\n",
    "            \n",
    "                acc = multilabel_accuracy(logits_per_image, class_inds)\n",
    "                acc_meter.update(acc, len(class_inds))\n",
    "\n",
    "        return acc_meter.average(), auc_meter.average(), loss_meter.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "from utils.device import get_device\n",
    "from utils.data import get_query_and_support_ids\n",
    "from models.embedding.dataset import Dataset\n",
    "from utils.sampling import MultilabelBalancedRandomSampler\n",
    "\n",
    "img_info = pd.read_pickle('data/vindr_cxr_split_labels.pkl')\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(img_info, 'data/vindr_train_query_set.pkl')\n",
    "\n",
    "IMG_PATH = 'datasets/vindr-cxr-png'\n",
    "batch_size = 10*14\n",
    "\n",
    "query_dataset = Dataset(IMG_PATH, img_info, query_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "query_loader = DataLoader(dataset=query_dataset, batch_size=batch_size, shuffle=True)\n",
    "support_dataset = Dataset(IMG_PATH, img_info, support_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "support_loader = DataLoader(dataset=support_dataset, batch_size=batch_size, sampler=MultilabelBalancedRandomSampler(support_dataset.get_class_indicators()))\n",
    "\n",
    "PROJ_SIZE = 512\n",
    "# device = 'cpu'\n",
    "device =  get_device()\n",
    "\n",
    "encoder = torch.load('imgtext_model_trained.pth')\n",
    "encoder.text_model.device = device\n",
    "model = LabelImagePrototypeModel(encoder, 8, PROJ_SIZE, num_layers=4)\n",
    "mtrainer = Trainer(model, support_dataset.class_labels(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ny/2ylc61455nv8t8z2px9ppsqw0000gn/T/ipykernel_13516/3181858521.py:43: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  classes = torch.nonzero(label_inds)[:,1] # (Np,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 17.496978759765625\n",
      "Batch 2: loss 17.415132522583008\n",
      "Batch 3: loss 17.284144083658855\n",
      "Batch 4: loss 17.347036361694336\n",
      "Batch 5: loss 17.448635864257813\n",
      "Batch 6: loss 17.36430263519287\n",
      "Batch 7: loss 17.151103973388672\n",
      "Batch 8: loss 17.109638214111328\n",
      "Batch 9: loss 17.049114227294922\n",
      "Batch 10: loss 17.017132186889647\n",
      "Batch 11: loss 16.98420524597168\n",
      "Batch 12: loss 16.90559991200765\n",
      "Batch 13: loss 16.88465353158804\n",
      "Batch 14: loss 16.8862669808524\n",
      "Batch 15: loss 16.849417877197265\n",
      "Batch 16: loss 16.845712184906006\n",
      "Batch 17: loss 16.815390642951517\n",
      "Batch 18: loss 16.77667744954427\n",
      "Batch 19: loss 16.764832546836452\n",
      "Batch 20: loss 16.744179344177248\n",
      "Batch 21: loss 16.730347951253254\n",
      "Batch 22: loss 16.734111179005016\n",
      "Batch 23: loss 16.719174592391305\n",
      "Batch 24: loss 16.67940290768941\n",
      "Batch 25: loss 16.654409332275392\n",
      "Batch 26: loss 16.661652638362003\n",
      "Batch 27: loss 16.65251710679796\n",
      "Batch 28: loss 16.635266304016113\n",
      "Batch 29: loss 16.619599506772797\n",
      "Batch 30: loss 16.61156520843506\n",
      "Batch 31: loss 16.592023111158802\n",
      "Batch 32: loss 16.54849049448967\n",
      "Batch 33: loss 16.536676320162687\n",
      "Batch 34: loss 16.521241440492517\n",
      "Batch 35: loss 16.520795140947616\n",
      "Batch 36: loss 16.52151714430915\n",
      "Batch 37: loss 16.500438587085622\n",
      "Batch 38: loss 16.488143920898438\n",
      "Batch 39: loss 16.466876494578827\n",
      "Batch 40: loss 16.46108160018921\n",
      "Batch 41: loss 16.437363322188215\n",
      "Batch 42: loss 16.43332408723377\n",
      "Batch 43: loss 16.413748231045034\n",
      "Batch 44: loss 16.40063808181069\n",
      "Batch 45: loss 16.385558213127982\n",
      "Batch 46: loss 16.37415539700052\n",
      "Batch 47: loss 16.36019181190653\n",
      "Batch 48: loss 16.33603670199712\n",
      "Batch 49: loss 16.32383531453658\n",
      "Batch 50: loss 16.30610090255737\n",
      "Batch 51: loss 16.293129229078104\n",
      "Batch 52: loss 16.28626984816331\n",
      "Batch 53: loss 16.267988025017505\n",
      "Batch 54: loss 16.24912140104506\n",
      "Batch 55: loss 16.23688855604692\n",
      "Batch 56: loss 16.220539825303213\n",
      "Batch 57: loss 16.219205538431805\n",
      "Batch 58: loss 16.203102095373744\n",
      "Batch 59: loss 16.192266868332684\n",
      "Batch 60: loss 16.173954916000366\n",
      "Batch 61: loss 16.163194390593983\n",
      "Batch 62: loss 16.15243842524867\n",
      "Batch 63: loss 16.14168674226791\n",
      "Batch 64: loss 16.137686774134636\n",
      "Batch 65: loss 16.11632596529447\n",
      "Batch 66: loss 16.0964782743743\n",
      "Batch 67: loss 16.076809399163544\n",
      "Batch 68: loss 16.066521602518417\n",
      "Batch 69: loss 16.060277620951336\n",
      "Batch 70: loss 16.057416766030446\n",
      "Batch 71: loss 16.04538282206361\n",
      "Batch 72: loss 16.04561377896203\n",
      "Batch 73: loss 16.029314210970107\n",
      "Batch 74: loss 16.015718653395368\n",
      "Batch 75: loss 16.005929374694823\n",
      "Batch 76: loss 15.991694964860615\n",
      "Batch 77: loss 15.978720603051123\n",
      "Batch 78: loss 15.972218794700426\n",
      "Batch 79: loss 15.962035372287412\n",
      "Batch 80: loss 15.946674811840058\n",
      "Batch 81: loss 15.933814802287538\n",
      "Batch 82: loss 15.919063719307504\n",
      "Batch 83: loss 15.913712409605463\n",
      "Batch 84: loss 15.906058163869949\n",
      "Batch 85: loss 15.891652286753935\n",
      "Batch 86: loss 15.878426762514335\n",
      "Batch 87: loss 15.868351563640024\n",
      "Batch 88: loss 15.849013426087119\n",
      "Batch 89: loss 15.837879063038343\n",
      "Batch 90: loss 15.831114101409913\n",
      "Batch 91: loss 15.826726944891961\n",
      "Batch 92: loss 15.814755761105081\n",
      "Batch 93: loss 15.796734635547924\n",
      "Batch 94: loss 15.785259043916742\n",
      "Batch 95: loss 15.772269238923725\n",
      "Batch 96: loss 15.761906027793884\n",
      "Batch 97: loss 15.754678932661863\n",
      "Batch 98: loss 15.739327382068245\n",
      "Batch 99: loss 15.724437482429273\n",
      "Batch 100: loss 15.719464902877808\n",
      "Batch 101: loss 15.705313852517913\n",
      "Batch 102: loss 15.695795386445289\n",
      "Batch 103: loss 15.687132696503575\n",
      "Batch 104: loss 15.673880320328932\n",
      "Batch 105: loss 15.668542934599376\n",
      "Batch 106: loss 15.658813539540992\n",
      "Batch 107: loss 15.650915564777694\n",
      "Batch 108: loss 15.635326703389486\n",
      "Batch 109: loss 15.626323852839827\n",
      "Epoch 1: Training loss 15.626323852839827\n",
      "Epoch 1: Validation loss 15.733339977264404 | Accuracy 97.31632653061226 | AUC 0.9972962441324491\n",
      "Batch 1: loss 14.676203727722168\n",
      "Batch 2: loss 14.535718441009521\n",
      "Batch 3: loss 14.407486915588379\n",
      "Batch 4: loss 14.544005870819092\n",
      "Batch 5: loss 14.475189971923829\n",
      "Batch 6: loss 14.40248727798462\n",
      "Batch 7: loss 14.425190108163017\n",
      "Batch 8: loss 14.41597056388855\n",
      "Batch 9: loss 14.357040617201063\n",
      "Batch 10: loss 14.310597229003907\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "mtrainer.run_train(2, support_loader, query_loader, lr=5e-5, full_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34.75510204081633, 0.34221592378135657, 18.589404773712157)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, query_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ny/2ylc61455nv8t8z2px9ppsqw0000gn/T/ipykernel_13023/3181858521.py:43: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  classes = torch.nonzero(label_inds)[:,1] # (Np,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(34.755102040816325, 0.4901053936718201, 19.51178112030029)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.model, query_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.model.attention, 'attention-model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
