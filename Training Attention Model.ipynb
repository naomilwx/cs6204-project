{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utils.contrastive_loss import SupConLoss\n",
    "\n",
    "def image_text_logits(text_embeddings, prototypes, scale=1):\n",
    "    # text_embeddings: (14, 512) x prototypes: (140, 14, 512) -> (140, 14)\n",
    "    fac = text_embeddings.unsqueeze(0).expand_as(prototypes)\n",
    "    return (fac * prototypes).sum(axis=2) * scale\n",
    "\n",
    "class LabelImageAttention(nn.Module):\n",
    "    def __init__(self, dim_in, n_head, dropout=0.1, num_layers=6, temperature=1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Transformer(dim_in, batch_first=True, nhead=n_head, dropout=dropout, num_decoder_layers=num_layers, num_encoder_layers=num_layers)\n",
    "        self.con_loss = SupConLoss(temperature=temperature)\n",
    "\n",
    "    def forward(self, texts, images, label_inds=None):\n",
    "        # transformer: (N, S, E), (N, T, E) -> (N, T, E)\n",
    "        # texts: (L,D) , images: (N,D,H,W), label_inds: (N, L)\n",
    "        texts = texts.repeat(images.shape[0], 1, 1)\n",
    "        # view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
    "        images = images.flatten(start_dim=2).permute(0, 2, 1)\n",
    "        mask = None\n",
    "        if label_inds is not None:\n",
    "            mask = (1 - label_inds).bool()\n",
    "        \n",
    "        # Texts: NxLxD (decode)\n",
    "        # Mask irrelevant labels with tgt_key_padding_mask, set masked positions to True\n",
    "        # Images: Nx(HxW)xD\n",
    "        # Output: (N, L, D)\n",
    "        out = self.attn(images, texts, tgt_key_padding_mask=mask)\n",
    "        return out / out.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    def loss(self, results, label_inds):\n",
    "        # results: (N, L, D), labels: (N, L)\n",
    "        classes = torch.nonzero(label_inds)[:,1] # (Np,)\n",
    "        prototypes = results[label_inds.bool()] # (Np, D)\n",
    "        return self.con_loss(prototypes.unsqueeze(1), classes)\n",
    "\n",
    "\n",
    "class LabelImagePrototypeModel(nn.Module):\n",
    "    def __init__(self, encoder, n_head, dim_in=512, dropout=0.1, num_layers=4, temperature=1):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.attention = LabelImageAttention(dim_in, n_head, dropout, num_layers, temperature)\n",
    "\n",
    "    def freeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_encoder(self):\n",
    "        self.encoder.img_model.proj.requires_grad = True\n",
    "        self.encoder.text_model.proj.requires_grad = True\n",
    "    \n",
    "    def forward(self, class_labels, images, label_inds):\n",
    "        text_embedding, image_emedding = self.encoder(class_labels, images, False)\n",
    "        prototypes = self.attention(text_embedding, image_emedding, label_inds)\n",
    "        return text_embedding, image_emedding, prototypes\n",
    "    \n",
    "    def attention_loss(self, prototypes, label_inds):\n",
    "        return self.attention.loss(prototypes, label_inds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from utils.metrics import AverageMeter, calculate_auc, multilabel_accuracy\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, class_labels, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.class_labels = class_labels\n",
    "    \n",
    "    def run_train(self, epochs, dataloader, val_dataloader, lr=1e-4, full_training=False):\n",
    "        model = self.model.to(self.device)\n",
    "        best_epoch = None\n",
    "        best_loss = None\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        if full_training:\n",
    "            model.unfreeze_encoder()\n",
    "        else:\n",
    "            model.freeze_encoder()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            loss_meter = AverageMeter()\n",
    "            for i, (images, class_inds) in enumerate(dataloader):\n",
    "                images, class_inds = images.to(self.device), class_inds.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                text_embeddings, _, prototypes = model(self.class_labels, images, class_inds)\n",
    "                print('has nan', torch.any(prototypes.isnan()))\n",
    "                loss = model.attention_loss(prototypes, class_inds)\n",
    "                if full_training:\n",
    "                    logits_per_image = image_text_logits(text_embeddings, prototypes, model.encoder.get_logit_scale())\n",
    "                    loss += 0.5*model.encoder.contrastive_logit_loss(logits_per_image.t(), logits_per_image, class_inds)\n",
    "               \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_meter.update(loss.item(), len(class_inds))\n",
    "                print(f\"Batch {i+1}: loss {loss_meter.average()}\")\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Training loss {loss_meter.average()}\")\n",
    "\n",
    "            val_acc, val_auc, val_loss = self.run_eval(model, val_dataloader)\n",
    "            print(f\"Epoch {epoch+1}: Validation loss {val_loss} | Accuracy {val_acc} | AUC {val_auc}\")\n",
    "\n",
    "            if best_loss is None or val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                self.best_model = copy.deepcopy(model)\n",
    "                best_epoch = epoch\n",
    "        self.model = model\n",
    "        print('Best epoch: ', best_epoch+1)\n",
    "\n",
    "    def run_eval(self, model, dataloader, full_training=False):\n",
    "        model.eval()\n",
    "        model = model.to(self.device)\n",
    "        \n",
    "        loss_meter = AverageMeter()\n",
    "        auc_meter = AverageMeter()\n",
    "        acc_meter = AverageMeter()\n",
    "        with torch.no_grad():\n",
    "            for images, class_inds in dataloader:\n",
    "                images, class_inds = images.to(self.device), class_inds.to(self.device)\n",
    "                text_embeddings, _, prototypes = model(self.class_labels, images, class_inds)\n",
    "\n",
    "                loss = model.attention_loss(prototypes, class_inds).item()\n",
    "                if full_training:\n",
    "                    logits_per_image = image_text_logits(text_embeddings, prototypes, model.encoder.get_logit_scale())\n",
    "                    loss += 0.5*model.encoder.contrastive_logit_loss(logits_per_image.t(), logits_per_image, class_inds).item()\n",
    "        \n",
    "                loss_meter.update(loss, len(class_inds))\n",
    "\n",
    "                auc = calculate_auc(logits_per_image, class_inds)\n",
    "                auc_meter.update(auc, len(class_inds))\n",
    "            \n",
    "                acc = multilabel_accuracy(logits_per_image, class_inds)\n",
    "                acc_meter.update(acc, len(class_inds))\n",
    "\n",
    "        return loss_meter.average(), auc_meter.average(), loss_meter.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "from utils.device import get_device\n",
    "from utils.data import get_query_and_support_ids\n",
    "from models.embedding.dataset import Dataset\n",
    "\n",
    "img_info = pd.read_pickle('data/vindr_cxr_split_labels.pkl')\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(img_info, 'data/vindr_train_query_set.pkl')\n",
    "\n",
    "IMG_PATH = 'datasets/vindr-cxr-png'\n",
    "batch_size = 10*14\n",
    "query_dataset = Dataset(IMG_PATH, img_info, query_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "query_loader = DataLoader(dataset=query_dataset, batch_size=batch_size, shuffle=True)\n",
    "support_dataset = Dataset(IMG_PATH, img_info, support_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "support_loader = DataLoader(dataset=support_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "PROJ_SIZE = 512\n",
    "# device = 'cpu'\n",
    "device =  get_device()\n",
    "\n",
    "encoder = torch.load('imgtext_model_trained1-newlib.pth')\n",
    "encoder.text_model.device = device\n",
    "model = LabelImagePrototypeModel(encoder, 8, PROJ_SIZE)\n",
    "mtrainer = Trainer(model, support_dataset.class_labels(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "mtrainer.run_train(5, support_loader, query_loader, lr=1e-4, full_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(model, dataloader, class_labels, device, full_training=False):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    loss_meter = AverageMeter()\n",
    "    auc_meter = AverageMeter()\n",
    "    acc_meter = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for images, class_inds in dataloader:\n",
    "            images, class_inds = images.to(device), class_inds.to(device)\n",
    "            text_embeddings, _, prototypes = model(class_labels, images, class_inds)\n",
    "\n",
    "            loss = model.attention_loss(prototypes, class_inds).item()\n",
    "            if full_training:\n",
    "                logits_per_image = image_text_logits(text_embeddings, prototypes, model.encoder.get_logit_scale())\n",
    "                loss += 0.5*model.encoder.contrastive_logit_loss(logits_per_image.t(), logits_per_image, class_inds).item()\n",
    "    \n",
    "            loss_meter.update(loss, len(class_inds))\n",
    "            \n",
    "            auc = calculate_auc(logits_per_image, class_inds)\n",
    "            auc_meter.update(auc, len(class_inds))\n",
    "            \n",
    "            acc = multilabel_accuracy(logits_per_image, class_inds)\n",
    "            acc_meter.update(acc, len(class_inds))\n",
    "    return acc_meter.average(), auc_meter.average(), loss_meter.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(mtrainer.model, query_loader, mtrainer.class_labels, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, query_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
