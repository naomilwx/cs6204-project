{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from models.attention.model import LabelImageAttention, LabelImagePrototypeModel, LabelImageMHAttention\n",
    "from models.attention.trainer import Trainer\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "\n",
    "from utils.data import get_query_and_support_ids, DatasetConfig\n",
    "from utils.device import get_device\n",
    "from utils.data import get_query_and_support_ids\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT, VINDR_SPLIT2\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "from models.embedding.dataset import Dataset\n",
    "from utils.sampling import MultilabelBalancedRandomSampler\n",
    "from utils.f1_loss import BalAccuracyLoss\n",
    "\n",
    "\n",
    "configs = {\n",
    "    'vindr1': DatasetConfig('datasets/vindr-cxr-png', 'data/vindr_cxr_split_labels.pkl', 'data/vindr_train_query_set.pkl', VINDR_CXR_LABELS, VINDR_SPLIT, MEAN_STDS['chestmnist']),\n",
    "    'vindr2': DatasetConfig('datasets/vindr-cxr-png', 'data/vindr_cxr_split_labels2.pkl', 'data/vindr_train_query_set2.pkl', VINDR_CXR_LABELS, VINDR_SPLIT2, MEAN_STDS['chestmnist'])\n",
    "}\n",
    "\n",
    "config = configs['vindr2']\n",
    "\n",
    "batch_size = 10*14\n",
    "\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(config.img_info, config.training_split_path)\n",
    "query_dataset = Dataset(config.img_path, config.img_info, query_image_ids, config.label_names_map, config.classes_split_map['train'], mean_std=config.mean_std)\n",
    "query_loader = DataLoader(dataset=query_dataset, batch_size=batch_size, shuffle=True)\n",
    "support_dataset = Dataset(config.img_path, config.img_info, support_image_ids, config.label_names_map, config.classes_split_map['train'], mean_std=config.mean_std)\n",
    "support_loader = DataLoader(dataset=support_dataset, batch_size=batch_size, sampler=MultilabelBalancedRandomSampler(support_dataset.get_class_indicators()))\n",
    "\n",
    "\n",
    "PROJ_SIZE = 512\n",
    "device =  get_device()\n",
    "\n",
    "encoder = torch.load('models/embedding/model/vindr2/imgtext_model_trained1.pth')\n",
    "encoder.text_model.device = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_model = LabelImageMHAttention(PROJ_SIZE, 8, cls_weight=5, cls_loss=BalAccuracyLoss(), device=device)\n",
    "# attn_model = torch.load('models/attention/model/vindr2/attention-8h.pth')\n",
    "model = LabelImagePrototypeModel(encoder, 8, PROJ_SIZE, attn_model=attn_model)\n",
    "mtrainer = Trainer(model, support_dataset.class_labels(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LabelImagePrototypeModel(encoder, 8, PROJ_SIZE, num_layers=4, cls_weight=5, cls_loss=BalAccuracyLoss())\n",
    "mtrainer = Trainer(model, support_dataset.class_labels(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_model = torch.load('models/attention/model/vindr2/attention-model8h4l.pth')\n",
    "model = LabelImagePrototypeModel(encoder, 8, PROJ_SIZE, attn_model=attn_model)\n",
    "mtrainer = Trainer(model, support_dataset.class_labels(), device)\n",
    "\n",
    "mtrainer.run_train(10, support_loader, query_loader, lr=2e-5, encoder_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 8.660057067871094\n",
      "Batch 2: loss 8.663658618927002\n",
      "Batch 3: loss 8.644646644592285\n",
      "Batch 4: loss 8.623297691345215\n",
      "Batch 5: loss 8.601280212402344\n",
      "Batch 6: loss 8.568521658579508\n",
      "Batch 7: loss 8.551353318350655\n",
      "Batch 8: loss 8.525346159934998\n",
      "Batch 9: loss 8.504292594061958\n",
      "Batch 10: loss 8.486051654815673\n",
      "Batch 11: loss 8.464292006059127\n",
      "Batch 12: loss 8.448318401972452\n",
      "Batch 13: loss 8.429268250098595\n",
      "Batch 14: loss 8.41521692276001\n",
      "Batch 15: loss 8.401623344421386\n",
      "Batch 16: loss 8.39047235250473\n",
      "Batch 17: loss 8.377981802996468\n",
      "Batch 18: loss 8.365034421284994\n",
      "Batch 19: loss 8.354037033884149\n",
      "Batch 20: loss 8.342796993255615\n",
      "Batch 21: loss 8.331245376950218\n",
      "Batch 22: loss 8.321566061540084\n",
      "Batch 23: loss 8.313089204871137\n",
      "Batch 24: loss 8.304989377657572\n",
      "Batch 25: loss 8.29705753326416\n",
      "Batch 26: loss 8.288236764761118\n",
      "Batch 27: loss 8.279079896432382\n",
      "Batch 28: loss 8.266863226890564\n",
      "Batch 29: loss 8.255518156906653\n",
      "Batch 30: loss 8.244403266906739\n",
      "Batch 31: loss 8.233300885846537\n",
      "Batch 32: loss 8.222580194473267\n",
      "Batch 33: loss 8.210947658076432\n",
      "Batch 34: loss 8.199256602455588\n",
      "Batch 35: loss 8.187725925445557\n",
      "Batch 36: loss 8.177734282281664\n",
      "Batch 37: loss 8.167101872933877\n",
      "Batch 38: loss 8.158253908157349\n",
      "Batch 39: loss 8.149379510145922\n",
      "Batch 40: loss 8.141200065612793\n",
      "Batch 41: loss 8.132252332640856\n",
      "Batch 42: loss 8.124156679425921\n",
      "Batch 43: loss 8.11597378309383\n",
      "Batch 44: loss 8.108572006225586\n",
      "Batch 45: loss 8.101325692070855\n",
      "Batch 46: loss 8.094536066055298\n",
      "Batch 47: loss 8.086988743315352\n",
      "Batch 48: loss 8.080919524033865\n",
      "Batch 49: loss 8.074733685474007\n",
      "Batch 50: loss 8.068472509384156\n",
      "Batch 51: loss 8.062082795535817\n",
      "Batch 52: loss 8.056455227044912\n",
      "Batch 53: loss 8.050249387633126\n",
      "Batch 54: loss 8.044487997337624\n",
      "Batch 55: loss 8.038542626120828\n",
      "Batch 56: loss 8.033566398280007\n",
      "Batch 57: loss 8.028099520164623\n",
      "Batch 58: loss 8.022437070978098\n",
      "Batch 59: loss 8.017387826563949\n",
      "Batch 60: loss 8.01218679745992\n",
      "Batch 61: loss 8.007465745581955\n",
      "Batch 62: loss 8.002264868828558\n",
      "Batch 63: loss 7.997702000633119\n",
      "Batch 64: loss 7.993388004601002\n",
      "Batch 65: loss 7.988232421875\n",
      "Batch 66: loss 7.983703302614616\n",
      "Batch 67: loss 7.979644035225484\n",
      "Batch 68: loss 7.9755250215530396\n",
      "Batch 69: loss 7.971308535423832\n",
      "Batch 70: loss 7.967980820792062\n",
      "Batch 71: loss 7.963500828810141\n",
      "Batch 72: loss 7.959463735421498\n",
      "Batch 73: loss 7.955230503866117\n",
      "Batch 74: loss 7.951711087613492\n",
      "Batch 75: loss 7.947438793182373\n",
      "Batch 76: loss 7.943921189559133\n",
      "Batch 77: loss 7.94014818018133\n",
      "Batch 78: loss 7.936051753851084\n",
      "Batch 79: loss 7.932364192190049\n",
      "Batch 80: loss 7.9289003789424894\n",
      "Batch 81: loss 7.9251985844270685\n",
      "Batch 82: loss 7.921441892298256\n",
      "Batch 83: loss 7.918102597615805\n",
      "Batch 84: loss 7.914560618854704\n",
      "Batch 85: loss 7.911680574978099\n",
      "Batch 86: loss 7.908030232717825\n",
      "Batch 87: loss 7.904721281994348\n",
      "Batch 88: loss 7.9018777067011055\n",
      "Batch 89: loss 7.898853741335065\n",
      "Batch 90: loss 7.8957155121697316\n",
      "Batch 91: loss 7.892485094594432\n",
      "Batch 92: loss 7.889659917872885\n",
      "Batch 93: loss 7.886989357650921\n",
      "Batch 94: loss 7.884331038657655\n",
      "Batch 95: loss 7.881145969190095\n",
      "Batch 96: loss 7.878057156999906\n",
      "Batch 97: loss 7.874903310205519\n",
      "Batch 98: loss 7.872268525921569\n",
      "Batch 99: loss 7.869386003474997\n",
      "Batch 100: loss 7.866748681068421\n",
      "Batch 101: loss 7.864146822749978\n",
      "Batch 102: loss 7.861352336172964\n",
      "Batch 103: loss 7.858930634063425\n",
      "Batch 104: loss 7.854199619620954\n",
      "Epoch 1: Training loss 7.854199619620954\n",
      "Epoch 1: Validation loss 8.280400428771973 | Accuracy 0.616 | AUC 0.6285515983397613 | Acc H-Mean 0.6076797360695895 | Spec 0.6055672478675842 | Recall 0.609807014465332\n",
      "Batch 1: loss 7.5503830909729\n",
      "Batch 2: loss 7.57234525680542\n",
      "Batch 3: loss 7.589254379272461\n",
      "Batch 4: loss 7.581349968910217\n",
      "Batch 5: loss 7.58240966796875\n",
      "Batch 6: loss 7.581210295359294\n",
      "Batch 7: loss 7.583674498966762\n",
      "Batch 8: loss 7.586053013801575\n",
      "Batch 9: loss 7.591014014350043\n",
      "Batch 10: loss 7.590662002563477\n",
      "Batch 11: loss 7.588062719865278\n",
      "Batch 12: loss 7.585563659667969\n",
      "Batch 13: loss 7.5813578092134914\n",
      "Batch 14: loss 7.581090245928083\n",
      "Batch 15: loss 7.58230349222819\n",
      "Batch 16: loss 7.578905373811722\n",
      "Batch 17: loss 7.577660083770752\n",
      "Batch 18: loss 7.575341251161364\n",
      "Batch 19: loss 7.573300160859761\n",
      "Batch 20: loss 7.5727681636810305\n",
      "Batch 21: loss 7.570130802336193\n",
      "Batch 22: loss 7.568753719329834\n",
      "Batch 23: loss 7.567862800929857\n",
      "Batch 24: loss 7.56829818089803\n",
      "Batch 25: loss 7.569081707000732\n",
      "Batch 26: loss 7.5685413250556355\n",
      "Batch 27: loss 7.566933773182057\n",
      "Batch 28: loss 7.566332680838449\n",
      "Batch 29: loss 7.566324842387233\n",
      "Batch 30: loss 7.567041476567586\n",
      "Batch 31: loss 7.567010248861005\n",
      "Batch 32: loss 7.5661942064762115\n",
      "Batch 33: loss 7.56660608811812\n",
      "Batch 34: loss 7.56576748455272\n",
      "Batch 35: loss 7.564213957105364\n",
      "Batch 36: loss 7.563878734906514\n",
      "Batch 37: loss 7.561939729226602\n",
      "Batch 38: loss 7.561273825796027\n",
      "Batch 39: loss 7.561037968366574\n",
      "Batch 40: loss 7.559985399246216\n",
      "Batch 41: loss 7.55855982478072\n",
      "Batch 42: loss 7.558469681512742\n",
      "Batch 43: loss 7.556526472402173\n",
      "Batch 44: loss 7.5549235777421435\n",
      "Batch 45: loss 7.553638034396702\n",
      "Batch 46: loss 7.552680616793425\n",
      "Batch 47: loss 7.552275880854181\n",
      "Batch 48: loss 7.55183869600296\n",
      "Batch 49: loss 7.552232966131093\n",
      "Batch 50: loss 7.5523781490325925\n",
      "Batch 51: loss 7.551546087452009\n",
      "Batch 52: loss 7.5518219746076145\n",
      "Batch 53: loss 7.551129098208445\n",
      "Batch 54: loss 7.549704119011208\n",
      "Batch 55: loss 7.549060301347212\n",
      "Batch 56: loss 7.548809230327606\n",
      "Batch 57: loss 7.548602070724755\n",
      "Batch 58: loss 7.548334976722455\n",
      "Batch 59: loss 7.548003406847938\n",
      "Batch 60: loss 7.546429427464803\n",
      "Batch 61: loss 7.545289930750112\n",
      "Batch 62: loss 7.545026848393102\n",
      "Batch 63: loss 7.544260585118854\n",
      "Batch 64: loss 7.544247061014175\n",
      "Batch 65: loss 7.543814351008488\n",
      "Batch 66: loss 7.543966481179902\n",
      "Batch 67: loss 7.5440084685140585\n",
      "Batch 68: loss 7.543235905030194\n",
      "Batch 69: loss 7.543149478193643\n",
      "Batch 70: loss 7.543326255253383\n",
      "Batch 71: loss 7.542649121351645\n",
      "Batch 72: loss 7.5423593454890785\n",
      "Batch 73: loss 7.542309597746967\n",
      "Batch 74: loss 7.542108232910569\n",
      "Batch 75: loss 7.5413392384847\n",
      "Batch 76: loss 7.540014273241947\n",
      "Batch 77: loss 7.540184665035892\n",
      "Batch 78: loss 7.539764538789407\n",
      "Batch 79: loss 7.539743978765946\n",
      "Batch 80: loss 7.5397084057331085\n",
      "Batch 81: loss 7.538727089210793\n",
      "Batch 82: loss 7.537831364608392\n",
      "Batch 83: loss 7.537678506000932\n",
      "Batch 84: loss 7.537653491610572\n",
      "Batch 85: loss 7.536986687604119\n",
      "Batch 86: loss 7.536771081214727\n",
      "Batch 87: loss 7.53635137382595\n",
      "Batch 88: loss 7.53582591902126\n",
      "Batch 89: loss 7.535480359966836\n",
      "Batch 90: loss 7.5354397985670305\n",
      "Batch 91: loss 7.534590616330996\n",
      "Batch 92: loss 7.53450157849685\n",
      "Batch 93: loss 7.534374877970706\n",
      "Batch 94: loss 7.533581160484476\n",
      "Batch 95: loss 7.533292253393876\n",
      "Batch 96: loss 7.533303240935008\n",
      "Batch 97: loss 7.532702529553285\n",
      "Batch 98: loss 7.532326357705252\n",
      "Batch 99: loss 7.531790511776703\n",
      "Batch 100: loss 7.531684999465942\n",
      "Batch 101: loss 7.530894855461498\n",
      "Batch 102: loss 7.530308162464815\n",
      "Batch 103: loss 7.5297977947494354\n",
      "Batch 104: loss 7.526153798018107\n",
      "Epoch 2: Training loss 7.526153798018107\n",
      "Epoch 2: Validation loss 8.308079528808594 | Accuracy 0.6384000000000001 | AUC 0.643921926968585 | Acc H-Mean 0.6145652569247574 | Spec 0.6657601356506347 | Recall 0.5706816196441651\n",
      "Batch 1: loss 7.489552974700928\n",
      "Batch 2: loss 7.493065118789673\n",
      "Batch 3: loss 7.51038122177124\n",
      "Batch 4: loss 7.514781951904297\n",
      "Batch 5: loss 7.527497959136963\n",
      "Batch 6: loss 7.539088884989421\n",
      "Batch 7: loss 7.530874592917306\n",
      "Batch 8: loss 7.528064668178558\n",
      "Batch 9: loss 7.523493078019884\n",
      "Batch 10: loss 7.520308017730713\n",
      "Batch 11: loss 7.518346006220037\n",
      "Batch 12: loss 7.519483049710591\n",
      "Batch 13: loss 7.5203986167907715\n",
      "Batch 14: loss 7.521874836512974\n",
      "Batch 15: loss 7.522830295562744\n",
      "Batch 16: loss 7.519916921854019\n",
      "Batch 17: loss 7.517606987672694\n",
      "Batch 18: loss 7.515095313390096\n",
      "Batch 19: loss 7.516233845760948\n",
      "Batch 20: loss 7.513226771354676\n",
      "Batch 21: loss 7.512644041152227\n",
      "Batch 22: loss 7.511650627309626\n",
      "Batch 23: loss 7.511895138284435\n",
      "Batch 24: loss 7.511207302411397\n",
      "Batch 25: loss 7.511424560546875\n",
      "Batch 26: loss 7.51152176123399\n",
      "Batch 27: loss 7.5094316623829025\n",
      "Batch 28: loss 7.509194595473153\n",
      "Batch 29: loss 7.507336189006937\n",
      "Batch 30: loss 7.506620677312215\n",
      "Batch 31: loss 7.505186065550773\n",
      "Batch 32: loss 7.506497040390968\n",
      "Batch 33: loss 7.506194331429222\n",
      "Batch 34: loss 7.504193965126486\n",
      "Batch 35: loss 7.501821967533656\n",
      "Batch 36: loss 7.5010879966947765\n",
      "Batch 37: loss 7.500120639801025\n",
      "Batch 38: loss 7.499080745797408\n",
      "Batch 39: loss 7.496836124322353\n",
      "Batch 40: loss 7.496486735343933\n",
      "Batch 41: loss 7.494477655829453\n",
      "Batch 42: loss 7.494500137510753\n",
      "Batch 43: loss 7.495030613832696\n",
      "Batch 44: loss 7.493510907346552\n",
      "Batch 45: loss 7.4935294999016655\n",
      "Batch 46: loss 7.493665726288505\n",
      "Batch 47: loss 7.493582928434331\n",
      "Batch 48: loss 7.49344989657402\n",
      "Batch 49: loss 7.492465583645568\n",
      "Batch 50: loss 7.491957330703736\n",
      "Batch 51: loss 7.490959924810073\n",
      "Batch 52: loss 7.491192001562852\n",
      "Batch 53: loss 7.490604796499576\n",
      "Batch 54: loss 7.4894866501843484\n",
      "Batch 55: loss 7.4893646673722705\n",
      "Batch 56: loss 7.490041179316385\n",
      "Batch 57: loss 7.490424147823401\n",
      "Batch 58: loss 7.489647750196786\n",
      "Batch 59: loss 7.489881143731586\n",
      "Batch 60: loss 7.489610322316488\n",
      "Batch 61: loss 7.4892841401647345\n",
      "Batch 62: loss 7.488666495969219\n",
      "Batch 63: loss 7.48829370074802\n",
      "Batch 64: loss 7.487707108259201\n",
      "Batch 65: loss 7.487204133547269\n",
      "Batch 66: loss 7.486860997749098\n",
      "Batch 67: loss 7.48612438031097\n",
      "Batch 68: loss 7.486381278318517\n",
      "Batch 69: loss 7.485372971797335\n",
      "Batch 70: loss 7.48509396144322\n",
      "Batch 71: loss 7.484120489845814\n",
      "Batch 72: loss 7.4836115969551935\n",
      "Batch 73: loss 7.483088859140056\n",
      "Batch 74: loss 7.481560165817673\n",
      "Batch 75: loss 7.480451488494873\n",
      "Batch 76: loss 7.479753180554039\n",
      "Batch 77: loss 7.479651395376626\n",
      "Batch 78: loss 7.479389160107344\n",
      "Batch 79: loss 7.478436343277557\n",
      "Batch 80: loss 7.478167194128036\n",
      "Batch 81: loss 7.477661962862368\n",
      "Batch 82: loss 7.4777737245327085\n",
      "Batch 83: loss 7.478080450770366\n",
      "Batch 84: loss 7.477541673751104\n",
      "Batch 85: loss 7.477386973885929\n",
      "Batch 86: loss 7.477452788242074\n",
      "Batch 87: loss 7.477523113119191\n",
      "Batch 88: loss 7.477164013819261\n",
      "Batch 89: loss 7.4765909548555864\n",
      "Batch 90: loss 7.476820728513929\n",
      "Batch 91: loss 7.475913283589122\n",
      "Batch 92: loss 7.475678438725679\n",
      "Batch 93: loss 7.475534146831881\n",
      "Batch 94: loss 7.474770784378052\n",
      "Batch 95: loss 7.474018603877018\n",
      "Batch 96: loss 7.473140517870585\n",
      "Batch 97: loss 7.4728661222556205\n",
      "Batch 98: loss 7.472198695552592\n",
      "Batch 99: loss 7.471363197673451\n",
      "Batch 100: loss 7.470871396064759\n",
      "Batch 101: loss 7.470372827926485\n",
      "Batch 102: loss 7.469594436533311\n",
      "Batch 103: loss 7.4692505077250955\n",
      "Batch 104: loss 7.465340461274225\n",
      "Epoch 3: Training loss 7.465340461274225\n",
      "Epoch 3: Validation loss 8.339313926696777 | Accuracy 0.6582 | AUC 0.6333946636377552 | Acc H-Mean 0.6030352947320452 | Spec 0.7157111024856567 | Recall 0.521011438369751\n",
      "Batch 1: loss 7.413235664367676\n",
      "Batch 2: loss 7.401616811752319\n",
      "Batch 3: loss 7.416615168253581\n",
      "Batch 4: loss 7.419280767440796\n",
      "Batch 5: loss 7.430337715148926\n",
      "Batch 6: loss 7.431021451950073\n",
      "Batch 7: loss 7.427042143685477\n",
      "Batch 8: loss 7.421539485454559\n",
      "Batch 9: loss 7.419911596510145\n",
      "Batch 10: loss 7.413668155670166\n",
      "Batch 11: loss 7.411255836486816\n",
      "Batch 12: loss 7.410210808118184\n",
      "Batch 13: loss 7.416274547576904\n",
      "Batch 14: loss 7.418667282376971\n",
      "Batch 15: loss 7.416829713185629\n",
      "Batch 16: loss 7.414480447769165\n",
      "Batch 17: loss 7.414838678696576\n",
      "Batch 18: loss 7.411882744895087\n",
      "Batch 19: loss 7.409535834663792\n",
      "Batch 20: loss 7.409243655204773\n",
      "Batch 21: loss 7.412289551326206\n",
      "Batch 22: loss 7.411381873217496\n",
      "Batch 23: loss 7.4116674713466475\n",
      "Batch 24: loss 7.411738197008769\n",
      "Batch 25: loss 7.409430007934571\n",
      "Batch 26: loss 7.407128572463989\n",
      "Batch 27: loss 7.406424593042444\n",
      "Batch 28: loss 7.405438934053693\n",
      "Batch 29: loss 7.404314863270726\n",
      "Batch 30: loss 7.404252052307129\n",
      "Batch 31: loss 7.404340082599271\n",
      "Batch 32: loss 7.4041266441345215\n",
      "Batch 33: loss 7.406542026635372\n",
      "Batch 34: loss 7.406301610610065\n",
      "Batch 35: loss 7.406739916120257\n",
      "Batch 36: loss 7.405461284849379\n",
      "Batch 37: loss 7.4049368935662345\n",
      "Batch 38: loss 7.405301031313445\n",
      "Batch 39: loss 7.405122879223946\n",
      "Batch 40: loss 7.4050798416137695\n",
      "Batch 41: loss 7.404754731713272\n",
      "Batch 42: loss 7.4046585219247\n",
      "Batch 43: loss 7.4034641509832335\n",
      "Batch 44: loss 7.403351469473406\n",
      "Batch 45: loss 7.403879748450385\n",
      "Batch 46: loss 7.403487495754076\n",
      "Batch 47: loss 7.403653347745855\n",
      "Batch 48: loss 7.402961273988088\n",
      "Batch 49: loss 7.403252562698053\n",
      "Batch 50: loss 7.4027134037017825\n",
      "Batch 51: loss 7.403025879579432\n",
      "Batch 52: loss 7.402485755773691\n",
      "Batch 53: loss 7.402162515892173\n",
      "Batch 54: loss 7.401222476252803\n",
      "Batch 55: loss 7.400893766229803\n",
      "Batch 56: loss 7.4020116073744635\n",
      "Batch 57: loss 7.403330242424681\n",
      "Batch 58: loss 7.403183863080781\n",
      "Batch 59: loss 7.4027501930624755\n",
      "Batch 60: loss 7.402122259140015\n",
      "Batch 61: loss 7.401986137765353\n",
      "Batch 62: loss 7.401607605718797\n",
      "Batch 63: loss 7.401456068432521\n",
      "Batch 64: loss 7.4007871970534325\n",
      "Batch 65: loss 7.400557767427885\n",
      "Batch 66: loss 7.400360381964481\n",
      "Batch 67: loss 7.400849377931054\n",
      "Batch 68: loss 7.400487563189338\n",
      "Batch 69: loss 7.40099262845689\n",
      "Batch 70: loss 7.400755671092442\n",
      "Batch 71: loss 7.40029891779725\n",
      "Batch 72: loss 7.399762663576338\n",
      "Batch 73: loss 7.3990373284849404\n",
      "Batch 74: loss 7.400509183471267\n",
      "Batch 75: loss 7.399724089304606\n",
      "Batch 76: loss 7.400260429633291\n",
      "Batch 77: loss 7.399464347145774\n",
      "Batch 78: loss 7.399431864420573\n",
      "Batch 79: loss 7.399477367159687\n",
      "Batch 80: loss 7.399246668815612\n",
      "Batch 81: loss 7.399018505473196\n",
      "Batch 82: loss 7.3985579130126204\n",
      "Batch 83: loss 7.398024300494826\n",
      "Batch 84: loss 7.398446435020084\n",
      "Batch 85: loss 7.398126388998593\n",
      "Batch 86: loss 7.397794961929321\n",
      "Batch 87: loss 7.396948189570986\n",
      "Batch 88: loss 7.396748965436762\n",
      "Batch 89: loss 7.396321071667618\n",
      "Batch 90: loss 7.3956261422899034\n",
      "Batch 91: loss 7.394967928037539\n",
      "Batch 92: loss 7.395165961721669\n",
      "Batch 93: loss 7.394634072498609\n",
      "Batch 94: loss 7.394355986980682\n",
      "Batch 95: loss 7.394416166606702\n",
      "Batch 96: loss 7.393733620643616\n",
      "Batch 97: loss 7.393396279246537\n",
      "Batch 98: loss 7.392767035231298\n",
      "Batch 99: loss 7.39250931595311\n",
      "Batch 100: loss 7.392242212295532\n",
      "Batch 101: loss 7.392243328661022\n",
      "Batch 102: loss 7.392030804764991\n",
      "Batch 103: loss 7.391894419216415\n",
      "Batch 104: loss 7.388817239618805\n",
      "Epoch 4: Training loss 7.388817239618805\n",
      "Epoch 4: Validation loss 8.21075668334961 | Accuracy 0.6552 | AUC 0.6542844526412945 | Acc H-Mean 0.624450072516589 | Spec 0.6674717140197753 | Recall 0.5866385054588318\n",
      "Batch 1: loss 7.360260009765625\n",
      "Batch 2: loss 7.377233982086182\n",
      "Batch 3: loss 7.3760145505269366\n",
      "Batch 4: loss 7.382045388221741\n",
      "Batch 5: loss 7.3743291854858395\n",
      "Batch 6: loss 7.37347428003947\n",
      "Batch 7: loss 7.367578165871756\n",
      "Batch 8: loss 7.362820029258728\n",
      "Batch 9: loss 7.3631235758463545\n",
      "Batch 10: loss 7.368639135360718\n",
      "Batch 11: loss 7.370619427074086\n",
      "Batch 12: loss 7.371067484219869\n",
      "Batch 13: loss 7.371692620790922\n",
      "Batch 14: loss 7.370868989399502\n",
      "Batch 15: loss 7.369566122690837\n",
      "Batch 16: loss 7.368566542863846\n",
      "Batch 17: loss 7.365093231201172\n",
      "Batch 18: loss 7.366164180967543\n",
      "Batch 19: loss 7.368684843966835\n",
      "Batch 20: loss 7.373308491706848\n",
      "Batch 21: loss 7.372082437787737\n",
      "Batch 22: loss 7.373530973087657\n",
      "Batch 23: loss 7.374756584996763\n",
      "Batch 24: loss 7.3765217661857605\n",
      "Batch 25: loss 7.376670131683349\n",
      "Batch 26: loss 7.37434275333698\n",
      "Batch 27: loss 7.373336862634729\n",
      "Batch 28: loss 7.373521634510586\n",
      "Batch 29: loss 7.372788495030896\n",
      "Batch 30: loss 7.372927331924439\n",
      "Batch 31: loss 7.372866738227106\n",
      "Batch 32: loss 7.370487958192825\n",
      "Batch 33: loss 7.3719109766411055\n",
      "Batch 34: loss 7.371270614511826\n",
      "Batch 35: loss 7.371853392464774\n",
      "Batch 36: loss 7.371657980812921\n",
      "Batch 37: loss 7.372722239107699\n",
      "Batch 38: loss 7.373098938088668\n",
      "Batch 39: loss 7.373349923353929\n",
      "Batch 40: loss 7.373304724693298\n",
      "Batch 41: loss 7.372693410733851\n",
      "Batch 42: loss 7.372895127251034\n",
      "Batch 43: loss 7.373468099638473\n",
      "Batch 44: loss 7.373156352476641\n",
      "Batch 45: loss 7.373258442348904\n",
      "Batch 46: loss 7.372214814890986\n",
      "Batch 47: loss 7.371544939406375\n",
      "Batch 48: loss 7.371337334314982\n",
      "Batch 49: loss 7.371693017531414\n",
      "Batch 50: loss 7.370986137390137\n",
      "Batch 51: loss 7.370444765277937\n",
      "Batch 52: loss 7.3712546825408936\n",
      "Batch 53: loss 7.370567213814214\n",
      "Batch 54: loss 7.371453973982069\n",
      "Batch 55: loss 7.3716290820728645\n",
      "Batch 56: loss 7.370829769543239\n",
      "Batch 57: loss 7.371048785092538\n",
      "Batch 58: loss 7.371693693358322\n",
      "Batch 59: loss 7.372211569446629\n",
      "Batch 60: loss 7.371969930330912\n",
      "Batch 61: loss 7.371234862530818\n",
      "Batch 62: loss 7.372122903023997\n",
      "Batch 63: loss 7.3717049189976285\n",
      "Batch 64: loss 7.371903210878372\n",
      "Batch 65: loss 7.37165913948646\n",
      "Batch 66: loss 7.3714418555751\n",
      "Batch 67: loss 7.370937361646054\n",
      "Batch 68: loss 7.371117325390086\n",
      "Batch 69: loss 7.370965232019839\n",
      "Batch 70: loss 7.370934547696795\n",
      "Batch 71: loss 7.3701146286977846\n",
      "Batch 72: loss 7.370682670010461\n",
      "Batch 73: loss 7.369885836562065\n",
      "Batch 74: loss 7.3699560358717635\n",
      "Batch 75: loss 7.370535348256429\n",
      "Batch 76: loss 7.37093618669008\n",
      "Batch 77: loss 7.370098479382404\n",
      "Batch 78: loss 7.370022847102239\n",
      "Batch 79: loss 7.370008951501001\n",
      "Batch 80: loss 7.37077648639679\n",
      "Batch 81: loss 7.370858451466502\n",
      "Batch 82: loss 7.370849068571881\n",
      "Batch 83: loss 7.370706782283553\n",
      "Batch 84: loss 7.370486367316473\n",
      "Batch 85: loss 7.370730327157413\n",
      "Batch 86: loss 7.370559908622919\n",
      "Batch 87: loss 7.3701480613357715\n",
      "Batch 88: loss 7.369489295916124\n",
      "Batch 89: loss 7.3690017153707785\n",
      "Batch 90: loss 7.368621434105767\n",
      "Batch 91: loss 7.3682179398589085\n",
      "Batch 92: loss 7.36762547492981\n",
      "Batch 93: loss 7.36751921971639\n",
      "Batch 94: loss 7.368110154537445\n",
      "Batch 95: loss 7.368129850688733\n",
      "Batch 96: loss 7.3682645012935\n",
      "Batch 97: loss 7.367849669505641\n",
      "Batch 98: loss 7.368088104286972\n",
      "Batch 99: loss 7.367544930390637\n",
      "Batch 100: loss 7.36703806400299\n",
      "Batch 101: loss 7.367300958916692\n",
      "Batch 102: loss 7.36723853560055\n",
      "Batch 103: loss 7.367525290516974\n",
      "Batch 104: loss 7.364067567866515\n",
      "Epoch 5: Training loss 7.364067567866515\n",
      "Epoch 5: Validation loss 8.290238990783692 | Accuracy 0.6454000000000001 | AUC 0.6421782741653019 | Acc H-Mean 0.610991977105826 | Spec 0.6725607919692993 | Recall 0.5597502732276917\n",
      "Batch 1: loss 7.319846153259277\n",
      "Batch 2: loss 7.361033916473389\n",
      "Batch 3: loss 7.357259750366211\n",
      "Batch 4: loss 7.361642837524414\n",
      "Batch 5: loss 7.360354328155518\n",
      "Batch 6: loss 7.356961647669475\n",
      "Batch 7: loss 7.355551719665527\n",
      "Batch 8: loss 7.35827511548996\n",
      "Batch 9: loss 7.359275658925374\n",
      "Batch 10: loss 7.359235334396362\n",
      "Batch 11: loss 7.358865087682551\n",
      "Batch 12: loss 7.359980622927348\n",
      "Batch 13: loss 7.357797439281757\n",
      "Batch 14: loss 7.360234941755023\n",
      "Batch 15: loss 7.3601884841918945\n",
      "Batch 16: loss 7.357732057571411\n",
      "Batch 17: loss 7.356788803549374\n",
      "Batch 18: loss 7.354918373955621\n",
      "Batch 19: loss 7.354257784391704\n",
      "Batch 20: loss 7.35655665397644\n",
      "Batch 21: loss 7.358283519744873\n",
      "Batch 22: loss 7.35711145401001\n",
      "Batch 23: loss 7.357066569120987\n",
      "Batch 24: loss 7.359080513318379\n",
      "Batch 25: loss 7.357867240905762\n",
      "Batch 26: loss 7.357585998681875\n",
      "Batch 27: loss 7.361536608801948\n",
      "Batch 28: loss 7.360800453594753\n",
      "Batch 29: loss 7.360272275990453\n",
      "Batch 30: loss 7.360707171758016\n",
      "Batch 31: loss 7.360347394020327\n",
      "Batch 32: loss 7.359486445784569\n",
      "Batch 33: loss 7.357717904177579\n",
      "Batch 34: loss 7.357857774285709\n",
      "Batch 35: loss 7.357914079938616\n",
      "Batch 36: loss 7.358484493361579\n",
      "Batch 37: loss 7.357841981423868\n",
      "Batch 38: loss 7.356238741623728\n",
      "Batch 39: loss 7.35548953520946\n",
      "Batch 40: loss 7.355781900882721\n",
      "Batch 41: loss 7.353607107953327\n",
      "Batch 42: loss 7.353956574485416\n",
      "Batch 43: loss 7.353569840275964\n",
      "Batch 44: loss 7.354461604898626\n",
      "Batch 45: loss 7.355440669589573\n",
      "Batch 46: loss 7.354796793149865\n",
      "Batch 47: loss 7.354526529920862\n",
      "Batch 48: loss 7.355277985334396\n",
      "Batch 49: loss 7.355087241347955\n",
      "Batch 50: loss 7.355296859741211\n",
      "Batch 51: loss 7.356060467514337\n",
      "Batch 52: loss 7.355778437394362\n",
      "Batch 53: loss 7.355525304686348\n",
      "Batch 54: loss 7.355288046377677\n",
      "Batch 55: loss 7.355389560352672\n",
      "Batch 56: loss 7.355215770857675\n",
      "Batch 57: loss 7.355162771124589\n",
      "Batch 58: loss 7.354258899031015\n",
      "Batch 59: loss 7.353944212703381\n",
      "Batch 60: loss 7.352918807665507\n",
      "Batch 61: loss 7.351745785259809\n",
      "Batch 62: loss 7.352732389203964\n",
      "Batch 63: loss 7.353243880801731\n",
      "Batch 64: loss 7.353528015315533\n",
      "Batch 65: loss 7.353217337681697\n",
      "Batch 66: loss 7.353268305460612\n",
      "Batch 67: loss 7.3536160881839585\n",
      "Batch 68: loss 7.353223688462201\n",
      "Batch 69: loss 7.352867630944735\n",
      "Batch 70: loss 7.352267067773002\n",
      "Batch 71: loss 7.352124039555939\n",
      "Batch 72: loss 7.351880007319981\n",
      "Batch 73: loss 7.352116584777832\n",
      "Batch 74: loss 7.3514268978222\n",
      "Batch 75: loss 7.351082255045573\n",
      "Batch 76: loss 7.351211798818488\n",
      "Batch 77: loss 7.351183451615371\n",
      "Batch 78: loss 7.351164041421352\n",
      "Batch 79: loss 7.351567612418646\n",
      "Batch 80: loss 7.351184362173081\n",
      "Batch 81: loss 7.351115079573643\n",
      "Batch 82: loss 7.3510821098234596\n",
      "Batch 83: loss 7.35148263839354\n",
      "Batch 84: loss 7.35158554145268\n",
      "Batch 85: loss 7.35156665689805\n",
      "Batch 86: loss 7.351320261179015\n",
      "Batch 87: loss 7.351077731998488\n",
      "Batch 88: loss 7.35077289017764\n",
      "Batch 89: loss 7.3506653817851895\n",
      "Batch 90: loss 7.350382391611735\n",
      "Batch 91: loss 7.350302518069089\n",
      "Batch 92: loss 7.350360222484754\n",
      "Batch 93: loss 7.349898517772716\n",
      "Batch 94: loss 7.350391266193796\n",
      "Batch 95: loss 7.3501381020796925\n",
      "Batch 96: loss 7.349896773695946\n",
      "Batch 97: loss 7.349423610058027\n",
      "Batch 98: loss 7.348935594364089\n",
      "Batch 99: loss 7.348493706096303\n",
      "Batch 100: loss 7.348745255470276\n",
      "Batch 101: loss 7.348563364236662\n",
      "Batch 102: loss 7.348016636044371\n",
      "Batch 103: loss 7.347692322962493\n",
      "Batch 104: loss 7.344652646981973\n",
      "Epoch 6: Training loss 7.344652646981973\n",
      "Epoch 6: Validation loss 8.26565715789795 | Accuracy 0.6512 | AUC 0.6589394031812563 | Acc H-Mean 0.6187075643574651 | Spec 0.6673828911781311 | Recall 0.5766498208045959\n",
      "Batch 1: loss 7.266420841217041\n",
      "Batch 2: loss 7.29554295539856\n",
      "Batch 3: loss 7.312743663787842\n",
      "Batch 4: loss 7.316456437110901\n",
      "Batch 5: loss 7.310355567932129\n",
      "Batch 6: loss 7.307000716527303\n",
      "Batch 7: loss 7.3129706382751465\n",
      "Batch 8: loss 7.313548743724823\n",
      "Batch 9: loss 7.316547340816921\n",
      "Batch 10: loss 7.317111492156982\n",
      "Batch 11: loss 7.313990679654208\n",
      "Batch 12: loss 7.314837058385213\n",
      "Batch 13: loss 7.309838515061599\n",
      "Batch 14: loss 7.310216358729771\n",
      "Batch 15: loss 7.314745457967122\n",
      "Batch 16: loss 7.312394738197327\n",
      "Batch 17: loss 7.312681226169362\n",
      "Batch 18: loss 7.312138319015503\n",
      "Batch 19: loss 7.312049288498728\n",
      "Batch 20: loss 7.31135413646698\n",
      "Batch 21: loss 7.310503323872884\n",
      "Batch 22: loss 7.309943350878629\n",
      "Batch 23: loss 7.313100545302682\n",
      "Batch 24: loss 7.312540650367737\n",
      "Batch 25: loss 7.3145300483703615\n",
      "Batch 26: loss 7.315293183693519\n",
      "Batch 27: loss 7.316003322601318\n",
      "Batch 28: loss 7.317301596914019\n",
      "Batch 29: loss 7.317418608172186\n",
      "Batch 30: loss 7.31769871711731\n",
      "Batch 31: loss 7.318265376552459\n",
      "Batch 32: loss 7.318653240799904\n",
      "Batch 33: loss 7.319048751484264\n",
      "Batch 34: loss 7.319824443143957\n",
      "Batch 35: loss 7.319252191271101\n",
      "Batch 36: loss 7.320794516139561\n",
      "Batch 37: loss 7.320630730809392\n",
      "Batch 38: loss 7.320206353538914\n",
      "Batch 39: loss 7.3196928684528055\n",
      "Batch 40: loss 7.320928394794464\n",
      "Batch 41: loss 7.321060041101967\n",
      "Batch 42: loss 7.32214541662307\n",
      "Batch 43: loss 7.321564763091331\n",
      "Batch 44: loss 7.3207501064647325\n",
      "Batch 45: loss 7.320540406968859\n",
      "Batch 46: loss 7.321318958116614\n",
      "Batch 47: loss 7.320856358142609\n",
      "Batch 48: loss 7.3199432988961535\n",
      "Batch 49: loss 7.320965844757703\n",
      "Batch 50: loss 7.32217321395874\n",
      "Batch 51: loss 7.324347477333219\n",
      "Batch 52: loss 7.3261769368098335\n",
      "Batch 53: loss 7.32677383242913\n",
      "Batch 54: loss 7.326704819997151\n",
      "Batch 55: loss 7.327602542530407\n",
      "Batch 56: loss 7.328051754406521\n",
      "Batch 57: loss 7.328432300634551\n",
      "Batch 58: loss 7.3285267764124375\n",
      "Batch 59: loss 7.3290361630714544\n",
      "Batch 60: loss 7.328489891688029\n",
      "Batch 61: loss 7.3289592383337805\n",
      "Batch 62: loss 7.329470888260873\n",
      "Batch 63: loss 7.330823307945614\n",
      "Batch 64: loss 7.330546274781227\n",
      "Batch 65: loss 7.331124100318322\n",
      "Batch 66: loss 7.331477223020611\n",
      "Batch 67: loss 7.331486531157992\n",
      "Batch 68: loss 7.331933540456435\n",
      "Batch 69: loss 7.332663211269655\n",
      "Batch 70: loss 7.331982251576015\n",
      "Batch 71: loss 7.331860266940694\n",
      "Batch 72: loss 7.331655151314205\n",
      "Batch 73: loss 7.331346087259789\n",
      "Batch 74: loss 7.33107201473133\n",
      "Batch 75: loss 7.33134168624878\n",
      "Batch 76: loss 7.331115477963498\n",
      "Batch 77: loss 7.331271555516627\n",
      "Batch 78: loss 7.331575503716102\n",
      "Batch 79: loss 7.330855182454556\n",
      "Batch 80: loss 7.331316965818405\n",
      "Batch 81: loss 7.331522040896946\n",
      "Batch 82: loss 7.331099690460578\n",
      "Batch 83: loss 7.331219828272441\n",
      "Batch 84: loss 7.331177734193348\n",
      "Batch 85: loss 7.330671142129337\n",
      "Batch 86: loss 7.33051291731901\n",
      "Batch 87: loss 7.330420346095644\n",
      "Batch 88: loss 7.329900508577174\n",
      "Batch 89: loss 7.330231548695082\n",
      "Batch 90: loss 7.330287350548638\n",
      "Batch 91: loss 7.33037297804277\n",
      "Batch 92: loss 7.3309698001198145\n",
      "Batch 93: loss 7.331441253744146\n",
      "Batch 94: loss 7.331622032409019\n",
      "Batch 95: loss 7.331553850675884\n",
      "Batch 96: loss 7.331586857636769\n",
      "Batch 97: loss 7.331150630085738\n",
      "Batch 98: loss 7.331327900594594\n",
      "Batch 99: loss 7.3304754844819655\n",
      "Batch 100: loss 7.329879341125488\n",
      "Batch 101: loss 7.329532505262016\n",
      "Batch 102: loss 7.328957038767197\n",
      "Batch 103: loss 7.328294402187311\n",
      "Batch 104: loss 7.324908733927318\n",
      "Epoch 7: Training loss 7.324908733927318\n",
      "Epoch 7: Validation loss 8.254689903259278 | Accuracy 0.6602 | AUC 0.6617028715032204 | Acc H-Mean 0.6176915653349196 | Spec 0.6786759114265442 | Recall 0.5667634320259094\n",
      "Batch 1: loss 7.281877517700195\n",
      "Batch 2: loss 7.291372299194336\n",
      "Batch 3: loss 7.305408636728923\n",
      "Batch 4: loss 7.323487043380737\n",
      "Batch 5: loss 7.330412769317627\n",
      "Batch 6: loss 7.330052852630615\n",
      "Batch 7: loss 7.32661737714495\n",
      "Batch 8: loss 7.32405161857605\n",
      "Batch 9: loss 7.318447219000922\n",
      "Batch 10: loss 7.3204795837402346\n",
      "Batch 11: loss 7.323458758267489\n",
      "Batch 12: loss 7.322880347569783\n",
      "Batch 13: loss 7.321731860821064\n",
      "Batch 14: loss 7.326988083975656\n",
      "Batch 15: loss 7.326516437530517\n",
      "Batch 16: loss 7.325183600187302\n",
      "Batch 17: loss 7.323260335361256\n",
      "Batch 18: loss 7.322969675064087\n",
      "Batch 19: loss 7.320881191052888\n",
      "Batch 20: loss 7.319323229789734\n",
      "Batch 21: loss 7.316926842644101\n",
      "Batch 22: loss 7.317589066245339\n",
      "Batch 23: loss 7.318359084751295\n",
      "Batch 24: loss 7.319921096165975\n",
      "Batch 25: loss 7.318493976593017\n",
      "Batch 26: loss 7.318003709499653\n",
      "Batch 27: loss 7.3169251724525735\n",
      "Batch 28: loss 7.3185339995792935\n",
      "Batch 29: loss 7.318217672150711\n",
      "Batch 30: loss 7.319606653849283\n",
      "Batch 31: loss 7.319634160687847\n",
      "Batch 32: loss 7.3205628246068954\n",
      "Batch 33: loss 7.320437243490508\n",
      "Batch 34: loss 7.31896588381599\n",
      "Batch 35: loss 7.323473603384835\n",
      "Batch 36: loss 7.323004523913066\n",
      "Batch 37: loss 7.32156802512504\n",
      "Batch 38: loss 7.3229240367287085\n",
      "Batch 39: loss 7.3224771328461475\n",
      "Batch 40: loss 7.322465109825134\n",
      "Batch 41: loss 7.322323589790158\n",
      "Batch 42: loss 7.323571681976318\n",
      "Batch 43: loss 7.323915769887525\n",
      "Batch 44: loss 7.324841380119324\n",
      "Batch 45: loss 7.324793148040771\n",
      "Batch 46: loss 7.326532747434533\n",
      "Batch 47: loss 7.326669764011465\n",
      "Batch 48: loss 7.326129923264186\n",
      "Batch 49: loss 7.326170366637561\n",
      "Batch 50: loss 7.326033506393433\n",
      "Batch 51: loss 7.326061089833577\n",
      "Batch 52: loss 7.326387983102065\n",
      "Batch 53: loss 7.32645505329348\n",
      "Batch 54: loss 7.327513156113802\n",
      "Batch 55: loss 7.328175484050404\n",
      "Batch 56: loss 7.328707328864506\n",
      "Batch 57: loss 7.328999226553398\n",
      "Batch 58: loss 7.329209911412206\n",
      "Batch 59: loss 7.328699047282591\n",
      "Batch 60: loss 7.328897062937418\n",
      "Batch 61: loss 7.328931683399638\n",
      "Batch 62: loss 7.328477644151257\n",
      "Batch 63: loss 7.329301107497442\n",
      "Batch 64: loss 7.328717157244682\n",
      "Batch 65: loss 7.328659116304838\n",
      "Batch 66: loss 7.328382679910371\n",
      "Batch 67: loss 7.327836264425249\n",
      "Batch 68: loss 7.327718033510096\n",
      "Batch 69: loss 7.326506904933764\n",
      "Batch 70: loss 7.325841290610177\n",
      "Batch 71: loss 7.325152148663158\n",
      "Batch 72: loss 7.324931614928776\n",
      "Batch 73: loss 7.325170954612837\n",
      "Batch 74: loss 7.3249094808423845\n",
      "Batch 75: loss 7.324120922088623\n",
      "Batch 76: loss 7.323488618198194\n",
      "Batch 77: loss 7.322401771297702\n",
      "Batch 78: loss 7.322294223002898\n",
      "Batch 79: loss 7.32267493235914\n",
      "Batch 80: loss 7.32221075296402\n",
      "Batch 81: loss 7.322418371836345\n",
      "Batch 82: loss 7.322032352773155\n",
      "Batch 83: loss 7.3220488364437974\n",
      "Batch 84: loss 7.322007962635586\n",
      "Batch 85: loss 7.322102995479808\n",
      "Batch 86: loss 7.3218638120695605\n",
      "Batch 87: loss 7.321808710865591\n",
      "Batch 88: loss 7.321408353068612\n",
      "Batch 89: loss 7.32108412967639\n",
      "Batch 90: loss 7.320721509721544\n",
      "Batch 91: loss 7.319806402856177\n",
      "Batch 92: loss 7.319424484087073\n",
      "Batch 93: loss 7.319402248628678\n",
      "Batch 94: loss 7.3192168702470495\n",
      "Batch 95: loss 7.318400096893311\n",
      "Batch 96: loss 7.318231090903282\n",
      "Batch 97: loss 7.317266243020284\n",
      "Batch 98: loss 7.31681752204895\n",
      "Batch 99: loss 7.317014419671261\n",
      "Batch 100: loss 7.316254868507385\n",
      "Batch 101: loss 7.315979768734167\n",
      "Batch 102: loss 7.315336881899366\n",
      "Batch 103: loss 7.315084512951304\n",
      "Batch 104: loss 7.312019659827753\n",
      "Epoch 8: Training loss 7.312019659827753\n",
      "Epoch 8: Validation loss 8.243654937744141 | Accuracy 0.6886000000000001 | AUC 0.7024675283767433 | Acc H-Mean 0.6385229191577071 | Spec 0.7143440055847168 | Recall 0.5772528100013733\n",
      "Batch 1: loss 7.261609077453613\n",
      "Batch 2: loss 7.25459885597229\n",
      "Batch 3: loss 7.25322421391805\n",
      "Batch 4: loss 7.262535572052002\n",
      "Batch 5: loss 7.262963581085205\n",
      "Batch 6: loss 7.264888445536296\n",
      "Batch 7: loss 7.2635466030665805\n",
      "Batch 8: loss 7.264488041400909\n",
      "Batch 9: loss 7.267498758104113\n",
      "Batch 10: loss 7.272604465484619\n",
      "Batch 11: loss 7.274250030517578\n",
      "Batch 12: loss 7.277109265327454\n",
      "Batch 13: loss 7.279766009404109\n",
      "Batch 14: loss 7.276212045124599\n",
      "Batch 15: loss 7.275623448689779\n",
      "Batch 16: loss 7.272283911705017\n",
      "Batch 17: loss 7.274392464581658\n",
      "Batch 18: loss 7.272255208757189\n",
      "Batch 19: loss 7.275203353480289\n",
      "Batch 20: loss 7.27527494430542\n",
      "Batch 21: loss 7.275261856260753\n",
      "Batch 22: loss 7.274285186420787\n",
      "Batch 23: loss 7.275106264197308\n",
      "Batch 24: loss 7.274099230766296\n",
      "Batch 25: loss 7.275362510681152\n",
      "Batch 26: loss 7.27492460837731\n",
      "Batch 27: loss 7.278982091833044\n",
      "Batch 28: loss 7.281115923609052\n",
      "Batch 29: loss 7.282769170300714\n",
      "Batch 30: loss 7.282828267415365\n",
      "Batch 31: loss 7.2810567117506455\n",
      "Batch 32: loss 7.281912744045258\n",
      "Batch 33: loss 7.28026057734634\n",
      "Batch 34: loss 7.2776619125815\n",
      "Batch 35: loss 7.278158283233642\n",
      "Batch 36: loss 7.277417527304755\n",
      "Batch 37: loss 7.277993923908955\n",
      "Batch 38: loss 7.276236785085578\n",
      "Batch 39: loss 7.280123099302634\n",
      "Batch 40: loss 7.281718599796295\n",
      "Batch 41: loss 7.282755351648098\n",
      "Batch 42: loss 7.284136193139212\n",
      "Batch 43: loss 7.284766430078551\n",
      "Batch 44: loss 7.2855811660939995\n",
      "Batch 45: loss 7.284937053256565\n",
      "Batch 46: loss 7.285320675891379\n",
      "Batch 47: loss 7.285596715643051\n",
      "Batch 48: loss 7.285760333140691\n",
      "Batch 49: loss 7.287515396974524\n",
      "Batch 50: loss 7.287861051559449\n",
      "Batch 51: loss 7.288191514856675\n",
      "Batch 52: loss 7.288069009780884\n",
      "Batch 53: loss 7.287828589385411\n",
      "Batch 54: loss 7.288219610850017\n",
      "Batch 55: loss 7.287570363825018\n",
      "Batch 56: loss 7.287477842399052\n",
      "Batch 57: loss 7.289130102124131\n",
      "Batch 58: loss 7.288827649478255\n",
      "Batch 59: loss 7.289090180801133\n",
      "Batch 60: loss 7.289842025438944\n",
      "Batch 61: loss 7.289079720856713\n",
      "Batch 62: loss 7.289686618312713\n",
      "Batch 63: loss 7.289492001609196\n",
      "Batch 64: loss 7.290146790444851\n",
      "Batch 65: loss 7.289995604294997\n",
      "Batch 66: loss 7.290951598774303\n",
      "Batch 67: loss 7.290233163691279\n",
      "Batch 68: loss 7.291217221933253\n",
      "Batch 69: loss 7.29013789218405\n",
      "Batch 70: loss 7.289805500847953\n",
      "Batch 71: loss 7.2896167593942565\n",
      "Batch 72: loss 7.289329959286584\n",
      "Batch 73: loss 7.28854686919957\n",
      "Batch 74: loss 7.286888534958298\n",
      "Batch 75: loss 7.2871262677510575\n",
      "Batch 76: loss 7.288615044794585\n",
      "Batch 77: loss 7.288564329023485\n",
      "Batch 78: loss 7.287898363211216\n",
      "Batch 79: loss 7.287173808375491\n",
      "Batch 80: loss 7.286669105291367\n",
      "Batch 81: loss 7.2868412924401555\n",
      "Batch 82: loss 7.286549655402579\n",
      "Batch 83: loss 7.286060028765575\n",
      "Batch 84: loss 7.2864210946219305\n",
      "Batch 85: loss 7.285943867178524\n",
      "Batch 86: loss 7.285515452540198\n",
      "Batch 87: loss 7.284987910040494\n",
      "Batch 88: loss 7.286908734928478\n",
      "Batch 89: loss 7.2860914508948165\n",
      "Batch 90: loss 7.285779815249973\n",
      "Batch 91: loss 7.285085499941648\n",
      "Batch 92: loss 7.284878336864969\n",
      "Batch 93: loss 7.284197125383603\n",
      "Batch 94: loss 7.283725880561991\n",
      "Batch 95: loss 7.283860231700697\n",
      "Batch 96: loss 7.283402224381764\n",
      "Batch 97: loss 7.28383669902369\n",
      "Batch 98: loss 7.284226286168001\n",
      "Batch 99: loss 7.283842269820396\n",
      "Batch 100: loss 7.28375322341919\n",
      "Batch 101: loss 7.283625829337847\n",
      "Batch 102: loss 7.283405486275168\n",
      "Batch 103: loss 7.282594055805392\n",
      "Batch 104: loss 7.279317120654794\n",
      "Epoch 9: Training loss 7.279317120654794\n",
      "Epoch 9: Validation loss 8.147608795166015 | Accuracy 0.7031999999999999 | AUC 0.7263788039467625 | Acc H-Mean 0.6863857710641855 | Spec 0.7219055366516113 | Recall 0.6541974329948426\n",
      "Batch 1: loss 7.249203681945801\n",
      "Batch 2: loss 7.231076002120972\n",
      "Batch 3: loss 7.242180506388347\n",
      "Batch 4: loss 7.22482967376709\n",
      "Batch 5: loss 7.216905879974365\n",
      "Batch 6: loss 7.2132134437561035\n",
      "Batch 7: loss 7.217840807778495\n",
      "Batch 8: loss 7.230891287326813\n",
      "Batch 9: loss 7.241410732269287\n",
      "Batch 10: loss 7.245691585540771\n",
      "Batch 11: loss 7.240527759898793\n",
      "Batch 12: loss 7.241096218427022\n",
      "Batch 13: loss 7.239591341752273\n",
      "Batch 14: loss 7.238614048276629\n",
      "Batch 15: loss 7.235841019948324\n",
      "Batch 16: loss 7.238497167825699\n",
      "Batch 17: loss 7.236194526447969\n",
      "Batch 18: loss 7.239703363842434\n",
      "Batch 19: loss 7.2432435437252645\n",
      "Batch 20: loss 7.242863488197327\n",
      "Batch 21: loss 7.249218327658517\n",
      "Batch 22: loss 7.249476844614202\n",
      "Batch 23: loss 7.250402098116667\n",
      "Batch 24: loss 7.25080794095993\n",
      "Batch 25: loss 7.251522235870361\n",
      "Batch 26: loss 7.250362781377939\n",
      "Batch 27: loss 7.252473672231038\n",
      "Batch 28: loss 7.253121546336582\n",
      "Batch 29: loss 7.251674109491809\n",
      "Batch 30: loss 7.251219956080119\n",
      "Batch 31: loss 7.250126407992456\n",
      "Batch 32: loss 7.252010762691498\n",
      "Batch 33: loss 7.252212033127293\n",
      "Batch 34: loss 7.250770120059743\n",
      "Batch 35: loss 7.250899955204555\n",
      "Batch 36: loss 7.254049102465312\n",
      "Batch 37: loss 7.254195844804919\n",
      "Batch 38: loss 7.254905788522017\n",
      "Batch 39: loss 7.25537710923415\n",
      "Batch 40: loss 7.255792140960693\n",
      "Batch 41: loss 7.254764649926162\n",
      "Batch 42: loss 7.256330694471087\n",
      "Batch 43: loss 7.256164628405903\n",
      "Batch 44: loss 7.255290952595797\n",
      "Batch 45: loss 7.256011888715956\n",
      "Batch 46: loss 7.254386912221494\n",
      "Batch 47: loss 7.254435914628049\n",
      "Batch 48: loss 7.253811577955882\n",
      "Batch 49: loss 7.253505521891069\n",
      "Batch 50: loss 7.25320873260498\n",
      "Batch 51: loss 7.252514147290997\n",
      "Batch 52: loss 7.252216183222258\n",
      "Batch 53: loss 7.2516950931189195\n",
      "Batch 54: loss 7.251497215694851\n",
      "Batch 55: loss 7.2557300047440965\n",
      "Batch 56: loss 7.254860758781433\n",
      "Batch 57: loss 7.254324971583852\n",
      "Batch 58: loss 7.253143351653526\n",
      "Batch 59: loss 7.252362841266697\n",
      "Batch 60: loss 7.251978731155395\n",
      "Batch 61: loss 7.252641443346367\n",
      "Batch 62: loss 7.252245741505777\n",
      "Batch 63: loss 7.25286464842539\n",
      "Batch 64: loss 7.25254949182272\n",
      "Batch 65: loss 7.251837136195256\n",
      "Batch 66: loss 7.251147826512654\n",
      "Batch 67: loss 7.253003746715944\n",
      "Batch 68: loss 7.25324126552133\n",
      "Batch 69: loss 7.253446178159852\n",
      "Batch 70: loss 7.254287324632917\n",
      "Batch 71: loss 7.253138448151065\n",
      "Batch 72: loss 7.2522804670863685\n",
      "Batch 73: loss 7.2518529173446025\n",
      "Batch 74: loss 7.253567805161348\n",
      "Batch 75: loss 7.252976932525635\n",
      "Batch 76: loss 7.252611706131383\n",
      "Batch 77: loss 7.253403744140229\n",
      "Batch 78: loss 7.252884326837002\n",
      "Batch 79: loss 7.252386044852341\n",
      "Batch 80: loss 7.251060926914215\n",
      "Batch 81: loss 7.250113287089784\n",
      "Batch 82: loss 7.250089261589981\n",
      "Batch 83: loss 7.249716615102377\n",
      "Batch 84: loss 7.249748224303836\n",
      "Batch 85: loss 7.249078722561107\n",
      "Batch 86: loss 7.250047855598982\n",
      "Batch 87: loss 7.249776133175554\n",
      "Batch 88: loss 7.249155635183508\n",
      "Batch 89: loss 7.249485690941971\n",
      "Batch 90: loss 7.248596053653293\n",
      "Batch 91: loss 7.24887513590383\n",
      "Batch 92: loss 7.248721599578857\n",
      "Batch 93: loss 7.24857247260309\n",
      "Batch 94: loss 7.2490187249285105\n",
      "Batch 95: loss 7.248637405194734\n",
      "Batch 96: loss 7.249474376440048\n",
      "Batch 97: loss 7.2488836013164715\n",
      "Batch 98: loss 7.248796312176451\n",
      "Batch 99: loss 7.248512051322243\n",
      "Batch 100: loss 7.248385648727417\n",
      "Batch 101: loss 7.247785875112704\n",
      "Batch 102: loss 7.248101463504866\n",
      "Batch 103: loss 7.247454184930302\n",
      "Batch 104: loss 7.2441999929208425\n",
      "Epoch 10: Training loss 7.2441999929208425\n",
      "Epoch 10: Validation loss 8.185050430297851 | Accuracy 0.6904 | AUC 0.7219209210235789 | Acc H-Mean 0.679044707227142 | Spec 0.7155904603004456 | Recall 0.6460504174232483\n",
      "Best epoch:  9\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# mtrainer.run_train(6, support_loader, query_loader, lr=2e-5, full_training=True)\n",
    "# mtrainer.run_train(10, support_loader, query_loader, lr=2e-5)\n",
    "mtrainer.run_train(10, support_loader, query_loader, lr=1e-4, min_lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.best_model.attention, 'models/attention/model/vindr2/attention-8h.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.best_model.attention, 'models/attention/model/vindr2/attention-trans-8h4l.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.model.attention, 'models/attention/model/vindr2/full-mh/attention-model8h.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.model.encoder, 'models/attention/model/vindr2/full-mh/imgtxt-encoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import AverageMeter, calculate_auc, multilabel_logit_accuracy\n",
    "from torchmetrics.classification import MultilabelRecall, MultilabelSpecificity, MultilabelPrecision, MultilabelF1Score\n",
    "from models.attention.model import image_text_logits\n",
    "\n",
    "def run_eval(model, dataloader, device, class_labels):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    auc_meter = AverageMeter()\n",
    "    acc_meter = AverageMeter()\n",
    "    spec_meter = AverageMeter()\n",
    "    rec_meter = AverageMeter()\n",
    "    f1_meter = AverageMeter()\n",
    "\n",
    "    num_labels = len(class_labels)\n",
    "    specificity = MultilabelSpecificity(num_labels=num_labels).to(device)\n",
    "    recall = MultilabelRecall(num_labels=num_labels).to(device)\n",
    "    precision = MultilabelPrecision(num_labels=num_labels).to(device)\n",
    "    f1_func = MultilabelF1Score(num_labels=num_labels).to(device)\n",
    "    with torch.no_grad():\n",
    "         for images, class_inds in dataloader:\n",
    "                images, class_inds = images.to(device), class_inds.to(device)\n",
    "\n",
    "                text_embeddings, _, prototypes = model(class_labels, images)\n",
    "\n",
    "                logits_per_image = image_text_logits(text_embeddings, prototypes, model.encoder.get_logit_scale())\n",
    "                \n",
    "                f1 = f1_func(logits_per_image, class_inds)\n",
    "                f1_meter.update(f1.item(), len(class_inds))\n",
    "\n",
    "                auc = calculate_auc(logits_per_image, class_inds)\n",
    "                auc_meter.update(auc, len(class_inds))\n",
    "            \n",
    "                acc = multilabel_logit_accuracy(logits_per_image, class_inds)\n",
    "                acc_meter.update(acc, len(class_inds))\n",
    "\n",
    "                spec = specificity(logits_per_image, class_inds)\n",
    "                spec_meter.update(spec.item(), len(class_inds))\n",
    "                rec = recall(logits_per_image, class_inds)\n",
    "                rec_meter.update(rec.item(), len(class_inds))\n",
    "                prec = precision(logits_per_image, class_inds)\n",
    "                print(f\"F1 {f1} | Accuracy {acc} | AUC {auc} | Specificity {spec} | Recall {rec} | Precision {prec}\")\n",
    "            \n",
    "    return f1_meter.average(), acc_meter.average(), auc_meter.average(), spec_meter.average(), rec_meter.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.29421404004096985 | Accuracy 0.5387755102040817 | AUC 0.5364233972863903 | Specificity 0.529010534286499 | Recall 0.5449503064155579 | Precision 0.22477076947689056\n",
      "F1 0.2881873548030853 | Accuracy 0.5704081632653061 | AUC 0.5492518288214842 | Specificity 0.5554752349853516 | Recall 0.549533486366272 | Precision 0.22559234499931335\n",
      "F1 0.2743912935256958 | Accuracy 0.5265306122448979 | AUC 0.6043750009261554 | Specificity 0.5206221342086792 | Recall 0.6165170669555664 | Precision 0.21531684696674347\n",
      "F1 0.24442556500434875 | Accuracy 0.536734693877551 | AUC 0.47709730714792276 | Specificity 0.551681399345398 | Recall 0.4351627230644226 | Precision 0.20781458914279938\n",
      "F1 0.2748141288757324 | Accuracy 0.5673469387755102 | AUC 0.5204891878065561 | Specificity 0.5728052854537964 | Recall 0.4932914078235626 | Precision 0.22820404171943665\n",
      "F1 0.29293304681777954 | Accuracy 0.573469387755102 | AUC 0.538747686447979 | Specificity 0.5742725729942322 | Recall 0.4904204308986664 | Precision 0.23970824480056763\n",
      "F1 0.27186647057533264 | Accuracy 0.5438775510204081 | AUC 0.47988527393314445 | Specificity 0.5362706780433655 | Recall 0.4225888252258301 | Precision 0.2240271419286728\n",
      "F1 0.24854853749275208 | Accuracy 0.5326530612244897 | AUC 0.5148203115582364 | Specificity 0.5412131547927856 | Recall 0.4685187041759491 | Precision 0.20596542954444885\n",
      "F1 0.24197162687778473 | Accuracy 0.5540816326530612 | AUC 0.497310286430087 | Specificity 0.5695083141326904 | Recall 0.40416666865348816 | Precision 0.20291800796985626\n",
      "F1 0.25726333260536194 | Accuracy 0.5336734693877551 | AUC 0.497625075214228 | Specificity 0.5390307307243347 | Recall 0.4696599841117859 | Precision 0.21170565485954285\n",
      "F1 0.2719917595386505 | Accuracy 0.557997557997558 | AUC 0.5069136720309414 | Specificity 0.5396623611450195 | Recall 0.456191748380661 | Precision 0.2242627590894699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.26910296070709305,\n",
       " 0.5485450607401827,\n",
       " 0.5204696436438535,\n",
       " 0.548269678120874,\n",
       " 0.48691349972135234)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = Dataset(config.img_path, config.img_info, config.img_info[config.img_info['meta_split'] == 'test']['image_id'].to_list(), config.label_names_map, config.classes_split_map['test'], mean_std=config.mean_std)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "run_eval(mtrainer.model, test_loader, device, test_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.33683517575263977 | Accuracy 0.5867346938775511 | AUC 0.5775634642396389 | Specificity 0.5719772577285767 | Recall 0.5820599794387817 | Precision 0.2731553614139557\n",
      "F1 0.23812535405158997 | Accuracy 0.523469387755102 | AUC 0.4520744551409893 | Specificity 0.5198322534561157 | Recall 0.37458890676498413 | Precision 0.20049630105495453\n",
      "F1 0.26939091086387634 | Accuracy 0.5173469387755102 | AUC 0.4940022823974166 | Specificity 0.5003655552864075 | Recall 0.5163915157318115 | Precision 0.2047646939754486\n",
      "F1 0.2377161681652069 | Accuracy 0.5316326530612245 | AUC 0.5195210574333502 | Specificity 0.5455586910247803 | Recall 0.45703208446502686 | Precision 0.18662647902965546\n",
      "F1 0.24718846380710602 | Accuracy 0.5540816326530612 | AUC 0.47868969583608384 | Specificity 0.5614677667617798 | Recall 0.36818647384643555 | Precision 0.2032890021800995\n",
      "F1 0.2704866826534271 | Accuracy 0.4959183673469388 | AUC 0.5424808905209917 | Specificity 0.4695279002189636 | Recall 0.562262773513794 | Precision 0.20899400115013123\n",
      "F1 0.26698237657546997 | Accuracy 0.5306122448979592 | AUC 0.5080064719974378 | Specificity 0.5206609964370728 | Recall 0.5012738108634949 | Precision 0.20675770938396454\n",
      "F1 0.27929529547691345 | Accuracy 0.5510204081632653 | AUC 0.5560697590735516 | Specificity 0.5411891937255859 | Recall 0.518082857131958 | Precision 0.22030343115329742\n",
      "F1 0.2899353504180908 | Accuracy 0.55 | AUC 0.5446444578379428 | Specificity 0.5465253591537476 | Recall 0.5438764691352844 | Precision 0.2316693216562271\n",
      "F1 0.2933295667171478 | Accuracy 0.5540816326530612 | AUC 0.5516389357097452 | Specificity 0.5429940819740295 | Recall 0.5127435922622681 | Precision 0.23758786916732788\n",
      "F1 0.24612608551979065 | Accuracy 0.5213675213675214 | AUC 0.48489942228788824 | Specificity 0.5235278010368347 | Recall 0.4643669128417969 | Precision 0.20004045963287354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2708613712809631,\n",
       " 0.5380920990677088,\n",
       " 0.5195715479458692,\n",
       " 0.531355715575938,\n",
       " 0.49139137352935247)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_eval(mtrainer.best_model, test_loader, device, test_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.551537275314331 | Accuracy 0.675 | AUC 0.7134491798187556 | Specificity 0.6937583088874817 | Recall 0.630710244178772 | Precision 0.5674383640289307\n",
      "F1 0.5394936800003052 | Accuracy 0.6928571428571428 | AUC 0.7143775906323018 | Specificity 0.7368800640106201 | Recall 0.605082094669342 | Precision 0.5787002444267273\n",
      "F1 0.5987014770507812 | Accuracy 0.6964285714285714 | AUC 0.7165412844172835 | Specificity 0.7004305124282837 | Recall 0.6797438263893127 | Precision 0.6326056122779846\n",
      "F1 0.6119371652603149 | Accuracy 0.7025 | AUC 0.756248347628764 | Specificity 0.7451474666595459 | Recall 0.6682404279708862 | Precision 0.6369891166687012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5710350275039673,\n",
       " 0.6904,\n",
       " 0.7214227909837376,\n",
       " 0.7159228825569153,\n",
       " 0.6432685947418213)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_eval(mtrainer.model, query_loader, device, query_dataset.class_labels())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
