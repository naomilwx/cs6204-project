{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/HobbitLong/SupContrast/blob/master/losses.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07, contrast_mode='all'):\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.contrast_mode = contrast_mode\n",
    "\n",
    "    def forward(self, features, labels=None, mask=None):\n",
    "        \"\"\"Compute loss for model. If both `labels` and `mask` are None,\n",
    "        it degenerates to SimCLR unsupervised loss:\n",
    "        https://arxiv.org/pdf/2002.05709.pdf\n",
    "        Args:\n",
    "            features: hidden vector of shape [bsz, n_views, ...].\n",
    "            labels: ground truth of shape [bsz].\n",
    "            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j\n",
    "                has the same class as sample i. Can be asymmetric.\n",
    "        Returns:\n",
    "            A loss scalar.\n",
    "        \"\"\"\n",
    "        device = (torch.device('cuda')\n",
    "                  if features.is_cuda\n",
    "                  else torch.device('mps') if features.is_mps else torch.device('cpu'))\n",
    "\n",
    "        if len(features.shape) < 3:\n",
    "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
    "                             'at least 3 dimensions are required')\n",
    "        if len(features.shape) > 3:\n",
    "            features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "        batch_size = features.shape[0]\n",
    "        if labels is not None and mask is not None:\n",
    "            raise ValueError('Cannot define both `labels` and `mask`')\n",
    "        elif labels is None and mask is None:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "        elif labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            if labels.shape[0] != batch_size:\n",
    "                raise ValueError('Num of labels does not match num of features')\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        else:\n",
    "            mask = mask.float().to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "        if self.contrast_mode == 'one':\n",
    "            anchor_feature = features[:, 0]\n",
    "            anchor_count = 1\n",
    "        elif self.contrast_mode == 'all':\n",
    "            anchor_feature = contrast_feature\n",
    "            anchor_count = contrast_count\n",
    "        else:\n",
    "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
    "\n",
    "        # compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature)\n",
    "        # for numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "        # compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True)).nan_to_num(0, 0, 0)\n",
    "        # compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "        # loss\n",
    "        loss = - mean_log_prob_pos.nan_to_num(0, 0, 0)\n",
    "        loss = loss.view(anchor_count, batch_size).mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utils.contrastive_loss import SupConLoss\n",
    "\n",
    "class LabelImageAttention(nn.Module):\n",
    "    def __init__(self, dim_in, n_head, dropout=0.1, num_layers=4, temperature=1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Transformer(dim_in, batch_first=True, nhead=n_head, dropout=dropout, num_decoder_layers=num_layers, num_encoder_layers=num_layers)\n",
    "        self.con_loss = SupConLoss(temperature=temperature)\n",
    "\n",
    "    def forward(self, texts, images, label_inds=None):\n",
    "        # transformer: (N, S, E), (N, T, E) -> (N, T, E)\n",
    "        # texts: (L,D) , images: (N,D,H,W), label_inds: (N, L)\n",
    "        texts = texts.repeat(images.shape[0], 1, 1)\n",
    "        # view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
    "        images = images.flatten(start_dim=2).permute(0, 2, 1)\n",
    "        mask = None\n",
    "        if label_inds is not None:\n",
    "            mask = (1 - label_inds).bool()\n",
    "        # Output: (N, L, D)\n",
    "        # Texts: NxLxD (decode)\n",
    "        # Mask irrelevant labels with tgt_key_padding_mask, set masked positions to True\n",
    "        # Images: Nx(HxW)xD\n",
    "        out = self.attn(images, texts, tgt_key_padding_mask=mask)\n",
    "        return out\n",
    "    \n",
    "    def loss(self, results, label_inds):\n",
    "        # results: (N, L, D), labels: (N, L)\n",
    "        classes = torch.nonzero(label_inds)[:,1] # (Np,)\n",
    "        prototypes = results[label_inds.bool()] # (Np, D)\n",
    "        return self.con_loss(prototypes.unsqueeze(1), classes)\n",
    "\n",
    "\n",
    "class LabelImagePrototypeModel(nn.Module):\n",
    "    def __init__(self, encoder, n_head, dim_in=512, dropout=0.1, num_layers=4, temperature=1):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.attention = LabelImageAttention(dim_in, n_head, dropout, num_layers, temperature)\n",
    "\n",
    "    def freeze_encoder(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_encoder(self):\n",
    "        self.encoder.img_model.proj.requires_grad = True\n",
    "        self.encoder.text_model.proj.requires_grad = True\n",
    "    \n",
    "    def forward(self, class_labels, images, label_inds):\n",
    "        text_embedding, image_emedding = self.encoder(class_labels, images, False)\n",
    "        prototypes = self.attention(text_embedding, image_emedding, label_inds)\n",
    "        return text_embedding, image_emedding, prototypes\n",
    "    \n",
    "    def attention_loss(self, prototypes, label_inds):\n",
    "        return self.attention.loss(prototypes, label_inds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "from utils.metrics import AverageMeter, calculate_auc, multilabel_accuracy\n",
    "from utils.prototype import class_prototype_mean\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, class_labels, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.class_labels = class_labels\n",
    "    \n",
    "    def run_train(self, epochs, dataloader, val_dataloader, lr=1e-4, full_training=False):\n",
    "        model = self.model.to(self.device)\n",
    "        best_epoch = None\n",
    "        best_acc = None\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        if full_training:\n",
    "            model.unfreeze_encoder()\n",
    "        else:\n",
    "            model.freeze_encoder()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            loss_meter = AverageMeter()\n",
    "            for i, (images, class_inds) in enumerate(dataloader):\n",
    "                images, class_inds = images.to(self.device), class_inds.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                text_embeddings, _, prototypes = model(self.class_labels, images, class_inds)\n",
    "                loss = model.attention_loss(prototypes, class_inds)\n",
    "                if full_training:\n",
    "                    class_prototypes = class_prototype_mean(prototypes)\n",
    "                    loss += 0.5 * self.encoder.loss(text_embeddings, class_prototypes, class_inds)\n",
    "               \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_meter.update(loss.item(), len(class_inds))\n",
    "                print(f\"Batch {i+1}: loss {loss_meter.average()}\")\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Training loss {loss_meter.average()}\")\n",
    "            val_acc, val_auc, val_loss = self.run_eval(model, val_dataloader)\n",
    "            print(f\"Epoch {epoch+1}: Validation loss {val_loss} | Accuracy {val_acc} | AUC {val_auc}\")\n",
    "            \n",
    "            if best_acc is None or val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                self.best_model = copy.deepcopy(model)\n",
    "                best_epoch = epoch\n",
    "        print('Best epoch: ', best_epoch+1)\n",
    "\n",
    "    def run_eval(self, model, dataloader, full_training=False):\n",
    "        model.eval()\n",
    "        model = model.to(self.device)\n",
    "        \n",
    "        loss_meter = AverageMeter()\n",
    "        auc_meter = AverageMeter()\n",
    "        acc_meter = AverageMeter()\n",
    "        with torch.no_grad():\n",
    "            for images, class_inds in dataloader:\n",
    "                images, class_inds = images.to(self.device), class_inds.to(self.device)\n",
    "                text_embeddings, _, prototypes = model(self.class_labels, images, class_inds)\n",
    "                class_prototypes = class_prototype_mean(prototypes)\n",
    "                logits_per_text, logits_per_image = model.encoder.compute_logits(text_embeddings, class_prototypes)\n",
    "\n",
    "                loss = model.attention_loss(prototypes, class_inds).item()\n",
    "                if full_training:\n",
    "                    loss += model.encoder.contrastive_logit_loss(logits_per_text, logits_per_image, class_inds).item()\n",
    "        \n",
    "                loss_meter.update(loss, len(class_inds))\n",
    "\n",
    "                auc = calculate_auc(logits_per_image, class_inds)\n",
    "                auc_meter.update(auc, len(class_inds))\n",
    "                \n",
    "                acc = multilabel_accuracy(logits_per_image, class_inds)\n",
    "                acc_meter.update(acc, len(class_inds))\n",
    "        return acc_meter.average(), auc_meter.average(), loss_meter.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "from utils.device import get_device\n",
    "from utils.data import get_query_and_support_ids\n",
    "from models.embedding.dataset import Dataset\n",
    "\n",
    "img_info = pd.read_pickle('data/vindr_cxr_split_labels.pkl')\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(img_info, 'data/vindr_train_query_set.pkl')\n",
    "\n",
    "IMG_PATH = 'datasets/vindr-cxr-png'\n",
    "batch_size = 10*14\n",
    "query_dataset = Dataset(IMG_PATH, img_info, query_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "query_loader = DataLoader(dataset=query_dataset, batch_size=batch_size, shuffle=True)\n",
    "support_dataset = Dataset(IMG_PATH, img_info, support_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "support_loader = DataLoader(dataset=support_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "PROJ_SIZE = 512\n",
    "# device = 'cpu'\n",
    "device =  get_device()\n",
    "\n",
    "encoder = torch.load('imgtext_model_trained1-newlib.pth')\n",
    "encoder.text_model.device = device\n",
    "model = LabelImagePrototypeModel(encoder, 8, PROJ_SIZE)\n",
    "mtrainer = Trainer(model, support_dataset.class_labels(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ny/2ylc61455nv8t8z2px9ppsqw0000gn/T/ipykernel_92060/1616687668.py:29: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  classes = torch.nonzero(label_inds)[:,1] # (Np,)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::nan_to_num.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/naomileow/Documents/school/CS6240/project/Training Attention Model.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Training%20Attention%20Model.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mtrainer\u001b[39m.\u001b[39;49mrun_train(\u001b[39m5\u001b[39;49m, support_loader, query_loader, lr\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m, full_training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;32m/Users/naomileow/Documents/school/CS6240/project/Training Attention Model.ipynb Cell 5\u001b[0m in \u001b[0;36mTrainer.run_train\u001b[0;34m(self, epochs, dataloader, val_dataloader, lr, full_training)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Training%20Attention%20Model.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Training%20Attention%20Model.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m text_embeddings, _, prototypes \u001b[39m=\u001b[39m model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_labels, images, class_inds)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Training%20Attention%20Model.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mattention_loss(prototypes, class_inds)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Training%20Attention%20Model.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mif\u001b[39;00m full_training:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Training%20Attention%20Model.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     class_prototypes \u001b[39m=\u001b[39m class_prototype_mean(prototypes)\n",
      "\u001b[1;32m/Users/naomileow/Documents/school/CS6240/project/Training Attention Model.ipynb Cell 5\u001b[0m in \u001b[0;36mLabelImagePrototypeModel.attention_loss\u001b[0;34m(self, prototypes, label_inds)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Training%20Attention%20Model.ipynb#W5sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mattention_loss\u001b[39m(\u001b[39mself\u001b[39m, prototypes, label_inds):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Training%20Attention%20Model.ipynb#W5sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention\u001b[39m.\u001b[39;49mloss(prototypes, label_inds)\n",
      "\u001b[1;32m/Users/naomileow/Documents/school/CS6240/project/Training Attention Model.ipynb Cell 5\u001b[0m in \u001b[0;36mLabelImageAttention.loss\u001b[0;34m(self, results, label_inds)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Training%20Attention%20Model.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m classes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnonzero(label_inds)[:,\u001b[39m1\u001b[39m] \u001b[39m# (Np,)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Training%20Attention%20Model.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m prototypes \u001b[39m=\u001b[39m results[label_inds\u001b[39m.\u001b[39mbool()] \u001b[39m# (Np, D)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Training%20Attention%20Model.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcon_loss(prototypes\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m), classes)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240-1/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/school/CS6240/project/utils/contrastive_loss.py:79\u001b[0m, in \u001b[0;36mSupConLoss.forward\u001b[0;34m(self, features, labels, mask)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m# compute log_prob\u001b[39;00m\n\u001b[1;32m     78\u001b[0m exp_logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(logits) \u001b[39m*\u001b[39m logits_mask\n\u001b[0;32m---> 79\u001b[0m log_prob \u001b[39m=\u001b[39m logits \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39;49mlog(exp_logits\u001b[39m.\u001b[39;49msum(\u001b[39m1\u001b[39;49m, keepdim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\u001b[39m.\u001b[39;49mnan_to_num(\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     80\u001b[0m \u001b[39m# compute mean of log-likelihood over positive\u001b[39;00m\n\u001b[1;32m     81\u001b[0m mean_log_prob_pos \u001b[39m=\u001b[39m (mask \u001b[39m*\u001b[39m log_prob)\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m mask\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::nan_to_num.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "mtrainer.run_train(5, support_loader, query_loader, lr=1e-4, full_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n",
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/transformers/attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(41.07142857142857, 0.5212234249734956, 77.43980865478515)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.model, query_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, query_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
