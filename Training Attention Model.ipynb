{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from models.attention.model import LabelImageAttention, LabelImagePrototypeModel, LabelImageMHAttention\n",
    "from models.attention.trainer import Trainer\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "\n",
    "from utils.data import get_query_and_support_ids, DatasetConfig\n",
    "from utils.device import get_device\n",
    "from utils.data import get_query_and_support_ids\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT, VINDR_SPLIT2\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "from models.embedding.dataset import Dataset\n",
    "from utils.sampling import MultilabelBalancedRandomSampler\n",
    "from utils.f1_loss import BalAccuracyLoss\n",
    "\n",
    "\n",
    "configs = {\n",
    "    'vindr1': DatasetConfig('datasets/vindr-cxr-png', 'data/vindr_cxr_split_labels.pkl', 'data/vindr_train_query_set.pkl', VINDR_CXR_LABELS, VINDR_SPLIT, MEAN_STDS['chestmnist']),\n",
    "    'vindr2': DatasetConfig('datasets/vindr-cxr-png', 'data/vindr_cxr_split_labels2.pkl', 'data/vindr_train_query_set2.pkl', VINDR_CXR_LABELS, VINDR_SPLIT2, MEAN_STDS['chestmnist'])\n",
    "}\n",
    "\n",
    "config = configs['vindr2']\n",
    "\n",
    "IMG_PATH = 'datasets/vindr-cxr-png'\n",
    "batch_size = 10*14\n",
    "\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(config.img_info, config.training_split_path)\n",
    "query_dataset = Dataset(config.img_path, config.img_info, query_image_ids, config.label_names_map, config.classes_split_map['train'], mean_std=config.mean_std)\n",
    "query_loader = DataLoader(dataset=query_dataset, batch_size=batch_size, shuffle=True)\n",
    "support_dataset = Dataset(config.img_path, config.img_info, support_image_ids, config.label_names_map, config.classes_split_map['train'], mean_std=config.mean_std)\n",
    "support_loader = DataLoader(dataset=support_dataset, batch_size=batch_size, sampler=MultilabelBalancedRandomSampler(support_dataset.get_class_indicators()))\n",
    "\n",
    "\n",
    "PROJ_SIZE = 512\n",
    "device =  get_device()\n",
    "\n",
    "encoder = torch.load('models/embedding/model/vindr2/imgtext_model_trained1.pth')\n",
    "encoder.text_model.device = device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attn_model = LabelImageMHAttention(PROJ_SIZE, 8, cls_weight=1, cls_loss=BalAccuracyLoss(), device=device)\n",
    "attn_model = torch.load('models/attention/model/vindr2/attention-8h.pth')\n",
    "model = LabelImagePrototypeModel(encoder, 8, PROJ_SIZE, attn_model=attn_model)\n",
    "mtrainer = Trainer(model, support_dataset.class_labels(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LabelImagePrototypeModel(encoder, 8, PROJ_SIZE, num_layers=4, cls_weight=1, cls_loss=BalAccuracyLoss())\n",
    "mtrainer = Trainer(model, support_dataset.class_labels(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_model = torch.load('models/attention/model/vindr2/attention-model8h4l.pth') # previously trained up to 10, 6 the best epoch\n",
    "model = LabelImagePrototypeModel(encoder, 8, PROJ_SIZE, attn_model=attn_model)\n",
    "mtrainer = Trainer(model, support_dataset.class_labels(), device)\n",
    "\n",
    "mtrainer.run_train(10, support_loader, query_loader, lr=2e-5, encoder_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naomileow/Documents/school/CS6240/project/models/attention/model.py:43: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  classes = torch.nonzero(label_inds)[:,1] # (Np,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 6.230784893035889\n",
      "Batch 2: loss 6.230344533920288\n",
      "Batch 3: loss 6.243095715840657\n",
      "Batch 4: loss 6.236725568771362\n",
      "Batch 5: loss 6.236250114440918\n",
      "Batch 6: loss 6.228386958440145\n",
      "Batch 7: loss 6.232144560132708\n",
      "Batch 8: loss 6.2259631752967834\n",
      "Batch 9: loss 6.220034069485134\n",
      "Batch 10: loss 6.217113447189331\n",
      "Batch 11: loss 6.217128970406272\n",
      "Batch 12: loss 6.216728091239929\n",
      "Batch 13: loss 6.217156556936411\n",
      "Batch 14: loss 6.2182431902204245\n",
      "Batch 15: loss 6.215466626485189\n",
      "Batch 16: loss 6.213330298662186\n",
      "Batch 17: loss 6.215746262494256\n",
      "Batch 18: loss 6.21423535876804\n",
      "Batch 19: loss 6.21462345123291\n",
      "Batch 20: loss 6.2128478527069095\n",
      "Batch 21: loss 6.212313402266729\n",
      "Batch 22: loss 6.2133310274644336\n",
      "Batch 23: loss 6.212853307309358\n",
      "Batch 24: loss 6.213221510251363\n",
      "Batch 25: loss 6.212041397094726\n",
      "Batch 26: loss 6.211079414074238\n",
      "Batch 27: loss 6.209144609945792\n",
      "Batch 28: loss 6.209561807768686\n",
      "Batch 29: loss 6.211801611143967\n",
      "Batch 30: loss 6.212913354237874\n",
      "Batch 31: loss 6.212457164641349\n",
      "Batch 32: loss 6.213035345077515\n",
      "Batch 33: loss 6.21341423554854\n",
      "Batch 34: loss 6.213375007405\n",
      "Batch 35: loss 6.213915075574603\n",
      "Batch 36: loss 6.213129745589362\n",
      "Batch 37: loss 6.2146882624239534\n",
      "Batch 38: loss 6.214513954363372\n",
      "Batch 39: loss 6.214467928959773\n",
      "Batch 40: loss 6.214852499961853\n",
      "Batch 41: loss 6.21498114888261\n",
      "Batch 42: loss 6.215840759731474\n",
      "Batch 43: loss 6.215066155721975\n",
      "Batch 44: loss 6.214260838248513\n",
      "Batch 45: loss 6.2130402565002445\n",
      "Batch 46: loss 6.212952230287635\n",
      "Batch 47: loss 6.212792467563711\n",
      "Batch 48: loss 6.213457236687343\n",
      "Batch 49: loss 6.213144925175881\n",
      "Batch 50: loss 6.21342137336731\n",
      "Batch 51: loss 6.2137595438489726\n",
      "Batch 52: loss 6.213154838635371\n",
      "Batch 53: loss 6.2126631646786095\n",
      "Batch 54: loss 6.213156064351399\n",
      "Batch 55: loss 6.212762173739347\n",
      "Batch 56: loss 6.212604888847896\n",
      "Batch 57: loss 6.211919140397456\n",
      "Batch 58: loss 6.212519407272339\n",
      "Batch 59: loss 6.213083154064114\n",
      "Batch 60: loss 6.213296937942505\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# mtrainer.run_train(6, support_loader, query_loader, lr=2e-5, full_training=True)\n",
    "# mtrainer.run_train(10, support_loader, query_loader, lr=2e-5)\n",
    "mtrainer.run_train(4, support_loader, query_loader, lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.best_model.attention, 'models/attention/model/vindr2/attention-8h.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.best_model.attention, 'models/attention/model/vindr2/attention-model8h.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrainer.run_eval(mtrainer.model, query_loader, additional_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.model.attention, 'models/attention/model/vindr2/full-mh/attention-model8h.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.model.encoder, 'models/attention/model/vindr2/full-mh/imgtxt-encoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import AverageMeter, calculate_auc, multilabel_logit_accuracy\n",
    "from torchmetrics.classification import MultilabelRecall, MultilabelSpecificity, MultilabelPrecision, MultilabelF1Score\n",
    "from models.attention.model import image_text_logits\n",
    "\n",
    "def run_eval(model, dataloader, device, class_labels):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    auc_meter = AverageMeter()\n",
    "    acc_meter = AverageMeter()\n",
    "    spec_meter = AverageMeter()\n",
    "    rec_meter = AverageMeter()\n",
    "    f1_meter = AverageMeter()\n",
    "\n",
    "    num_labels = len(class_labels)\n",
    "    specificity = MultilabelSpecificity(num_labels=num_labels).to(device)\n",
    "    recall = MultilabelRecall(num_labels=num_labels).to(device)\n",
    "    precision = MultilabelPrecision(num_labels=num_labels).to(device)\n",
    "    f1_func = MultilabelF1Score(num_labels=num_labels).to(device)\n",
    "    with torch.no_grad():\n",
    "         for images, class_inds in dataloader:\n",
    "                images, class_inds = images.to(device), class_inds.to(device)\n",
    "\n",
    "                text_embeddings, _, prototypes = model(class_labels, images)\n",
    "\n",
    "                logits_per_image = image_text_logits(text_embeddings, prototypes, model.encoder.get_logit_scale())\n",
    "                \n",
    "                f1 = f1_func(logits_per_image, class_inds)\n",
    "                f1_meter.update(f1.item(), len(class_inds))\n",
    "\n",
    "                auc = calculate_auc(logits_per_image, class_inds)\n",
    "                auc_meter.update(auc, len(class_inds))\n",
    "            \n",
    "                acc = multilabel_logit_accuracy(logits_per_image, class_inds)\n",
    "                acc_meter.update(acc, len(class_inds))\n",
    "\n",
    "                spec = specificity(logits_per_image, class_inds)\n",
    "                spec_meter.update(spec.item(), len(class_inds))\n",
    "                rec = recall(logits_per_image, class_inds)\n",
    "                rec_meter.update(rec.item(), len(class_inds))\n",
    "                prec = precision(logits_per_image, class_inds)\n",
    "                print(f\"F1 {f1} | Accuracy {acc} | AUC {auc} | Specificity {spec} | Recall {rec} | Precision {prec}\")\n",
    "            \n",
    "    return f1_meter.average(), acc_meter.average(), auc_meter.average(), spec_meter.average(), rec_meter.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs6240-1/lib/python3.9/site-packages/torch/functional.py:791: UserWarning: The operator 'aten::_unique2' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  output, inverse_indices, counts = torch._unique2(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.1788504421710968 | Accuracy 0.30918367346938774 | AUC 0.4892710192307759 | Specificity 0.2857142984867096 | Recall 0.7142857313156128 | Precision 0.11530613154172897\n",
      "F1 0.19723308086395264 | Accuracy 0.3346938775510204 | AUC 0.47060236046163706 | Specificity 0.2857142984867096 | Recall 0.7142857313156128 | Precision 0.12959183752536774\n",
      "F1 0.17511191964149475 | Accuracy 0.30612244897959184 | AUC 0.511020381339966 | Specificity 0.2857142984867096 | Recall 0.7142857313156128 | Precision 0.11122449487447739\n",
      "F1 0.1742706298828125 | Accuracy 0.30612244897959184 | AUC 0.46802414855294805 | Specificity 0.2857142984867096 | Recall 0.7142857313156128 | Precision 0.11122449487447739\n",
      "F1 0.19095423817634583 | Accuracy 0.32653061224489793 | AUC 0.4958032233303745 | Specificity 0.2857142984867096 | Recall 0.7142857313156128 | Precision 0.12551021575927734\n",
      "F1 0.19806042313575745 | Accuracy 0.32755102040816325 | AUC 0.41697510452881664 | Specificity 0.2857142984867096 | Recall 0.7142857313156128 | Precision 0.12448980659246445\n",
      "F1 0.1950427144765854 | Accuracy 0.32346938775510203 | AUC 0.47814726416552683 | Specificity 0.2857142984867096 | Recall 0.7142857313156128 | Precision 0.12755101919174194\n",
      "F1 0.17979764938354492 | Accuracy 0.3173469387755102 | AUC 0.4850612646753719 | Specificity 0.2857142984867096 | Recall 0.7142857313156128 | Precision 0.11632654070854187\n",
      "F1 0.1836666762828827 | Accuracy 0.3295918367346939 | AUC 0.47881088015331824 | Specificity 0.2857142686843872 | Recall 0.7142857313156128 | Precision 0.11734694242477417\n",
      "F1 0.20594635605812073 | Accuracy 0.33979591836734696 | AUC 0.44418967009827465 | Specificity 0.2857142984867096 | Recall 0.7142857313156128 | Precision 0.13571429252624512\n",
      "F1 0.19708451628684998 | Accuracy 0.3235653235653236 | AUC 0.4517806715351487 | Specificity 0.2857142984867096 | Recall 0.7142857313156128 | Precision 0.1269841492176056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.18860228517846048,\n",
       " 0.322158395329127,\n",
       " 0.4720930012424481,\n",
       " 0.28571429573633045,\n",
       " 0.7142857313156128)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = Dataset(config.img_path, config.img_info, config.img_info[config.img_info['meta_split'] == 'test']['image_id'].to_list(), config.label_names_map, config.classes_split_map['test'], mean_std=config.mean_std)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "run_eval(mtrainer.model, test_loader, device, test_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.19492429494857788 | Accuracy 0.3948979591836735 | AUC 0.44718458887647267 | Specificity 0.35591596364974976 | Recall 0.5714285969734192 | Precision 0.12244898080825806\n",
      "F1 0.20341134071350098 | Accuracy 0.4010204081632653 | AUC 0.5006839339776653 | Specificity 0.3451247215270996 | Recall 0.581632673740387 | Precision 0.13001494109630585\n",
      "F1 0.19422617554664612 | Accuracy 0.386734693877551 | AUC 0.48136104522629214 | Specificity 0.3431149423122406 | Recall 0.5904762148857117 | Precision 0.12210884690284729\n",
      "F1 0.20395749807357788 | Accuracy 0.3877551020408163 | AUC 0.5019968995514166 | Specificity 0.34141138195991516 | Recall 0.6181318759918213 | Precision 0.1273934245109558\n",
      "F1 0.17863449454307556 | Accuracy 0.373469387755102 | AUC 0.4428214562747441 | Specificity 0.354105144739151 | Recall 0.5714285969734192 | Precision 0.11122448742389679\n",
      "F1 0.1938256323337555 | Accuracy 0.3836734693877551 | AUC 0.4724027450705705 | Specificity 0.3388245701789856 | Recall 0.5714285969734192 | Precision 0.12346938997507095\n",
      "F1 0.19917598366737366 | Accuracy 0.36122448979591837 | AUC 0.461654993878183 | Specificity 0.32766443490982056 | Recall 0.617347002029419 | Precision 0.12245268374681473\n",
      "F1 0.18717122077941895 | Accuracy 0.373469387755102 | AUC 0.5234225232779475 | Specificity 0.3413277864456177 | Recall 0.590062141418457 | Precision 0.11836735904216766\n",
      "F1 0.20728418231010437 | Accuracy 0.38571428571428573 | AUC 0.423756047491805 | Specificity 0.3522389531135559 | Recall 0.5873016119003296 | Precision 0.13038548827171326\n",
      "F1 0.21726083755493164 | Accuracy 0.3979591836734694 | AUC 0.4459127976848554 | Specificity 0.3541337251663208 | Recall 0.6130952835083008 | Precision 0.13858163356781006\n",
      "F1 0.20490825176239014 | Accuracy 0.38461538461538464 | AUC 0.48655851661530664 | Specificity 0.34752750396728516 | Recall 0.6109890341758728 | Precision 0.12878592312335968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.19852096105612024,\n",
       " 0.38459365288633574,\n",
       " 0.47138756152101796,\n",
       " 0.34555131530950317,\n",
       " 0.5927569414725332)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_eval(mtrainer.best_model, test_loader, device, test_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.262796014547348 | Accuracy 0.30306122448979594 | AUC 0.471879121071541 | Specificity 0.21807709336280823 | Recall 0.7181841135025024 | Precision 0.19198301434516907\n",
      "F1 0.2434573471546173 | Accuracy 0.31326530612244896 | AUC 0.490890122745222 | Specificity 0.24420768022537231 | Recall 0.7156593799591064 | Precision 0.19905515015125275\n",
      "F1 0.24523988366127014 | Accuracy 0.30612244897959184 | AUC 0.4906162395933992 | Specificity 0.24129928648471832 | Recall 0.7169082760810852 | Precision 0.19371946156024933\n",
      "F1 0.2599810063838959 | Accuracy 0.3408163265306122 | AUC 0.5007050690120927 | Specificity 0.25909245014190674 | Recall 0.747619092464447 | Precision 0.20544257760047913\n",
      "F1 0.2756465673446655 | Accuracy 0.30714285714285716 | AUC 0.4967361411829624 | Specificity 0.2088400423526764 | Recall 0.810031533241272 | Precision 0.20025786757469177\n",
      "F1 0.28784602880477905 | Accuracy 0.31326530612244896 | AUC 0.5232660702535715 | Specificity 0.21574905514717102 | Recall 0.8339827656745911 | Precision 0.21618740260601044\n",
      "F1 0.2766427993774414 | Accuracy 0.33979591836734696 | AUC 0.47615351008903933 | Specificity 0.24990400671958923 | Recall 0.7128427028656006 | Precision 0.22268682718276978\n",
      "F1 0.23512043058872223 | Accuracy 0.2897959183673469 | AUC 0.43888662388175714 | Specificity 0.2141905128955841 | Recall 0.7164851427078247 | Precision 0.17284110188484192\n",
      "F1 0.2589722275733948 | Accuracy 0.29591836734693877 | AUC 0.46803037426019595 | Specificity 0.2169429212808609 | Recall 0.7139765024185181 | Precision 0.20883606374263763\n",
      "F1 0.25303971767425537 | Accuracy 0.32857142857142857 | AUC 0.42403072113019563 | Specificity 0.2616077661514282 | Recall 0.7180375456809998 | Precision 0.20797768235206604\n",
      "F1 0.253878653049469 | Accuracy 0.326007326007326 | AUC 0.547101471675124 | Specificity 0.252621591091156 | Recall 0.8067494630813599 | Precision 0.19516609609127045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25941179013990934,\n",
       " 0.3147189000847537,\n",
       " 0.48343970417718274,\n",
       " 0.23450510232324417,\n",
       " 0.7454920730546815)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_eval(mtrainer.model, test_loader, device, test_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_eval(mtrainer.best_model, query_loader, device, query_dataset.class_labels())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
