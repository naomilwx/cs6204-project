{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.device import get_device\n",
    "from utils.data import DatasetConfig\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "\n",
    "IMG_PATH = 'datasets/vindr-cxr-png'\n",
    "\n",
    "NUM_SHOTS = 5\n",
    "NUM_WAYS = 7\n",
    "TRAIN_NUM_WAYS= 7\n",
    "dataset_config = DatasetConfig(IMG_PATH, 'data/vindr_cxr_split_labels.pkl', 'data/vindr_train_query_set.pkl', VINDR_CXR_LABELS, VINDR_SPLIT, MEAN_STDS['chestmnist'])\n",
    "device =  get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models with attention\n",
    "### Run experiments on proposed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.attention.model import LabelImageAttention, LabelImagePrototypeModel\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "\n",
    "from utils.prototype import class_prototype_inf\n",
    "from models.metaclassifier.trainer import ControlledMetaTrainer\n",
    "from models.metaclassifier.model import ClsModel\n",
    "\n",
    "\n",
    "encoder = torch.load('models/embedding/model/imgtext_model_trained.pth')\n",
    "encoder.text_model.device = device\n",
    "attention = torch.load('models/attention/model/attention-model-8h4l.pth')\n",
    "model = ClsModel(encoder, attention, 512, class_prototype_inf, fc_hidden_size=16)\n",
    "mtrainer = ControlledMetaTrainer(model, NUM_SHOTS, NUM_WAYS, dataset_config, train_n_ways=TRAIN_NUM_WAYS, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrainer.run_eval(mtrainer.model, mtrainer.test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.361516034985424, 0.5202569099543621, 0.7382618546485901)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.model, mtrainer.val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 0.6902595162391663 | Acc 53.46938775510204\n",
      "Batch 2: loss 0.6968538761138916 | Acc 42.44897959183673\n",
      "Batch 3: loss 0.7003634770711263 | Acc 37.95918367346939\n",
      "Batch 4: loss 0.6974166184663773 | Acc 53.46938775510204\n",
      "Batch 5: loss 0.6979597449302674 | Acc 44.08163265306123\n",
      "Batch 6: loss 0.6973948379357656 | Acc 48.97959183673469\n",
      "Batch 7: loss 0.6978198204721723 | Acc 44.08163265306123\n",
      "Batch 8: loss 0.6980315819382668 | Acc 45.714285714285715\n",
      "Batch 9: loss 0.6969953179359436 | Acc 53.87755102040816\n",
      "Batch 10: loss 0.6979582905769348 | Acc 39.183673469387756\n",
      "Batch 11: loss 0.6966771320863203 | Acc 57.55102040816327\n",
      "Batch 12: loss 0.6973831256230673 | Acc 40.0\n",
      "Batch 13: loss 0.6974497345777658 | Acc 46.53061224489796\n",
      "Batch 14: loss 0.6974168930734906 | Acc 46.93877551020408\n",
      "Batch 15: loss 0.697468622525533 | Acc 44.89795918367347\n",
      "Batch 16: loss 0.6972164176404476 | Acc 49.38775510204081\n",
      "Batch 17: loss 0.6974642136517692 | Acc 43.26530612244898\n",
      "Batch 18: loss 0.6976404885450999 | Acc 44.48979591836735\n",
      "Batch 19: loss 0.697437584400177 | Acc 50.61224489795918\n",
      "Batch 20: loss 0.6982869148254395 | Acc 32.6530612244898\n",
      "Batch 21: loss 0.6977014598392305 | Acc 53.87755102040816\n",
      "Batch 22: loss 0.6983142820271578 | Acc 36.3265306122449\n",
      "Batch 23: loss 0.6985053549642148 | Acc 42.04081632653061\n",
      "Batch 24: loss 0.6985820308327675 | Acc 44.08163265306123\n",
      "Batch 25: loss 0.6991787457466125 | Acc 32.6530612244898\n",
      "Batch 26: loss 0.6985859756286328 | Acc 57.95918367346938\n",
      "Batch 27: loss 0.698534177409278 | Acc 46.12244897959184\n",
      "Batch 28: loss 0.6989063514130456 | Acc 37.95918367346939\n",
      "Batch 29: loss 0.6984526843860231 | Acc 54.69387755102041\n",
      "Batch 30: loss 0.6989006300767263 | Acc 34.285714285714285\n",
      "Batch 31: loss 0.6992459412544004 | Acc 37.142857142857146\n",
      "Batch 32: loss 0.6990807168185711 | Acc 48.97959183673469\n",
      "Batch 33: loss 0.6990183772462787 | Acc 48.57142857142857\n",
      "Batch 34: loss 0.6992399955497068 | Acc 39.183673469387756\n",
      "Batch 35: loss 0.6993974498340062 | Acc 41.224489795918366\n",
      "Batch 36: loss 0.6993901464674208 | Acc 46.12244897959184\n",
      "Batch 37: loss 0.6994660706133455 | Acc 42.04081632653061\n",
      "Batch 38: loss 0.699384890104595 | Acc 46.93877551020408\n",
      "Batch 39: loss 0.6990496516227722 | Acc 55.10204081632652\n",
      "Batch 40: loss 0.6992146909236908 | Acc 38.775510204081634\n",
      "Batch 41: loss 0.6991479992866516 | Acc 46.93877551020408\n",
      "Batch 42: loss 0.6992222425483522 | Acc 42.44897959183673\n",
      "Batch 43: loss 0.6992232841114665 | Acc 44.89795918367347\n",
      "Batch 44: loss 0.6990917176008224 | Acc 48.97959183673469\n",
      "Batch 45: loss 0.6990066965421041 | Acc 49.38775510204081\n",
      "Batch 46: loss 0.6991237687027972 | Acc 40.0\n",
      "Batch 47: loss 0.6992306709289551 | Acc 40.0\n",
      "Batch 48: loss 0.6991085310777029 | Acc 51.02040816326531\n",
      "Batch 49: loss 0.6991061689902325 | Acc 44.08163265306123\n",
      "Batch 50: loss 0.6991305923461915 | Acc 43.26530612244898\n",
      "Batch 51: loss 0.6990558201191472 | Acc 47.3469387755102\n",
      "Batch 52: loss 0.6992611965307822 | Acc 36.734693877551024\n",
      "Batch 53: loss 0.6992889642715454 | Acc 42.857142857142854\n",
      "Batch 54: loss 0.6991484540480154 | Acc 49.38775510204081\n",
      "Batch 55: loss 0.6991740075024692 | Acc 44.08163265306123\n",
      "Batch 56: loss 0.6989562213420868 | Acc 54.285714285714285\n",
      "Batch 57: loss 0.6990251248342949 | Acc 40.40816326530612\n",
      "Batch 58: loss 0.6988698706544679 | Acc 53.06122448979592\n",
      "Batch 59: loss 0.6988935207916518 | Acc 44.48979591836735\n",
      "Batch 60: loss 0.6988354702790578 | Acc 46.93877551020408\n",
      "Batch 61: loss 0.6987212700921981 | Acc 51.42857142857142\n",
      "Batch 62: loss 0.6989561280896587 | Acc 34.285714285714285\n",
      "Batch 63: loss 0.6989159129914784 | Acc 46.93877551020408\n",
      "Batch 64: loss 0.6989387217909098 | Acc 44.08163265306123\n",
      "Batch 65: loss 0.698858891083644 | Acc 50.204081632653065\n",
      "Batch 66: loss 0.6989195048809052 | Acc 40.816326530612244\n",
      "Batch 67: loss 0.6988088737672834 | Acc 53.06122448979592\n",
      "Batch 68: loss 0.6989070755593917 | Acc 38.775510204081634\n",
      "Batch 69: loss 0.6987871475841688 | Acc 51.83673469387755\n",
      "Batch 70: loss 0.6988296244825636 | Acc 41.63265306122449\n",
      "Batch 71: loss 0.6988656957384566 | Acc 42.04081632653061\n",
      "Batch 72: loss 0.6988107437888781 | Acc 48.57142857142857\n",
      "Batch 73: loss 0.6986776745482667 | Acc 51.42857142857142\n",
      "Batch 74: loss 0.698747763762603 | Acc 41.63265306122449\n",
      "Batch 75: loss 0.6988968404134115 | Acc 34.285714285714285\n",
      "Batch 76: loss 0.6988241350964496 | Acc 49.38775510204081\n",
      "Batch 77: loss 0.6987839542426072 | Acc 47.755102040816325\n",
      "Batch 78: loss 0.6988224593492655 | Acc 42.857142857142854\n",
      "Batch 79: loss 0.6986470396005655 | Acc 55.10204081632652\n",
      "Batch 80: loss 0.6988065771758556 | Acc 35.10204081632653\n",
      "Batch 81: loss 0.6988347974824317 | Acc 43.26530612244898\n",
      "Batch 82: loss 0.6987368362705882 | Acc 51.02040816326531\n",
      "Batch 83: loss 0.6987857509808368 | Acc 42.44897959183673\n",
      "Batch 84: loss 0.6988106157098498 | Acc 41.63265306122449\n",
      "Batch 85: loss 0.6988123213543611 | Acc 44.48979591836735\n",
      "Batch 86: loss 0.6988363716491434 | Acc 43.673469387755105\n",
      "Batch 87: loss 0.698715095547424 | Acc 53.87755102040816\n",
      "Batch 88: loss 0.6988034905357794 | Acc 36.3265306122449\n",
      "Batch 89: loss 0.6987988185346796 | Acc 44.48979591836735\n",
      "Batch 90: loss 0.6988417757882013 | Acc 42.04081632653061\n",
      "Batch 91: loss 0.6988565908683525 | Acc 42.857142857142854\n",
      "Batch 92: loss 0.698939332495565 | Acc 40.0\n",
      "Batch 93: loss 0.6989762449777255 | Acc 41.63265306122449\n",
      "Batch 94: loss 0.6988571292542397 | Acc 53.46938775510204\n",
      "Batch 95: loss 0.6988501630331341 | Acc 46.53061224489796\n",
      "Batch 96: loss 0.698791311432918 | Acc 49.795918367346935\n",
      "Batch 97: loss 0.6987897385026991 | Acc 42.857142857142854\n",
      "Batch 98: loss 0.6987442733073721 | Acc 50.61224489795918\n",
      "Epoch 1: Training loss 0.6987442733073721 | Acc: 45.168679716784695\n",
      "Epoch 1: Validation loss 0.736474529334477 | Accuracy 20.769679300291543 | AUC 0.4569750506480995\n",
      "Batch 1: loss 0.7044628858566284 | Acc 38.775510204081634\n",
      "Batch 2: loss 0.6940997242927551 | Acc 57.55102040816327\n",
      "Batch 3: loss 0.6945532957712809 | Acc 47.755102040816325\n",
      "Batch 4: loss 0.6950720399618149 | Acc 47.3469387755102\n",
      "Batch 5: loss 0.6966105818748474 | Acc 41.63265306122449\n",
      "Batch 6: loss 0.6953600943088531 | Acc 54.285714285714285\n",
      "Batch 7: loss 0.6928992526871818 | Acc 62.04081632653061\n",
      "Batch 8: loss 0.6949512287974358 | Acc 35.91836734693877\n",
      "Batch 9: loss 0.6966120335790846 | Acc 33.87755102040816\n",
      "Batch 10: loss 0.6944658756256104 | Acc 64.08163265306122\n",
      "Batch 11: loss 0.6938001892783425 | Acc 54.69387755102041\n",
      "Batch 12: loss 0.6951030741135279 | Acc 35.91836734693877\n",
      "Batch 13: loss 0.6956120821145865 | Acc 42.44897959183673\n",
      "Batch 14: loss 0.6956028640270233 | Acc 45.30612244897959\n",
      "Batch 15: loss 0.6964484135309855 | Acc 37.142857142857146\n",
      "Batch 16: loss 0.695210438221693 | Acc 64.48979591836735\n",
      "Batch 17: loss 0.6954904514200547 | Acc 44.08163265306123\n",
      "Batch 18: loss 0.6954486270745596 | Acc 46.93877551020408\n",
      "Batch 19: loss 0.6954178433669241 | Acc 47.755102040816325\n",
      "Batch 20: loss 0.6958093285560608 | Acc 40.40816326530612\n",
      "Batch 21: loss 0.6960378885269165 | Acc 42.44897959183673\n",
      "Batch 22: loss 0.6958069665865465 | Acc 50.204081632653065\n",
      "Batch 23: loss 0.6962297662444736 | Acc 37.142857142857146\n",
      "Batch 24: loss 0.6961251969138781 | Acc 50.61224489795918\n",
      "Batch 25: loss 0.6962594795227051 | Acc 46.93877551020408\n",
      "Batch 26: loss 0.6963288050431472 | Acc 42.857142857142854\n",
      "Batch 27: loss 0.6968896300704391 | Acc 34.285714285714285\n",
      "Batch 28: loss 0.6967681348323822 | Acc 48.57142857142857\n",
      "Batch 29: loss 0.6969031465464625 | Acc 41.63265306122449\n",
      "Batch 30: loss 0.6967681209246318 | Acc 48.57142857142857\n",
      "Batch 31: loss 0.6967145127634848 | Acc 45.714285714285715\n",
      "Batch 32: loss 0.6968089230358601 | Acc 44.89795918367347\n",
      "Batch 33: loss 0.6969500346617266 | Acc 42.857142857142854\n",
      "Batch 34: loss 0.6969892698175767 | Acc 45.714285714285715\n",
      "Batch 35: loss 0.6971894025802612 | Acc 38.775510204081634\n",
      "Batch 36: loss 0.6971558911932839 | Acc 46.53061224489796\n",
      "Batch 37: loss 0.6971979640625618 | Acc 44.08163265306123\n",
      "Batch 38: loss 0.6974332834544935 | Acc 36.734693877551024\n",
      "Batch 39: loss 0.6972491909296085 | Acc 53.46938775510204\n",
      "Batch 40: loss 0.6974675476551055 | Acc 33.87755102040816\n",
      "Batch 41: loss 0.6973858519298274 | Acc 48.57142857142857\n",
      "Batch 42: loss 0.6974053269340879 | Acc 44.08163265306123\n",
      "Batch 43: loss 0.6974436737770258 | Acc 44.48979591836735\n",
      "Batch 44: loss 0.6974154331467368 | Acc 47.3469387755102\n",
      "Batch 45: loss 0.6977510518497891 | Acc 33.06122448979592\n",
      "Batch 46: loss 0.6973712677540986 | Acc 60.816326530612244\n",
      "Batch 47: loss 0.6972544041085751 | Acc 51.83673469387755\n",
      "Batch 48: loss 0.6973107680678368 | Acc 42.04081632653061\n",
      "Batch 49: loss 0.6972122897907179 | Acc 49.795918367346935\n",
      "Batch 50: loss 0.69745645403862 | Acc 33.87755102040816\n",
      "Batch 51: loss 0.6972631531603196 | Acc 54.285714285714285\n",
      "Batch 52: loss 0.6975054717980899 | Acc 34.69387755102041\n",
      "Batch 53: loss 0.6973518436809756 | Acc 53.06122448979592\n",
      "Batch 54: loss 0.6975735988881853 | Acc 32.244897959183675\n",
      "Batch 55: loss 0.6974888324737549 | Acc 49.795918367346935\n",
      "Batch 56: loss 0.6974074042269162 | Acc 45.30612244897959\n",
      "Batch 57: loss 0.6974303994262427 | Acc 43.26530612244898\n",
      "Batch 58: loss 0.6974328057519321 | Acc 46.93877551020408\n",
      "Batch 59: loss 0.6975121780977411 | Acc 42.04081632653061\n",
      "Batch 60: loss 0.6973253617684047 | Acc 53.46938775510204\n",
      "Batch 61: loss 0.6974368710986903 | Acc 42.44897959183673\n",
      "Batch 62: loss 0.6973750139436414 | Acc 48.97959183673469\n",
      "Batch 63: loss 0.6972667347817194 | Acc 49.795918367346935\n",
      "Batch 64: loss 0.6974086724221706 | Acc 37.142857142857146\n",
      "Batch 65: loss 0.697331337745373 | Acc 51.42857142857142\n",
      "Batch 66: loss 0.6974015777761285 | Acc 41.224489795918366\n",
      "Batch 67: loss 0.6975743654948562 | Acc 33.46938775510204\n",
      "Batch 68: loss 0.6973868108847562 | Acc 57.95918367346938\n",
      "Batch 69: loss 0.6974135360855988 | Acc 43.673469387755105\n",
      "Batch 70: loss 0.6973637342453003 | Acc 50.61224489795918\n",
      "Batch 71: loss 0.697402684621408 | Acc 41.224489795918366\n",
      "Batch 72: loss 0.6972872498962615 | Acc 53.87755102040816\n",
      "Batch 73: loss 0.6973685940651044 | Acc 38.36734693877551\n",
      "Batch 74: loss 0.6973473260531554 | Acc 49.795918367346935\n",
      "Batch 75: loss 0.6974197753270467 | Acc 40.0\n",
      "Batch 76: loss 0.6973787836338344 | Acc 48.16326530612245\n",
      "Batch 77: loss 0.6974248746772865 | Acc 40.816326530612244\n",
      "Batch 78: loss 0.6974543401828179 | Acc 46.12244897959184\n",
      "Batch 79: loss 0.6975245875648305 | Acc 38.36734693877551\n",
      "Batch 80: loss 0.6974681831896306 | Acc 51.83673469387755\n",
      "Batch 81: loss 0.6975148916244507 | Acc 42.857142857142854\n",
      "Batch 82: loss 0.6974858852421365 | Acc 45.30612244897959\n",
      "Batch 83: loss 0.6974202775093447 | Acc 50.61224489795918\n",
      "Batch 84: loss 0.6974761173838661 | Acc 39.59183673469388\n",
      "Batch 85: loss 0.6975127689978655 | Acc 45.30612244897959\n",
      "Batch 86: loss 0.6974710662697636 | Acc 45.714285714285715\n",
      "Batch 87: loss 0.6975338137012789 | Acc 39.59183673469388\n",
      "Batch 88: loss 0.697557811032642 | Acc 42.04081632653061\n",
      "Batch 89: loss 0.6975906558251113 | Acc 40.40816326530612\n",
      "Batch 90: loss 0.6974085032939911 | Acc 57.95918367346938\n",
      "Batch 91: loss 0.6974860901360983 | Acc 40.816326530612244\n",
      "Batch 92: loss 0.6975787029318188 | Acc 36.3265306122449\n",
      "Batch 93: loss 0.6976746839861716 | Acc 35.10204081632653\n",
      "Batch 94: loss 0.6976825975357218 | Acc 44.48979591836735\n",
      "Batch 95: loss 0.6976946284896449 | Acc 45.714285714285715\n",
      "Batch 96: loss 0.6976971955349048 | Acc 44.89795918367347\n",
      "Batch 97: loss 0.6977878845844072 | Acc 36.3265306122449\n",
      "Batch 98: loss 0.69767649198065 | Acc 55.10204081632652\n",
      "Epoch 2: Training loss 0.69767649198065 | Acc: 45.13119533527698\n",
      "Epoch 2: Validation loss 0.735885557106563 | Accuracy 20.851311953352774 | AUC 0.45497777255634475\n",
      "Best epoch:  2\n"
     ]
    }
   ],
   "source": [
    "# 5 times?\n",
    "mtrainer.run_train(2, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19.93031358885018, 0.507083518313992, 0.7321400235338908)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, mtrainer.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrainer.model.attn_model.set_trainable(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 0.6971045732498169 | Acc 42.44897959183673\n",
      "Batch 2: loss 0.6954399049282074 | Acc 48.16326530612245\n",
      "Batch 3: loss 0.6974401871363322 | Acc 39.183673469387756\n",
      "Batch 4: loss 0.696072444319725 | Acc 50.61224489795918\n",
      "Batch 5: loss 0.693632709980011 | Acc 60.0\n",
      "Batch 6: loss 0.6950256526470184 | Acc 37.55102040816327\n",
      "Batch 7: loss 0.6943262900624957 | Acc 51.83673469387755\n",
      "Batch 8: loss 0.6948138996958733 | Acc 42.44897959183673\n",
      "Batch 9: loss 0.6948209206263224 | Acc 46.93877551020408\n",
      "Batch 10: loss 0.6951384782791138 | Acc 43.26530612244898\n",
      "Batch 11: loss 0.6944867806001143 | Acc 52.6530612244898\n",
      "Batch 12: loss 0.6939454972743988 | Acc 51.42857142857142\n",
      "Batch 13: loss 0.6941679395162142 | Acc 44.89795918367347\n",
      "Batch 14: loss 0.6944387938295092 | Acc 46.12244897959184\n",
      "Batch 15: loss 0.6944648385047912 | Acc 48.16326530612245\n",
      "Batch 16: loss 0.6943309046328068 | Acc 47.3469387755102\n",
      "Batch 17: loss 0.6938948526101953 | Acc 55.51020408163265\n",
      "Batch 18: loss 0.6942294736703237 | Acc 41.224489795918366\n",
      "Batch 19: loss 0.6947939239050213 | Acc 36.734693877551024\n",
      "Batch 20: loss 0.6946301907300949 | Acc 51.83673469387755\n",
      "Batch 21: loss 0.695044926234654 | Acc 40.0\n",
      "Batch 22: loss 0.6949186406352303 | Acc 51.42857142857142\n",
      "Batch 23: loss 0.6950044165486875 | Acc 46.93877551020408\n",
      "Batch 24: loss 0.6955882335702578 | Acc 38.36734693877551\n",
      "Batch 25: loss 0.6958429741859437 | Acc 44.48979591836735\n",
      "Batch 26: loss 0.696150506918247 | Acc 41.63265306122449\n",
      "Batch 27: loss 0.6961055221381011 | Acc 49.795918367346935\n",
      "Batch 28: loss 0.696749883038657 | Acc 32.6530612244898\n",
      "Batch 29: loss 0.6974111302145596 | Acc 31.428571428571427\n",
      "Batch 30: loss 0.6972325662771861 | Acc 52.244897959183675\n",
      "Batch 31: loss 0.697345627892402 | Acc 44.48979591836735\n",
      "Batch 32: loss 0.6974859405308962 | Acc 43.673469387755105\n",
      "Batch 33: loss 0.6976434552308285 | Acc 42.04081632653061\n",
      "Batch 34: loss 0.6976613682859084 | Acc 46.93877551020408\n",
      "Batch 35: loss 0.69798298733575 | Acc 35.51020408163265\n",
      "Batch 36: loss 0.6979411774211459 | Acc 47.755102040816325\n",
      "Batch 37: loss 0.6980928198711293 | Acc 41.224489795918366\n",
      "Batch 38: loss 0.6980905579893213 | Acc 46.93877551020408\n",
      "Batch 39: loss 0.6981622989361103 | Acc 44.48979591836735\n",
      "Batch 40: loss 0.698043592274189 | Acc 49.38775510204081\n",
      "Batch 41: loss 0.6980306564307794 | Acc 46.93877551020408\n",
      "Batch 42: loss 0.6981203556060791 | Acc 41.224489795918366\n",
      "Batch 43: loss 0.6982590472975443 | Acc 38.36734693877551\n",
      "Batch 44: loss 0.6979313465681943 | Acc 60.816326530612244\n",
      "Batch 45: loss 0.6977917247348362 | Acc 52.6530612244898\n",
      "Batch 46: loss 0.6977966790613921 | Acc 44.48979591836735\n",
      "Batch 47: loss 0.6978722252744309 | Acc 41.63265306122449\n",
      "Batch 48: loss 0.6978417771557966 | Acc 46.93877551020408\n",
      "Batch 49: loss 0.6978924031160316 | Acc 44.48979591836735\n",
      "Batch 50: loss 0.6979076039791107 | Acc 42.04081632653061\n",
      "Batch 51: loss 0.6979703657767352 | Acc 42.44897959183673\n",
      "Batch 52: loss 0.6977408769039007 | Acc 56.734693877551024\n",
      "Batch 53: loss 0.6975521411535874 | Acc 55.91836734693878\n",
      "Batch 54: loss 0.6976068947050307 | Acc 42.04081632653061\n",
      "Batch 55: loss 0.6975973205132918 | Acc 46.12244897959184\n",
      "Batch 56: loss 0.6975642048886844 | Acc 50.61224489795918\n",
      "Batch 57: loss 0.6975233983575252 | Acc 47.755102040816325\n",
      "Batch 58: loss 0.6975325130183121 | Acc 44.08163265306123\n",
      "Batch 59: loss 0.6975550166631149 | Acc 44.48979591836735\n",
      "Batch 60: loss 0.6974895348151525 | Acc 50.61224489795918\n",
      "Batch 61: loss 0.6974558361241074 | Acc 46.12244897959184\n",
      "Batch 62: loss 0.6974802074893829 | Acc 44.89795918367347\n",
      "Batch 63: loss 0.6972833077112833 | Acc 60.40816326530612\n",
      "Batch 64: loss 0.697296797297895 | Acc 44.48979591836735\n",
      "Batch 65: loss 0.6971664208632249 | Acc 54.285714285714285\n",
      "Batch 66: loss 0.6971752020445737 | Acc 45.30612244897959\n",
      "Batch 67: loss 0.6970892762070271 | Acc 52.6530612244898\n",
      "Batch 68: loss 0.6970981771455091 | Acc 44.89795918367347\n",
      "Batch 69: loss 0.6970813447150631 | Acc 46.93877551020408\n",
      "Batch 70: loss 0.6969942390918732 | Acc 53.46938775510204\n",
      "Batch 71: loss 0.6969931696502256 | Acc 44.89795918367347\n",
      "Batch 72: loss 0.696926425728533 | Acc 51.02040816326531\n",
      "Batch 73: loss 0.6968418498561807 | Acc 52.6530612244898\n",
      "Batch 74: loss 0.6968813401621741 | Acc 42.04081632653061\n",
      "Batch 75: loss 0.6969148286183675 | Acc 42.04081632653061\n",
      "Batch 76: loss 0.6968359170775664 | Acc 53.46938775510204\n",
      "Batch 77: loss 0.6967693907873971 | Acc 52.244897959183675\n",
      "Batch 78: loss 0.6967939245395172 | Acc 42.44897959183673\n",
      "Batch 79: loss 0.6968077609810648 | Acc 42.857142857142854\n",
      "Batch 80: loss 0.6967840440571308 | Acc 48.97959183673469\n",
      "Batch 81: loss 0.6967891035256563 | Acc 44.08163265306123\n",
      "Batch 82: loss 0.6967087010057961 | Acc 56.326530612244895\n",
      "Batch 83: loss 0.6966860832938229 | Acc 45.714285714285715\n",
      "Batch 84: loss 0.6966177267687661 | Acc 55.10204081632652\n",
      "Batch 85: loss 0.6966347112375147 | Acc 43.26530612244898\n",
      "Batch 86: loss 0.6966115111528441 | Acc 48.16326530612245\n",
      "Batch 87: loss 0.6965576389740253 | Acc 54.285714285714285\n",
      "Batch 88: loss 0.6965776377103545 | Acc 43.26530612244898\n",
      "Batch 89: loss 0.696546837185206 | Acc 48.16326530612245\n",
      "Batch 90: loss 0.6965786808066898 | Acc 39.59183673469388\n",
      "Batch 91: loss 0.6965633126405569 | Acc 47.755102040816325\n",
      "Batch 92: loss 0.6965153703223104 | Acc 53.46938775510204\n",
      "Batch 93: loss 0.6964508660377995 | Acc 55.91836734693878\n",
      "Batch 94: loss 0.6964192193873385 | Acc 47.755102040816325\n",
      "Batch 95: loss 0.6963630406480087 | Acc 54.69387755102041\n",
      "Batch 96: loss 0.6963626214613517 | Acc 44.08163265306123\n",
      "Batch 97: loss 0.6963505904699109 | Acc 44.08163265306123\n",
      "Batch 98: loss 0.6962621309319321 | Acc 58.77551020408164\n",
      "Epoch 1: Training loss 0.6962621309319321 | Acc: 46.947105372761335\n",
      "Epoch 1: Validation loss 0.7044891612870353 | Accuracy 28.408163265306115 | AUC 0.528573254872334\n",
      "Batch 1: loss 0.6911651492118835 | Acc 48.16326530612245\n",
      "Batch 2: loss 0.6906952559947968 | Acc 55.10204081632652\n",
      "Batch 3: loss 0.6897823413213094 | Acc 59.183673469387756\n",
      "Batch 4: loss 0.6918270438909531 | Acc 41.63265306122449\n",
      "Batch 5: loss 0.6905031204223633 | Acc 57.14285714285714\n",
      "Batch 6: loss 0.6900751193364462 | Acc 57.95918367346938\n",
      "Batch 7: loss 0.6898397973605565 | Acc 59.591836734693885\n",
      "Batch 8: loss 0.6902012228965759 | Acc 47.755102040816325\n",
      "Batch 9: loss 0.6901301278008355 | Acc 58.77551020408164\n",
      "Batch 10: loss 0.6902716994285584 | Acc 54.285714285714285\n",
      "Batch 11: loss 0.6901290850205855 | Acc 57.55102040816327\n",
      "Batch 12: loss 0.6904562413692474 | Acc 49.795918367346935\n",
      "Batch 13: loss 0.6909165382385254 | Acc 45.30612244897959\n",
      "Batch 14: loss 0.6908481461661202 | Acc 55.91836734693878\n",
      "Batch 15: loss 0.6908385912577312 | Acc 50.61224489795918\n",
      "Batch 16: loss 0.690532635897398 | Acc 57.14285714285714\n",
      "Batch 17: loss 0.6903846894993502 | Acc 59.183673469387756\n",
      "Batch 18: loss 0.6904270516501533 | Acc 49.795918367346935\n",
      "Batch 19: loss 0.6902600464067961 | Acc 59.183673469387756\n",
      "Batch 20: loss 0.690324142575264 | Acc 49.795918367346935\n",
      "Batch 21: loss 0.6903990705808004 | Acc 50.61224489795918\n",
      "Batch 22: loss 0.69023090059107 | Acc 58.36734693877551\n",
      "Batch 23: loss 0.6900481840838557 | Acc 58.77551020408164\n",
      "Batch 24: loss 0.6901360750198364 | Acc 53.87755102040816\n",
      "Batch 25: loss 0.6901242423057556 | Acc 54.285714285714285\n",
      "Batch 26: loss 0.6899504157213064 | Acc 59.183673469387756\n",
      "Batch 27: loss 0.6901089306230899 | Acc 45.30612244897959\n",
      "Batch 28: loss 0.6900407139744077 | Acc 58.36734693877551\n",
      "Batch 29: loss 0.689971689520211 | Acc 59.591836734693885\n",
      "Batch 30: loss 0.6900323410828908 | Acc 51.83673469387755\n",
      "Batch 31: loss 0.6901010544069351 | Acc 48.97959183673469\n",
      "Batch 32: loss 0.6902307998389006 | Acc 46.93877551020408\n",
      "Batch 33: loss 0.6902287186998309 | Acc 58.77551020408164\n",
      "Batch 34: loss 0.6902517974376678 | Acc 49.795918367346935\n",
      "Batch 35: loss 0.6902053713798523 | Acc 54.69387755102041\n",
      "Batch 36: loss 0.6902566171354718 | Acc 50.204081632653065\n",
      "Batch 37: loss 0.6902312346406885 | Acc 60.0\n",
      "Batch 38: loss 0.6901928406012686 | Acc 54.69387755102041\n",
      "Batch 39: loss 0.6902099771377368 | Acc 56.734693877551024\n",
      "Batch 40: loss 0.6900493070483208 | Acc 63.26530612244898\n",
      "Batch 41: loss 0.6900831344651013 | Acc 51.83673469387755\n",
      "Batch 42: loss 0.6899638161772773 | Acc 58.36734693877551\n",
      "Batch 43: loss 0.6900802712107814 | Acc 49.38775510204081\n",
      "Batch 44: loss 0.6900331676006317 | Acc 55.10204081632652\n",
      "Batch 45: loss 0.6900770876142713 | Acc 52.244897959183675\n",
      "Batch 46: loss 0.6899524538413339 | Acc 59.591836734693885\n",
      "Batch 47: loss 0.6899544495217343 | Acc 53.46938775510204\n",
      "Batch 48: loss 0.6900014926989874 | Acc 55.10204081632652\n",
      "Batch 49: loss 0.6899723617398009 | Acc 56.734693877551024\n",
      "Batch 50: loss 0.6899354040622712 | Acc 56.734693877551024\n",
      "Batch 51: loss 0.6899321909044304 | Acc 57.14285714285714\n",
      "Batch 52: loss 0.689936890051915 | Acc 55.51020408163265\n",
      "Batch 53: loss 0.6897990085043997 | Acc 62.857142857142854\n",
      "Batch 54: loss 0.6898343938368338 | Acc 52.6530612244898\n",
      "Batch 55: loss 0.689866088737141 | Acc 52.6530612244898\n",
      "Batch 56: loss 0.6897176206111908 | Acc 60.816326530612244\n",
      "Batch 57: loss 0.68953867230499 | Acc 63.6734693877551\n",
      "Batch 58: loss 0.6895624738315056 | Acc 48.16326530612245\n",
      "Batch 59: loss 0.6897034180366387 | Acc 48.16326530612245\n",
      "Batch 60: loss 0.6896352092425029 | Acc 60.40816326530612\n",
      "Batch 61: loss 0.689720516322089 | Acc 49.38775510204081\n",
      "Batch 62: loss 0.6895980748438066 | Acc 60.40816326530612\n",
      "Batch 63: loss 0.6895522475242615 | Acc 58.77551020408164\n",
      "Batch 64: loss 0.6894514886662364 | Acc 60.40816326530612\n",
      "Batch 65: loss 0.689367598753709 | Acc 63.26530612244898\n",
      "Batch 66: loss 0.6893356099273219 | Acc 58.77551020408164\n",
      "Batch 67: loss 0.6893061052507429 | Acc 57.14285714285714\n",
      "Batch 68: loss 0.6893071216695449 | Acc 55.10204081632652\n",
      "Batch 69: loss 0.6893884738286337 | Acc 48.16326530612245\n",
      "Batch 70: loss 0.6892953353268759 | Acc 64.08163265306122\n",
      "Batch 71: loss 0.6893041486471472 | Acc 50.61224489795918\n",
      "Batch 72: loss 0.6892430037260056 | Acc 58.36734693877551\n",
      "Batch 73: loss 0.6891183444898422 | Acc 61.224489795918366\n",
      "Batch 74: loss 0.689193340572151 | Acc 48.57142857142857\n",
      "Batch 75: loss 0.6891711250940958 | Acc 54.69387755102041\n",
      "Batch 76: loss 0.6891072808127654 | Acc 55.91836734693878\n",
      "Batch 77: loss 0.689254076449902 | Acc 45.714285714285715\n",
      "Batch 78: loss 0.6892141623374743 | Acc 55.10204081632652\n",
      "Batch 79: loss 0.6892058109935326 | Acc 54.285714285714285\n",
      "Batch 80: loss 0.6891518041491509 | Acc 56.734693877551024\n",
      "Batch 81: loss 0.6891525991169023 | Acc 52.244897959183675\n",
      "Batch 82: loss 0.6891218169433314 | Acc 57.14285714285714\n",
      "Batch 83: loss 0.6890834771006941 | Acc 59.183673469387756\n",
      "Batch 84: loss 0.689120474315825 | Acc 48.97959183673469\n",
      "Batch 85: loss 0.6892870103611666 | Acc 43.26530612244898\n",
      "Batch 86: loss 0.6891740033792895 | Acc 57.95918367346938\n",
      "Batch 87: loss 0.6891887784004211 | Acc 52.244897959183675\n",
      "Batch 88: loss 0.6891765086488291 | Acc 56.326530612244895\n",
      "Batch 89: loss 0.6891645474380321 | Acc 56.326530612244895\n",
      "Batch 90: loss 0.6892447743150923 | Acc 48.57142857142857\n",
      "Batch 91: loss 0.6891601124962607 | Acc 61.63265306122449\n",
      "Batch 92: loss 0.6892139425744181 | Acc 51.42857142857142\n",
      "Batch 93: loss 0.6891883727042906 | Acc 55.91836734693878\n",
      "Batch 94: loss 0.6891712362461901 | Acc 54.69387755102041\n",
      "Batch 95: loss 0.6891186977687634 | Acc 56.734693877551024\n",
      "Batch 96: loss 0.6891021523624659 | Acc 56.734693877551024\n",
      "Batch 97: loss 0.6890400119663513 | Acc 54.69387755102041\n",
      "Batch 98: loss 0.6891347887564678 | Acc 46.93877551020408\n",
      "Epoch 2: Training loss 0.6891347887564678 | Acc: 54.76051645147856\n",
      "Epoch 2: Validation loss 0.6612316472189766 | Accuracy 78.89212827988337 | AUC 0.5223173467927928\n",
      "Batch 1: loss 0.6987109780311584 | Acc 47.755102040816325\n",
      "Batch 2: loss 0.692996084690094 | Acc 53.87755102040816\n",
      "Batch 3: loss 0.6951832373936971 | Acc 46.53061224489796\n",
      "Batch 4: loss 0.6929234564304352 | Acc 55.10204081632652\n",
      "Batch 5: loss 0.6944148421287537 | Acc 44.08163265306123\n",
      "Batch 6: loss 0.691870371500651 | Acc 59.591836734693885\n",
      "Batch 7: loss 0.6903429968016488 | Acc 59.183673469387756\n",
      "Batch 8: loss 0.6900140941143036 | Acc 54.285714285714285\n",
      "Batch 9: loss 0.6911471221182082 | Acc 46.12244897959184\n",
      "Batch 10: loss 0.6897434651851654 | Acc 62.857142857142854\n",
      "Batch 11: loss 0.6881651986729015 | Acc 65.3061224489796\n",
      "Batch 12: loss 0.6891338179508845 | Acc 45.30612244897959\n",
      "Batch 13: loss 0.6897529180233295 | Acc 48.97959183673469\n",
      "Batch 14: loss 0.6889055626732963 | Acc 62.04081632653061\n",
      "Batch 15: loss 0.689293917020162 | Acc 48.57142857142857\n",
      "Batch 16: loss 0.6882546246051788 | Acc 64.89795918367346\n",
      "Batch 17: loss 0.6882795761613285 | Acc 53.06122448979592\n",
      "Batch 18: loss 0.6879952914184995 | Acc 59.183673469387756\n",
      "Batch 19: loss 0.687561505719235 | Acc 63.26530612244898\n",
      "Batch 20: loss 0.6874663293361664 | Acc 57.14285714285714\n",
      "Batch 21: loss 0.6875091337022328 | Acc 51.42857142857142\n",
      "Batch 22: loss 0.6871053928678686 | Acc 56.734693877551024\n",
      "Batch 23: loss 0.6871541971745698 | Acc 51.83673469387755\n",
      "Batch 24: loss 0.686801644663016 | Acc 61.63265306122449\n",
      "Batch 25: loss 0.6868631982803345 | Acc 52.6530612244898\n",
      "Batch 26: loss 0.6865255053226764 | Acc 57.55102040816327\n",
      "Batch 27: loss 0.6865771920592697 | Acc 55.91836734693878\n",
      "Batch 28: loss 0.6862731788839612 | Acc 59.591836734693885\n",
      "Batch 29: loss 0.6865313998584089 | Acc 54.285714285714285\n",
      "Batch 30: loss 0.6862991392612457 | Acc 62.857142857142854\n",
      "Batch 31: loss 0.6863538526719616 | Acc 55.91836734693878\n",
      "Batch 32: loss 0.6863611098378897 | Acc 55.10204081632652\n",
      "Batch 33: loss 0.6863502318208868 | Acc 56.326530612244895\n",
      "Batch 34: loss 0.6864835988072788 | Acc 51.42857142857142\n",
      "Batch 35: loss 0.6865571771349226 | Acc 53.46938775510204\n",
      "Batch 36: loss 0.6863431334495544 | Acc 60.40816326530612\n",
      "Batch 37: loss 0.6862329566800917 | Acc 61.63265306122449\n",
      "Batch 38: loss 0.6863283248324143 | Acc 55.51020408163265\n",
      "Batch 39: loss 0.6863564069454486 | Acc 55.91836734693878\n",
      "Batch 40: loss 0.6863020494580269 | Acc 60.816326530612244\n",
      "Batch 41: loss 0.6862977452394439 | Acc 60.0\n",
      "Batch 42: loss 0.6864842275778452 | Acc 52.6530612244898\n",
      "Batch 43: loss 0.6866391841755357 | Acc 51.83673469387755\n",
      "Batch 44: loss 0.6865577508102764 | Acc 59.591836734693885\n",
      "Batch 45: loss 0.6866442044576009 | Acc 53.46938775510204\n",
      "Batch 46: loss 0.6868095553439596 | Acc 51.42857142857142\n",
      "Batch 47: loss 0.6868234289453384 | Acc 58.77551020408164\n",
      "Batch 48: loss 0.6869058087468147 | Acc 53.06122448979592\n",
      "Batch 49: loss 0.6869323071168394 | Acc 57.95918367346938\n",
      "Batch 50: loss 0.6870786702632904 | Acc 50.61224489795918\n",
      "Batch 51: loss 0.6870971438931484 | Acc 57.55102040816327\n",
      "Batch 52: loss 0.6870836753111619 | Acc 56.734693877551024\n",
      "Batch 53: loss 0.6871565108029347 | Acc 52.6530612244898\n",
      "Batch 54: loss 0.6871772165651675 | Acc 57.14285714285714\n",
      "Batch 55: loss 0.6872316479682923 | Acc 52.244897959183675\n",
      "Batch 56: loss 0.6872024631925991 | Acc 56.734693877551024\n",
      "Batch 57: loss 0.6872110670072991 | Acc 57.95918367346938\n",
      "Batch 58: loss 0.687167042288287 | Acc 57.14285714285714\n",
      "Batch 59: loss 0.6872797931654978 | Acc 50.61224489795918\n",
      "Batch 60: loss 0.687236883242925 | Acc 58.36734693877551\n",
      "Batch 61: loss 0.6872497259593401 | Acc 56.326530612244895\n",
      "Batch 62: loss 0.6872102156762154 | Acc 59.591836734693885\n",
      "Batch 63: loss 0.6871213723742773 | Acc 61.63265306122449\n",
      "Batch 64: loss 0.6871735099703074 | Acc 52.6530612244898\n",
      "Batch 65: loss 0.6872317616756146 | Acc 55.51020408163265\n",
      "Batch 66: loss 0.6872420997330637 | Acc 53.87755102040816\n",
      "Batch 67: loss 0.6873016330733228 | Acc 53.46938775510204\n",
      "Batch 68: loss 0.6872673972564585 | Acc 55.51020408163265\n",
      "Batch 69: loss 0.6874226409456005 | Acc 51.42857142857142\n",
      "Batch 70: loss 0.6874598860740662 | Acc 55.10204081632652\n",
      "Batch 71: loss 0.6873669867784205 | Acc 60.0\n",
      "Batch 72: loss 0.6874775207704968 | Acc 51.02040816326531\n",
      "Batch 73: loss 0.6873758614879765 | Acc 61.63265306122449\n",
      "Batch 74: loss 0.6874596114094192 | Acc 52.6530612244898\n",
      "Batch 75: loss 0.6873890256881714 | Acc 57.55102040816327\n",
      "Batch 76: loss 0.687363959456745 | Acc 58.36734693877551\n",
      "Batch 77: loss 0.6872944738957789 | Acc 58.77551020408164\n",
      "Batch 78: loss 0.6872872473337711 | Acc 56.326530612244895\n",
      "Batch 79: loss 0.6872640410556069 | Acc 57.95918367346938\n",
      "Batch 80: loss 0.6873113811016083 | Acc 55.10204081632652\n",
      "Batch 81: loss 0.6872323766166781 | Acc 62.44897959183674\n",
      "Batch 82: loss 0.6872115796659051 | Acc 57.55102040816327\n",
      "Batch 83: loss 0.6871903761323676 | Acc 58.36734693877551\n",
      "Batch 84: loss 0.6871051639318466 | Acc 61.63265306122449\n",
      "Batch 85: loss 0.6871227678130655 | Acc 51.83673469387755\n",
      "Batch 86: loss 0.686965158512426 | Acc 63.6734693877551\n",
      "Batch 87: loss 0.6870731170150055 | Acc 50.204081632653065\n",
      "Batch 88: loss 0.6869264922358773 | Acc 64.89795918367346\n",
      "Batch 89: loss 0.6870193622085485 | Acc 48.97959183673469\n",
      "Batch 90: loss 0.6869343101978302 | Acc 58.36734693877551\n",
      "Batch 91: loss 0.6867940157324404 | Acc 63.6734693877551\n",
      "Batch 92: loss 0.6868831787420355 | Acc 51.02040816326531\n",
      "Batch 93: loss 0.6867775365870487 | Acc 60.816326530612244\n",
      "Batch 94: loss 0.6869117421038607 | Acc 47.755102040816325\n",
      "Batch 95: loss 0.6867367976590206 | Acc 65.71428571428571\n",
      "Batch 96: loss 0.6868949619432291 | Acc 48.16326530612245\n",
      "Batch 97: loss 0.6866457812564889 | Acc 68.57142857142857\n",
      "Batch 98: loss 0.686875407793084 | Acc 40.0\n",
      "Epoch 3: Training loss 0.686875407793084 | Acc: 55.94752186588921\n",
      "Epoch 3: Validation loss 0.6498899681227548 | Accuracy 77.865889212828 | AUC 0.5201141500427718\n",
      "Best epoch:  2\n"
     ]
    }
   ],
   "source": [
    "mtrainer.run_train(3, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.01891488302637, 0.4961423906653175, 0.6495074789698531)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model trained 3 epochs with lr=5e-6, 3 epochs with lr=1e-6, 3 epochs with lr=1e-5\n",
    "mtrainer.run_eval(mtrainer.model, mtrainer.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.21353907416626, 0.5099220733360005, 0.6617257754977156)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, mtrainer.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.best_model.cls.state_dict(), 'models/metaclassifier/model/comb3/cls_weights-16.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.best_model.attn_model, 'models/metaclassifier/model/comb3/attention-model-8h4l.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototypical Network with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.attention.model import LabelImageAttention, LabelImagePrototypeModel\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "\n",
    "from utils.prototype import class_prototype_inf\n",
    "from models.metaclassifier.base import euclidean_distance\n",
    "from models.metaclassifier.trainer import ControlledMetaTrainer\n",
    "from models.metaclassifier.model import ProtoNetAttention\n",
    "\n",
    "device =  get_device()\n",
    "\n",
    "encoder = torch.load('models/embedding/model/imgtext_model_trained.pth')\n",
    "encoder.text_model.device = device\n",
    "attention = torch.load('models/attention/model/attention-model-8h4l.pth')\n",
    "# imgtxt_encoder, attn_model, class_prototype_aggregator, distance_func\n",
    "model = ProtoNetAttention(encoder, attention, class_prototype_inf, euclidean_distance)\n",
    "mtrainer = ControlledMetaTrainer(model, NUM_SHOTS, NUM_WAYS, dataset_config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naomileow/Documents/school/CS6240/project/utils/prototype.py:55: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  classes_count = torch.nonzero(label_inds)[:,1].bincount()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.6663146018981934 | Accuracy 81.63265306122449 | AUC 0.5520890348476556\n",
      "Loss 0.6675408482551575 | Accuracy 77.9591836734694 | AUC 0.592805177243311\n",
      "Loss 0.6662786602973938 | Accuracy 80.81632653061224 | AUC 0.5031611300576818\n",
      "Loss 0.6649683117866516 | Accuracy 80.0 | AUC 0.5192899818312419\n",
      "Loss 0.6666518449783325 | Accuracy 80.0 | AUC 0.5661068620956023\n",
      "Loss 0.665653645992279 | Accuracy 80.81632653061224 | AUC 0.5215153647665963\n",
      "Loss 0.6630535125732422 | Accuracy 81.63265306122449 | AUC 0.48086569424007847\n",
      "Loss 0.6627781987190247 | Accuracy 81.63265306122449 | AUC 0.5596055745021262\n",
      "Loss 0.6620592474937439 | Accuracy 82.44897959183673 | AUC 0.45533174308684515\n",
      "Loss 0.6656913161277771 | Accuracy 79.59183673469387 | AUC 0.5300442635824945\n",
      "Loss 0.666347324848175 | Accuracy 81.22448979591836 | AUC 0.5316662740132128\n",
      "Loss 0.6675103306770325 | Accuracy 79.18367346938776 | AUC 0.5914888911229517\n",
      "Loss 0.6687719821929932 | Accuracy 77.9591836734694 | AUC 0.5190180989547633\n",
      "Loss 0.6678135395050049 | Accuracy 81.63265306122449 | AUC 0.5030692436814886\n",
      "Loss 0.6668816804885864 | Accuracy 80.40816326530611 | AUC 0.5499452257997985\n",
      "Loss 0.6622057557106018 | Accuracy 80.40816326530611 | AUC 0.5284938606367178\n",
      "Loss 0.666708767414093 | Accuracy 77.9591836734694 | AUC 0.5530734637166727\n",
      "Loss 0.6685255169868469 | Accuracy 80.40816326530611 | AUC 0.5230501011237175\n",
      "Loss 0.6651032567024231 | Accuracy 79.59183673469387 | AUC 0.5732378830021335\n",
      "Loss 0.6652759909629822 | Accuracy 81.63265306122449 | AUC 0.4988360888466448\n",
      "Loss 0.6661949157714844 | Accuracy 80.0 | AUC 0.4626874657909141\n",
      "Loss 0.6704035401344299 | Accuracy 78.36734693877551 | AUC 0.5474009827845155\n",
      "Loss 0.6634404063224792 | Accuracy 80.0 | AUC 0.5676296117675428\n",
      "Loss 0.6621778607368469 | Accuracy 82.0408163265306 | AUC 0.5448544386698779\n",
      "Loss 0.6679819822311401 | Accuracy 78.77551020408163 | AUC 0.5663474756535981\n",
      "Loss 0.6682234406471252 | Accuracy 79.18367346938776 | AUC 0.5030020295974461\n",
      "Loss 0.6644898056983948 | Accuracy 80.81632653061224 | AUC 0.6171108401843036\n",
      "Loss 0.6677553057670593 | Accuracy 82.44897959183673 | AUC 0.5112857142857143\n",
      "Loss 0.6693733930587769 | Accuracy 79.18367346938776 | AUC 0.5032710700043564\n",
      "Loss 0.6647546887397766 | Accuracy 80.0 | AUC 0.5484820213799806\n"
     ]
    }
   ],
   "source": [
    "mtrainer.run_eval(model, mtrainer.test_loader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.37026239067053, 0.5161583489962621, 0.6703500832830157)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(model, mtrainer.val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 0.645804762840271 | Acc 59.795918367346935\n",
      "Batch 2: loss 0.64562126994133 | Acc 58.877551020408156\n",
      "Batch 3: loss 0.6446925004323324 | Acc 60.51020408163266\n",
      "Batch 4: loss 0.6453973799943924 | Acc 59.285714285714285\n",
      "Batch 5: loss 0.6450785756111145 | Acc 58.673469387755105\n",
      "Batch 6: loss 0.6450744767983755 | Acc 59.38775510204082\n",
      "Batch 7: loss 0.6451364670481 | Acc 58.57142857142858\n",
      "Batch 8: loss 0.6450803205370903 | Acc 60.71428571428571\n",
      "Batch 9: loss 0.6445973383055793 | Acc 61.530612244897966\n",
      "Batch 10: loss 0.6443249225616455 | Acc 60.71428571428571\n",
      "Batch 11: loss 0.6443740454587069 | Acc 59.591836734693885\n",
      "Batch 12: loss 0.6442251602808634 | Acc 60.10204081632653\n",
      "Batch 13: loss 0.6441494272305415 | Acc 60.0\n",
      "Batch 14: loss 0.6442059363637652 | Acc 57.44897959183673\n",
      "Batch 15: loss 0.6439297715822856 | Acc 62.755102040816325\n",
      "Batch 16: loss 0.6437373459339142 | Acc 59.08163265306122\n",
      "Batch 17: loss 0.6442396991393146 | Acc 56.93877551020409\n",
      "Batch 18: loss 0.644419295920266 | Acc 57.6530612244898\n",
      "Batch 19: loss 0.6443859276018644 | Acc 61.224489795918366\n",
      "Batch 20: loss 0.644442817568779 | Acc 60.204081632653065\n",
      "Batch 21: loss 0.644468279111953 | Acc 58.36734693877551\n",
      "Batch 22: loss 0.6442746682600542 | Acc 60.61224489795919\n",
      "Batch 23: loss 0.6440734267234802 | Acc 59.48979591836735\n",
      "Batch 24: loss 0.6440364544590315 | Acc 58.57142857142858\n",
      "Batch 25: loss 0.6440093660354614 | Acc 59.08163265306122\n",
      "Batch 26: loss 0.6440682090245761 | Acc 59.693877551020414\n",
      "Batch 27: loss 0.6444474480770253 | Acc 58.57142857142858\n",
      "Batch 28: loss 0.6445617356470653 | Acc 58.265306122448976\n",
      "Batch 29: loss 0.6447201946686054 | Acc 59.48979591836735\n",
      "Batch 30: loss 0.6448805093765259 | Acc 59.285714285714285\n",
      "Batch 31: loss 0.6448275543028309 | Acc 60.30612244897959\n",
      "Batch 32: loss 0.6447042729705572 | Acc 59.38775510204082\n",
      "Batch 33: loss 0.6446310606869784 | Acc 61.224489795918366\n",
      "Batch 34: loss 0.6444737595670363 | Acc 61.3265306122449\n",
      "Batch 35: loss 0.6446235792977469 | Acc 59.183673469387756\n",
      "Batch 36: loss 0.644515100452635 | Acc 58.97959183673469\n",
      "Batch 37: loss 0.6445720469629442 | Acc 58.97959183673469\n",
      "Batch 38: loss 0.6446787918868818 | Acc 56.63265306122449\n",
      "Batch 39: loss 0.6446952407176678 | Acc 59.183673469387756\n",
      "Batch 40: loss 0.6447015598416328 | Acc 59.693877551020414\n",
      "Batch 41: loss 0.6445464331929277 | Acc 63.469387755102034\n",
      "Batch 42: loss 0.644495780978884 | Acc 59.08163265306122\n",
      "Batch 43: loss 0.6445212960243225 | Acc 58.57142857142858\n",
      "Batch 44: loss 0.6444095495072278 | Acc 62.55102040816326\n",
      "Batch 45: loss 0.6444143811861675 | Acc 57.44897959183673\n",
      "Batch 46: loss 0.6443672724392103 | Acc 60.61224489795919\n",
      "Batch 47: loss 0.644300141233079 | Acc 59.08163265306122\n",
      "Batch 48: loss 0.6441943148771921 | Acc 58.77551020408164\n",
      "Batch 49: loss 0.644330764303402 | Acc 58.16326530612245\n",
      "Epoch 1: Training loss 0.644330764303402 | Acc: 59.53352769679299\n",
      "Epoch 1: Validation loss 0.6641835195677621 | Accuracy 79.12536443148687 | AUC 0.5295648644525054\n",
      "Batch 1: loss 0.6464527249336243 | Acc 59.897959183673464\n",
      "Batch 2: loss 0.6458544433116913 | Acc 58.97959183673469\n",
      "Batch 3: loss 0.6446337898572286 | Acc 60.0\n",
      "Batch 4: loss 0.6461596041917801 | Acc 58.77551020408164\n",
      "Batch 5: loss 0.6460518360137939 | Acc 58.57142857142858\n",
      "Batch 6: loss 0.6460611820220947 | Acc 59.38775510204082\n",
      "Batch 7: loss 0.6462189384869167 | Acc 58.16326530612245\n",
      "Batch 8: loss 0.6465961188077927 | Acc 60.51020408163266\n",
      "Batch 9: loss 0.6457337074809604 | Acc 61.224489795918366\n",
      "Batch 10: loss 0.6456472396850585 | Acc 60.71428571428571\n",
      "Batch 11: loss 0.6451704556291754 | Acc 60.51020408163266\n",
      "Batch 12: loss 0.6448740611473719 | Acc 59.285714285714285\n",
      "Batch 13: loss 0.645025005707374 | Acc 59.795918367346935\n",
      "Batch 14: loss 0.6451449223927089 | Acc 57.44897959183673\n",
      "Batch 15: loss 0.6449779907862345 | Acc 62.34693877551021\n",
      "Batch 16: loss 0.644949484616518 | Acc 59.48979591836735\n",
      "Batch 17: loss 0.6451979805441463 | Acc 56.53061224489796\n",
      "Batch 18: loss 0.6452026930120256 | Acc 57.85714285714286\n",
      "Batch 19: loss 0.645241097400063 | Acc 61.836734693877546\n",
      "Batch 20: loss 0.6450112730264663 | Acc 60.51020408163266\n",
      "Batch 21: loss 0.6450335355032057 | Acc 57.75510204081633\n",
      "Batch 22: loss 0.6448475230823864 | Acc 61.0204081632653\n",
      "Batch 23: loss 0.6448500571043595 | Acc 59.38775510204082\n",
      "Batch 24: loss 0.6447729443510374 | Acc 59.183673469387756\n",
      "Batch 25: loss 0.6447567558288574 | Acc 58.265306122448976\n",
      "Batch 26: loss 0.6445562747808603 | Acc 61.12244897959184\n",
      "Batch 27: loss 0.6447264772874338 | Acc 58.265306122448976\n",
      "Batch 28: loss 0.6448971884591239 | Acc 56.83673469387755\n",
      "Batch 29: loss 0.6452014322938591 | Acc 57.75510204081633\n",
      "Batch 30: loss 0.6452295184135437 | Acc 58.77551020408164\n",
      "Batch 31: loss 0.6452498012973417 | Acc 59.285714285714285\n",
      "Batch 32: loss 0.6454197894781828 | Acc 58.673469387755105\n",
      "Batch 33: loss 0.6453623265931101 | Acc 59.693877551020414\n",
      "Batch 34: loss 0.6451991393285639 | Acc 61.12244897959184\n",
      "Batch 35: loss 0.6451753411974226 | Acc 56.53061224489796\n",
      "Batch 36: loss 0.6451225678126017 | Acc 59.693877551020414\n",
      "Batch 37: loss 0.6452148524490563 | Acc 58.77551020408164\n",
      "Batch 38: loss 0.6452572518273404 | Acc 59.48979591836735\n",
      "Batch 39: loss 0.6452626158029605 | Acc 58.97959183673469\n",
      "Batch 40: loss 0.6452720627188683 | Acc 60.51020408163266\n",
      "Batch 41: loss 0.6451658359388026 | Acc 62.04081632653061\n",
      "Batch 42: loss 0.6454526327905201 | Acc 57.6530612244898\n",
      "Batch 43: loss 0.6454908057700756 | Acc 59.08163265306122\n",
      "Batch 44: loss 0.645400112325495 | Acc 60.71428571428571\n",
      "Batch 45: loss 0.6453922775056627 | Acc 57.6530612244898\n",
      "Batch 46: loss 0.6452509229597838 | Acc 60.91836734693877\n",
      "Batch 47: loss 0.6452864966493972 | Acc 59.183673469387756\n",
      "Batch 48: loss 0.6451647480328878 | Acc 60.40816326530612\n",
      "Batch 49: loss 0.64504068603321 | Acc 61.0204081632653\n",
      "Epoch 2: Training loss 0.64504068603321 | Acc: 59.421074552269886\n",
      "Epoch 2: Validation loss 0.6642832006726946 | Accuracy 79.5218658892128 | AUC 0.5293396045695373\n",
      "Best epoch:  2\n"
     ]
    }
   ],
   "source": [
    "mtrainer.run_train(2, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(1.0008, device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(mtrainer.best_model.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80.38825286212047, 0.5417728137567698, 0.6569410795118751)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, mtrainer.test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiments on baseline models without attention\n",
    "### RelationNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.attention.model import LabelImageAttention, LabelImagePrototypeModel\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "\n",
    "from utils.prototype import class_prototype_inf\n",
    "from models.metaclassifier.trainer import ControlledMetaTrainer\n",
    "from models.metaclassifier.baselines import RelationNet\n",
    "\n",
    "encoder = torch.load('models/embedding/model/imgtext_model_trained.pth')\n",
    "encoder.text_model.device = device\n",
    "base_model = RelationNet(encoder, 512, class_prototype_inf, fc_hidden_size=16)\n",
    "btrainer = ControlledMetaTrainer(base_model, NUM_SHOTS, NUM_WAYS, dataset_config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.01457725947522, 0.5016376709062274, 0.6972464016505651)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btrainer.run_eval(btrainer.model, btrainer.val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.079641612742655, 0.5146355623202179, 0.6978601042817278)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btrainer.run_eval(btrainer.model, btrainer.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 0.679679274559021 | Acc 68.16326530612244\n",
      "Batch 2: loss 0.6872178614139557 | Acc 48.16326530612245\n",
      "Batch 3: loss 0.6854563554128011 | Acc 63.6734693877551\n",
      "Batch 4: loss 0.6872857213020325 | Acc 53.46938775510204\n",
      "Batch 5: loss 0.6890603423118591 | Acc 52.6530612244898\n",
      "Batch 6: loss 0.687427838643392 | Acc 66.12244897959184\n",
      "Batch 7: loss 0.6865879552704948 | Acc 62.04081632653061\n",
      "Batch 8: loss 0.6865909770131111 | Acc 61.224489795918366\n",
      "Batch 9: loss 0.6880093084441291 | Acc 43.673469387755105\n",
      "Batch 10: loss 0.6870020091533661 | Acc 68.57142857142857\n",
      "Batch 11: loss 0.6873770952224731 | Acc 53.87755102040816\n",
      "Batch 12: loss 0.6869907428820928 | Acc 61.63265306122449\n",
      "Batch 13: loss 0.6864107847213745 | Acc 69.38775510204081\n",
      "Batch 14: loss 0.6862963523183551 | Acc 58.77551020408164\n",
      "Batch 15: loss 0.6861640095710755 | Acc 60.0\n",
      "Batch 16: loss 0.6858026422560215 | Acc 64.08163265306122\n",
      "Batch 17: loss 0.68585064831902 | Acc 57.14285714285714\n",
      "Batch 18: loss 0.6858793861336179 | Acc 58.77551020408164\n",
      "Batch 19: loss 0.6854901721602992 | Acc 66.12244897959184\n",
      "Batch 20: loss 0.6862986445426941 | Acc 44.89795918367347\n",
      "Batch 21: loss 0.6859852983838036 | Acc 61.224489795918366\n",
      "Batch 22: loss 0.6863649026914076 | Acc 49.795918367346935\n",
      "Batch 23: loss 0.6859448733537094 | Acc 66.12244897959184\n",
      "Batch 24: loss 0.6861392507950465 | Acc 51.42857142857142\n",
      "Batch 25: loss 0.686033296585083 | Acc 62.04081632653061\n",
      "Batch 26: loss 0.6860054868918198 | Acc 58.36734693877551\n",
      "Batch 27: loss 0.6862060317286739 | Acc 51.02040816326531\n",
      "Batch 28: loss 0.6860193014144897 | Acc 65.3061224489796\n",
      "Batch 29: loss 0.685966446481902 | Acc 60.816326530612244\n",
      "Batch 30: loss 0.6861148258050283 | Acc 53.46938775510204\n",
      "Batch 31: loss 0.6861777171011894 | Acc 57.95918367346938\n",
      "Batch 32: loss 0.6859908066689968 | Acc 63.6734693877551\n",
      "Batch 33: loss 0.6863168929562424 | Acc 50.61224489795918\n",
      "Batch 34: loss 0.686203122138977 | Acc 61.63265306122449\n",
      "Batch 35: loss 0.6862580265317645 | Acc 57.95918367346938\n",
      "Batch 36: loss 0.6861586521069208 | Acc 62.04081632653061\n",
      "Batch 37: loss 0.6860735803037077 | Acc 62.44897959183674\n",
      "Batch 38: loss 0.6862599112485585 | Acc 52.6530612244898\n",
      "Batch 39: loss 0.6862701559678103 | Acc 56.326530612244895\n",
      "Batch 40: loss 0.6861455470323563 | Acc 61.63265306122449\n",
      "Batch 41: loss 0.6859097175481843 | Acc 67.75510204081633\n",
      "Batch 42: loss 0.6861493488152822 | Acc 53.06122448979592\n",
      "Batch 43: loss 0.6861644262491271 | Acc 60.40816326530612\n",
      "Batch 44: loss 0.6861361319368536 | Acc 59.183673469387756\n",
      "Batch 45: loss 0.6859990000724793 | Acc 61.63265306122449\n",
      "Batch 46: loss 0.6862297472746476 | Acc 49.795918367346935\n",
      "Batch 47: loss 0.6861953646578687 | Acc 59.183673469387756\n",
      "Batch 48: loss 0.686012198527654 | Acc 64.48979591836735\n",
      "Batch 49: loss 0.6861263756849327 | Acc 54.69387755102041\n",
      "Batch 50: loss 0.6861001336574555 | Acc 62.04081632653061\n",
      "Batch 51: loss 0.6860652636079227 | Acc 61.224489795918366\n",
      "Batch 52: loss 0.6862311775867755 | Acc 49.38775510204081\n",
      "Batch 53: loss 0.6861241606046569 | Acc 63.26530612244898\n",
      "Batch 54: loss 0.686188409725825 | Acc 54.69387755102041\n",
      "Batch 55: loss 0.6860619924285195 | Acc 67.3469387755102\n",
      "Batch 56: loss 0.6862971314362117 | Acc 46.12244897959184\n",
      "Batch 57: loss 0.6864463132724428 | Acc 52.244897959183675\n",
      "Batch 58: loss 0.6862736775957304 | Acc 71.0204081632653\n",
      "Batch 59: loss 0.6864105180158453 | Acc 48.97959183673469\n",
      "Batch 60: loss 0.6862976471583049 | Acc 64.08163265306122\n",
      "Batch 61: loss 0.6862442933145116 | Acc 65.3061224489796\n",
      "Batch 62: loss 0.6862541619808443 | Acc 59.591836734693885\n",
      "Batch 63: loss 0.686182281327626 | Acc 61.63265306122449\n",
      "Batch 64: loss 0.6862784456461668 | Acc 53.06122448979592\n",
      "Batch 65: loss 0.6863786605688241 | Acc 51.02040816326531\n",
      "Batch 66: loss 0.6862714380928965 | Acc 67.3469387755102\n",
      "Batch 67: loss 0.6862880925634014 | Acc 62.44897959183674\n",
      "Batch 68: loss 0.6862285522853627 | Acc 60.816326530612244\n",
      "Batch 69: loss 0.6861109215280284 | Acc 66.53061224489795\n",
      "Batch 70: loss 0.6861976223332541 | Acc 53.87755102040816\n",
      "Batch 71: loss 0.6861527453006153 | Acc 62.04081632653061\n",
      "Batch 72: loss 0.6862441193726327 | Acc 56.734693877551024\n",
      "Batch 73: loss 0.68622157181779 | Acc 63.26530612244898\n",
      "Batch 74: loss 0.6862205726069373 | Acc 60.0\n",
      "Batch 75: loss 0.686271653175354 | Acc 55.10204081632652\n",
      "Batch 76: loss 0.6862196977201261 | Acc 64.89795918367346\n",
      "Batch 77: loss 0.6862564938408988 | Acc 56.326530612244895\n",
      "Batch 78: loss 0.6862516372631757 | Acc 55.51020408163265\n",
      "Batch 79: loss 0.6861759967441801 | Acc 62.857142857142854\n",
      "Batch 80: loss 0.6861750558018684 | Acc 57.95918367346938\n",
      "Batch 81: loss 0.6861522094703015 | Acc 59.183673469387756\n",
      "Batch 82: loss 0.6861482168116221 | Acc 60.0\n",
      "Batch 83: loss 0.6861032944127737 | Acc 62.44897959183674\n",
      "Batch 84: loss 0.6860755085945129 | Acc 62.857142857142854\n",
      "Batch 85: loss 0.6860248292193694 | Acc 62.04081632653061\n",
      "Batch 86: loss 0.6861230093379354 | Acc 52.6530612244898\n",
      "Batch 87: loss 0.6861885011881247 | Acc 56.734693877551024\n",
      "Batch 88: loss 0.6861094988205216 | Acc 61.224489795918366\n",
      "Batch 89: loss 0.68607506390368 | Acc 62.04081632653061\n",
      "Batch 90: loss 0.6861249347527821 | Acc 53.87755102040816\n",
      "Batch 91: loss 0.6861159172686901 | Acc 60.816326530612244\n",
      "Batch 92: loss 0.6860519757737285 | Acc 65.3061224489796\n",
      "Batch 93: loss 0.6859866220463988 | Acc 65.3061224489796\n",
      "Batch 94: loss 0.6861258650079687 | Acc 43.673469387755105\n",
      "Batch 95: loss 0.6860519440550553 | Acc 65.71428571428571\n",
      "Batch 96: loss 0.6861012056469917 | Acc 55.10204081632652\n",
      "Batch 97: loss 0.6860048598849896 | Acc 67.75510204081633\n",
      "Batch 98: loss 0.6860991783288061 | Acc 49.38775510204081\n",
      "Epoch 1: Training loss 0.6860991783288061 | Acc: 59.00041649312786\n",
      "Epoch 1: Validation loss 0.6726657918521336 | Accuracy 72.32653061224488 | AUC 0.5241775528725205\n",
      "Batch 1: loss 0.6899733543395996 | Acc 54.285714285714285\n",
      "Batch 2: loss 0.6848945617675781 | Acc 67.3469387755102\n",
      "Batch 3: loss 0.6872022151947021 | Acc 53.87755102040816\n",
      "Batch 4: loss 0.6855256259441376 | Acc 63.26530612244898\n",
      "Batch 5: loss 0.6869061946868896 | Acc 52.6530612244898\n",
      "Batch 6: loss 0.6860408782958984 | Acc 64.08163265306122\n",
      "Batch 7: loss 0.6859152657645089 | Acc 60.40816326530612\n",
      "Batch 8: loss 0.6862817108631134 | Acc 57.55102040816327\n",
      "Batch 9: loss 0.6853578156895108 | Acc 64.89795918367346\n",
      "Batch 10: loss 0.6863217055797577 | Acc 49.795918367346935\n",
      "Batch 11: loss 0.6860607970844615 | Acc 57.95918367346938\n",
      "Batch 12: loss 0.6865395605564117 | Acc 52.244897959183675\n",
      "Batch 13: loss 0.686865169268388 | Acc 52.244897959183675\n",
      "Batch 14: loss 0.6866682640143803 | Acc 61.63265306122449\n",
      "Batch 15: loss 0.6859461307525635 | Acc 68.57142857142857\n",
      "Batch 16: loss 0.6864418983459473 | Acc 51.02040816326531\n",
      "Batch 17: loss 0.6866597638410681 | Acc 55.51020408163265\n",
      "Batch 18: loss 0.6862850354777442 | Acc 61.63265306122449\n",
      "Batch 19: loss 0.6866826603287145 | Acc 50.61224489795918\n",
      "Batch 20: loss 0.6863340169191361 | Acc 66.53061224489795\n",
      "Batch 21: loss 0.6861436452184405 | Acc 62.04081632653061\n",
      "Batch 22: loss 0.6859120672399347 | Acc 62.44897959183674\n",
      "Batch 23: loss 0.6858772013498389 | Acc 60.816326530612244\n",
      "Batch 24: loss 0.685884435971578 | Acc 57.95918367346938\n",
      "Batch 25: loss 0.6857252860069275 | Acc 62.04081632653061\n",
      "Batch 26: loss 0.6856267704413488 | Acc 59.591836734693885\n",
      "Batch 27: loss 0.6855097457214638 | Acc 61.224489795918366\n",
      "Batch 28: loss 0.6857226107801709 | Acc 49.38775510204081\n",
      "Batch 29: loss 0.6858627940046376 | Acc 56.734693877551024\n",
      "Batch 30: loss 0.6856659690539042 | Acc 67.3469387755102\n",
      "Batch 31: loss 0.6859661802168815 | Acc 51.83673469387755\n",
      "Batch 32: loss 0.6856746152043343 | Acc 63.26530612244898\n",
      "Batch 33: loss 0.6855620723782163 | Acc 63.6734693877551\n",
      "Batch 34: loss 0.6858321936691508 | Acc 49.795918367346935\n",
      "Batch 35: loss 0.6857544541358948 | Acc 64.08163265306122\n",
      "Batch 36: loss 0.6858676986561881 | Acc 56.326530612244895\n",
      "Batch 37: loss 0.6857975028656624 | Acc 61.224489795918366\n",
      "Batch 38: loss 0.6859246241418939 | Acc 53.46938775510204\n",
      "Batch 39: loss 0.6860489952258575 | Acc 53.87755102040816\n",
      "Batch 40: loss 0.685893501341343 | Acc 66.53061224489795\n",
      "Batch 41: loss 0.6861743389106378 | Acc 50.61224489795918\n",
      "Batch 42: loss 0.6860205474353972 | Acc 65.3061224489796\n",
      "Batch 43: loss 0.6860861584197643 | Acc 55.91836734693878\n",
      "Batch 44: loss 0.6860133897174489 | Acc 62.857142857142854\n",
      "Batch 45: loss 0.6858252022001479 | Acc 64.89795918367346\n",
      "Batch 46: loss 0.6859711343827455 | Acc 50.61224489795918\n",
      "Batch 47: loss 0.6859812153146622 | Acc 56.734693877551024\n",
      "Batch 48: loss 0.6858579479157925 | Acc 62.04081632653061\n",
      "Batch 49: loss 0.6859530271316061 | Acc 55.91836734693878\n",
      "Batch 50: loss 0.6858721387386322 | Acc 63.6734693877551\n",
      "Batch 51: loss 0.6858368457532397 | Acc 64.48979591836735\n",
      "Batch 52: loss 0.6859973932688053 | Acc 53.06122448979592\n",
      "Batch 53: loss 0.6858971591265697 | Acc 62.04081632653061\n",
      "Batch 54: loss 0.6858683062924279 | Acc 61.224489795918366\n",
      "Batch 55: loss 0.6857433427463878 | Acc 67.3469387755102\n",
      "Batch 56: loss 0.6858252968106952 | Acc 56.734693877551024\n",
      "Batch 57: loss 0.6859765429245798 | Acc 51.42857142857142\n",
      "Batch 58: loss 0.68581030183825 | Acc 67.3469387755102\n",
      "Batch 59: loss 0.6857508693711233 | Acc 64.08163265306122\n",
      "Batch 60: loss 0.6859213650226593 | Acc 48.97959183673469\n",
      "Batch 61: loss 0.6857789432416197 | Acc 67.75510204081633\n",
      "Batch 62: loss 0.6859360981372095 | Acc 53.46938775510204\n",
      "Batch 63: loss 0.6862099804575481 | Acc 42.857142857142854\n",
      "Batch 64: loss 0.6861227620393038 | Acc 62.44897959183674\n",
      "Batch 65: loss 0.6860559206742507 | Acc 63.26530612244898\n",
      "Batch 66: loss 0.6861024074482195 | Acc 55.91836734693878\n",
      "Batch 67: loss 0.6861351826297704 | Acc 57.14285714285714\n",
      "Batch 68: loss 0.6860616540207582 | Acc 68.16326530612244\n",
      "Batch 69: loss 0.6860747415086498 | Acc 58.36734693877551\n",
      "Batch 70: loss 0.6860257744789123 | Acc 60.816326530612244\n",
      "Batch 71: loss 0.6860919880195403 | Acc 54.69387755102041\n",
      "Batch 72: loss 0.6860605171985097 | Acc 61.224489795918366\n",
      "Batch 73: loss 0.6862136101069516 | Acc 48.57142857142857\n",
      "Batch 74: loss 0.6861263231651203 | Acc 65.3061224489796\n",
      "Batch 75: loss 0.6863530548413594 | Acc 42.857142857142854\n",
      "Batch 76: loss 0.6862286290055827 | Acc 68.57142857142857\n",
      "Batch 77: loss 0.6861827381245502 | Acc 61.224489795918366\n",
      "Batch 78: loss 0.686274915933609 | Acc 52.244897959183675\n",
      "Batch 79: loss 0.6864111906365503 | Acc 46.93877551020408\n",
      "Batch 80: loss 0.686340344697237 | Acc 63.26530612244898\n",
      "Batch 81: loss 0.6862710979249742 | Acc 63.26530612244898\n",
      "Batch 82: loss 0.6863599157914883 | Acc 50.61224489795918\n",
      "Batch 83: loss 0.686479854296489 | Acc 50.61224489795918\n",
      "Batch 84: loss 0.6863823980093002 | Acc 66.53061224489795\n",
      "Batch 85: loss 0.6864506812656627 | Acc 53.06122448979592\n",
      "Batch 86: loss 0.6864142806030983 | Acc 61.224489795918366\n",
      "Batch 87: loss 0.6863265106047707 | Acc 60.816326530612244\n",
      "Batch 88: loss 0.6863647658716548 | Acc 54.285714285714285\n",
      "Batch 89: loss 0.6863426720158438 | Acc 57.55102040816327\n",
      "Batch 90: loss 0.6863557325469123 | Acc 55.51020408163265\n",
      "Batch 91: loss 0.6863161685702565 | Acc 60.816326530612244\n",
      "Batch 92: loss 0.6863045634134955 | Acc 60.0\n",
      "Batch 93: loss 0.6863105207361201 | Acc 55.51020408163265\n",
      "Batch 94: loss 0.6862852719235928 | Acc 63.6734693877551\n",
      "Batch 95: loss 0.6862719134280556 | Acc 61.63265306122449\n",
      "Batch 96: loss 0.6862260227402052 | Acc 62.857142857142854\n",
      "Batch 97: loss 0.686230236722022 | Acc 57.55102040816327\n",
      "Batch 98: loss 0.6862373169587583 | Acc 59.183673469387756\n",
      "Epoch 2: Training loss 0.6862373169587583 | Acc: 58.621407746772135\n",
      "Epoch 2: Validation loss 0.6721009288515364 | Accuracy 72.36151603498544 | AUC 0.5239913606961554\n",
      "Best epoch:  2\n"
     ]
    }
   ],
   "source": [
    "btrainer.run_train(2, lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72.12827988338194, 0.5262049313442498, 0.6789789761815752)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btrainer.run_eval(btrainer.best_model, btrainer.val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70.60228969636637, 0.5078939691060346, 0.6737378544923736)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btrainer.run_eval(btrainer.best_model, btrainer.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(btrainer.best_model.cls.state_dict(), 'relnet_weights-16.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototypical Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.attention.model import LabelImageAttention, LabelImagePrototypeModel\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "\n",
    "from utils.prototype import class_prototype_inf\n",
    "from models.metaclassifier.base import euclidean_distance\n",
    "from models.metaclassifier.trainer import ControlledMetaTrainer\n",
    "from models.metaclassifier.baselines import ProtoNet\n",
    "\n",
    "encoder = torch.load('models/embedding/model/imgtext_model_trained.pth')\n",
    "encoder.text_model.device = device\n",
    "base_model = ProtoNet(encoder, class_prototype_inf, euclidean_distance, trainable_base=False)\n",
    "btrainer = ControlledMetaTrainer(base_model, NUM_SHOTS, NUM_WAYS, dataset_config, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btrainer.run_eval(btrainer.model, btrainer.val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btrainer.run_eval(btrainer.best_model, btrainer.val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
