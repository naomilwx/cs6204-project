{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from utils.prototype import class_variance\n",
    "\n",
    "class ClsModel(nn.Module):\n",
    "    def __init__(self, imgtxt_encoder, attn_model, embed_dim, class_prototype_aggregator, fc_hidden_size=16, use_variance=False, activation=nn.ReLU, dropout=0.3) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = imgtxt_encoder\n",
    "        self.attn_model = attn_model\n",
    "        self.class_prototype_aggregator = class_prototype_aggregator\n",
    "        self.use_variance = use_variance\n",
    "        if use_variance:\n",
    "            self.cls = nn.Sequential(\n",
    "                nn.Linear(embed_dim*3, fc_hidden_size),\n",
    "                activation(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(fc_hidden_size, 1)\n",
    "            )\n",
    "        else:\n",
    "            self.cls = nn.Sequential(\n",
    "                nn.Linear(embed_dim*2, fc_hidden_size),\n",
    "                activation(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(fc_hidden_size, 1)\n",
    "            )\n",
    "        self.cls_loss_criterion = nn.BCEWithLogitsLoss()\n",
    "        self.freeze_child_models()\n",
    "\n",
    "    def freeze_child_models(self):\n",
    "        self.encoder.set_trainable(False, False, False)\n",
    "        self.attn_model.set_trainable(False)\n",
    "\n",
    "    def set_class_prototype_details(self, class_labels, support_images, support_label_inds):\n",
    "        text_embeddings, image_embeddings = self.encoder(class_labels, support_images, pool=False)\n",
    "        self.class_label_embeddings = text_embeddings\n",
    "        support_prototypes = self.attn_model(text_embeddings, image_embeddings, support_label_inds)\n",
    "        self.class_prototypes = self.class_prototype_aggregator(support_prototypes, support_label_inds)\n",
    "        if self.use_variance:\n",
    "            self.class_prototypes_var = class_variance(support_prototypes, support_label_inds)\n",
    "    \n",
    "    def update_support_and_classify(self, class_labels, support_images, support_label_inds, query_images):\n",
    "        self.set_class_prototype_details(class_labels, support_images, support_label_inds)\n",
    "        return self.forward(query_images)\n",
    "\n",
    "    def forward(self, query_images):\n",
    "        query_image_embeddings = self.encoder.embed_image(query_images, pool=False)\n",
    "        query_prototypes = self.attn_model(self.class_label_embeddings, query_image_embeddings)\n",
    "\n",
    "        # Prototypes: LxD (to repeat N times), variance: LxD (to repeat N times), query class prototype: NxLxD\n",
    "        class_prototypes = self.class_prototypes.repeat(query_prototypes.shape[0], 1, 1)\n",
    "        if self.use_variance:\n",
    "            class_prototypes_var = self.class_prototypes_var.repeat(query_prototypes.shape[0], 1, 1)\n",
    "\n",
    "        if self.use_variance:\n",
    "            out = self.cls(torch.cat((class_prototypes, class_prototypes_var, query_prototypes), dim=2))\n",
    "        else:\n",
    "            out = self.cls(torch.cat((class_prototypes, query_prototypes), dim=2))\n",
    "        return out.squeeze(2) # NxLx1 -> NxL\n",
    "    \n",
    "    def loss(self, predictions, label_inds):\n",
    "        return self.cls_loss_criterion(predictions, label_inds.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def euclidean_distance(prototype, query):\n",
    "    # prototype: (L, D) | query: (N, L, D)\n",
    "    prototype = prototype.unsqueeze(0).expand(query.shape[0], -1, -1)\n",
    "    return ((prototype-query)**2).sum(2)\n",
    "    \n",
    "def cosine_distance(prototype, query):\n",
    "    prototype = prototype / prototype.norm(dim=-1, keepdim=True)\n",
    "    prototype = prototype.unsqueeze(0).expand(query.shape[0], -1, -1)\n",
    "    query = query / query.norm(dim=-1, keepdim=True)\n",
    "    cos = prototype * query\n",
    "    return -cos\n",
    "    \n",
    "\n",
    "class ProtoNet(nn.Module):\n",
    "    def __init__(self, img_encoder, class_prototype_aggregator, distance_func):\n",
    "        super().__init__()\n",
    "        self.encoder = img_encoder\n",
    "        self.class_prototype_aggregator = class_prototype_aggregator\n",
    "        self.distance_func = distance_func\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from utils.metrics import AverageMeter, calculate_auc, multilabel_accuracy\n",
    "\n",
    "class DataloaderIterator:\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self.iterator = iter(self.dataloader)\n",
    "\n",
    "    def next_batch(self):\n",
    "        batch = next(self.iterator, None)\n",
    "        if batch is None:\n",
    "            self.iterator = iter(self.dataloader)\n",
    "            batch = next(self.iterator)\n",
    "        return batch\n",
    "\n",
    "class MetaTrainer:\n",
    "    def __init__(self, model, train_class_labels, val_class_labels, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.train_class_labels = train_class_labels\n",
    "        self.val_class_labels = val_class_labels\n",
    "\n",
    "    def run_train(self, epochs, query_dataloader, support_dataloader, val_dataloader, lr=1e-5):\n",
    "        model = self.model.to(self.device)\n",
    "        best_epoch = None\n",
    "        best_acc = None\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        support_iterator = DataloaderIterator(support_dataloader)\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            loss_meter = AverageMeter()\n",
    "            acc_meter = AverageMeter()\n",
    "            for i, (qimages, qclass_inds) in enumerate(query_dataloader):\n",
    "                qimages, qclass_inds = qimages.to(self.device), qclass_inds.to(self.device)\n",
    "                simages, sclass_inds = support_iterator.next_batch()\n",
    "                simages, sclass_inds = simages.to(self.device), sclass_inds.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                predictions = model.update_support_and_classify(self.train_class_labels, simages, sclass_inds, qimages)\n",
    "                loss = model.loss(predictions, qclass_inds)\n",
    "\n",
    "                acc = multilabel_accuracy(predictions, qclass_inds)\n",
    "                acc_meter.update(acc, qclass_inds.shape[0])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_meter.update(loss.item(), len(qimages))\n",
    "                print(f\"Batch {i+1}: loss {loss_meter.average()} | Acc {acc}\")\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Training loss {loss_meter.average()} | Acc: {acc_meter.average()}\")\n",
    "\n",
    "            val_acc, val_auc, val_loss = self.run_eval(model, val_dataloader)\n",
    "            print(f\"Epoch {epoch+1}: Validation loss {val_loss} | Accuracy {val_acc} | AUC {val_auc}\")\n",
    "\n",
    "            if best_acc is None or val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                self.best_model = copy.deepcopy(model)\n",
    "                best_epoch = epoch\n",
    "        self.model = model\n",
    "        print('Best epoch: ', best_epoch+1)\n",
    "    \n",
    "    def run_eval(self, model, val_dataloader):\n",
    "        model.eval()\n",
    "        model = model.to(self.device)\n",
    "        \n",
    "        loss_meter = AverageMeter()\n",
    "        auc_meter = AverageMeter()\n",
    "        acc_meter = AverageMeter()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, class_inds in val_dataloader:\n",
    "                images, class_inds = images.to(self.device), class_inds.to(self.device)\n",
    "                shots = images.shape[0]//2\n",
    "                qimages, simages = images[:shots,:,:], images[shots:,:,:]\n",
    "                qclass_inds, sclass_inds = class_inds[:shots,:], class_inds[shots:,:]\n",
    "\n",
    "                predictions = model.update_support_and_classify(self.val_class_labels, simages, sclass_inds, qimages)\n",
    "                loss = model.loss(predictions, qclass_inds)\n",
    "\n",
    "                loss_meter.update(loss.item(), shots)\n",
    "\n",
    "                auc = calculate_auc(predictions, qclass_inds)\n",
    "                auc_meter.update(auc, shots)\n",
    "            \n",
    "                acc = multilabel_accuracy(predictions, qclass_inds)\n",
    "                acc_meter.update(acc, shots)\n",
    "        return acc_meter.average(), auc_meter.average(), loss_meter.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from utils.device import get_device\n",
    "from utils.data import get_query_and_support_ids\n",
    "from utils.sampling import FewShotBatchSampler\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "from models.embedding.dataset import Dataset\n",
    "\n",
    "img_info = pd.read_pickle('data/vindr_cxr_split_labels.pkl')\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(img_info, 'data/vindr_train_query_set.pkl')\n",
    "\n",
    "IMG_PATH = 'datasets/vindr-cxr-png'\n",
    "\n",
    "def meta_training_loader(dataset, shots, n_ways=None, include_query=False):\n",
    "    return DataLoader(dataset, batch_sampler=FewShotBatchSampler(dataset.get_class_indicators(), shots, n_ways=n_ways, include_query=include_query))\n",
    "\n",
    "def meta_training_dataset(img_info, split):\n",
    "    img_ids = img_info[img_info['meta_split'] == split]['image_id'].to_list()\n",
    "    return Dataset(IMG_PATH, img_info, img_ids, VINDR_CXR_LABELS, VINDR_SPLIT[split], mean_std=MEAN_STDS['chestmnist'])\n",
    "\n",
    "num_shots = 5\n",
    "train_query_dataset = Dataset(IMG_PATH, img_info, query_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "train_query_loader = meta_training_loader(train_query_dataset, num_shots)\n",
    "train_support_dataset = Dataset(IMG_PATH, img_info, support_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "train_support_loader = meta_training_loader(train_support_dataset, num_shots)\n",
    "\n",
    "val_dataset = meta_training_dataset(img_info, 'val')\n",
    "val_loader = meta_training_loader(val_dataset, num_shots, include_query=True)\n",
    "\n",
    "test_dataset = meta_training_dataset(img_info, 'test')\n",
    "test_loader = meta_training_loader(test_dataset, num_shots, include_query=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.attention.model import LabelImageAttention, LabelImagePrototypeModel\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "from utils.prototype import class_prototype_inf\n",
    "\n",
    "device =  get_device()\n",
    "\n",
    "encoder = torch.load('imgtext_model_trained.pth')\n",
    "encoder.text_model.device = device\n",
    "attention = torch.load('attention-model-8h4l.pth')\n",
    "model = ClsModel(encoder, attention, 512, class_prototype_inf)\n",
    "mtrainer = MetaTrainer(model, train_query_dataset.class_labels(), val_dataset.class_labels(), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.96017919362869, 0.5280027641417706, 0.6473812199220425)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naomileow/Documents/school/CS6240/project/utils/prototype.py:55: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  classes_count = torch.nonzero(label_inds)[:,1].bincount()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80.15928322548532, 0.5015153589662482, 0.6472647931517624)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(model, test_loader)\n",
    "# (80.06968641114983, 0.5413753165712019, 0.6457181776442179)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.39358600583091, 0.48764045693712416, 0.6482774785586766)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.98542274052481, 0.48061929521196595, 0.6488205228533064)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(model, val_loader)\n",
    "# (79.19533527696791, 0.49627472243658033, 0.646583594594683)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 0.6797685623168945 | Acc 59.693877551020414\n",
      "Batch 2: loss 0.6813422739505768 | Acc 58.06122448979592\n",
      "Batch 3: loss 0.6820537249247233 | Acc 57.244897959183675\n",
      "Batch 4: loss 0.6819744408130646 | Acc 58.16326530612245\n",
      "Batch 5: loss 0.6813647031784058 | Acc 60.204081632653065\n",
      "Batch 6: loss 0.6800132294495901 | Acc 63.26530612244898\n",
      "Batch 7: loss 0.6796564289501735 | Acc 60.71428571428571\n",
      "Batch 8: loss 0.6793731153011322 | Acc 60.816326530612244\n",
      "Batch 9: loss 0.6795809202724032 | Acc 58.57142857142858\n",
      "Batch 10: loss 0.6796349585056305 | Acc 59.183673469387756\n",
      "Batch 11: loss 0.6792561411857605 | Acc 61.73469387755102\n",
      "Batch 12: loss 0.6788489570220312 | Acc 62.142857142857146\n",
      "Batch 13: loss 0.6786480362598712 | Acc 60.816326530612244\n",
      "Batch 14: loss 0.6787015966006688 | Acc 59.38775510204082\n",
      "Batch 15: loss 0.6788699269294739 | Acc 58.36734693877551\n",
      "Batch 16: loss 0.6788304671645164 | Acc 59.897959183673464\n",
      "Batch 17: loss 0.6790687967749203 | Acc 56.83673469387755\n",
      "Batch 18: loss 0.6790375113487244 | Acc 59.48979591836735\n",
      "Batch 19: loss 0.6788494304606789 | Acc 61.42857142857143\n",
      "Batch 20: loss 0.6788965433835983 | Acc 58.97959183673469\n",
      "Batch 21: loss 0.6788200877961659 | Acc 60.204081632653065\n",
      "Batch 22: loss 0.6789194562218406 | Acc 58.16326530612245\n",
      "Batch 23: loss 0.6787953610005586 | Acc 61.224489795918366\n",
      "Batch 24: loss 0.678814431031545 | Acc 59.183673469387756\n",
      "Batch 25: loss 0.6788975429534913 | Acc 57.6530612244898\n",
      "Batch 26: loss 0.678994004543011 | Acc 57.3469387755102\n",
      "Batch 27: loss 0.6791622550399216 | Acc 56.224489795918366\n",
      "Batch 28: loss 0.6791915787117822 | Acc 57.75510204081633\n",
      "Batch 29: loss 0.6791503676052751 | Acc 59.48979591836735\n",
      "Batch 30: loss 0.6789953291416169 | Acc 61.3265306122449\n",
      "Batch 31: loss 0.6790061881465297 | Acc 58.16326530612245\n",
      "Batch 32: loss 0.6788170374929905 | Acc 61.93877551020408\n",
      "Batch 33: loss 0.6787566903865698 | Acc 59.285714285714285\n",
      "Batch 34: loss 0.6786790893358343 | Acc 60.10204081632653\n",
      "Batch 35: loss 0.6786687340055193 | Acc 58.97959183673469\n",
      "Batch 36: loss 0.6786540928814147 | Acc 58.57142857142858\n",
      "Batch 37: loss 0.6787275075912476 | Acc 57.04081632653061\n",
      "Batch 38: loss 0.6785490967725453 | Acc 62.04081632653061\n",
      "Batch 39: loss 0.6785619549262218 | Acc 58.265306122448976\n",
      "Batch 40: loss 0.6784061908721923 | Acc 61.93877551020408\n",
      "Batch 41: loss 0.6783907224492329 | Acc 58.97959183673469\n",
      "Batch 42: loss 0.6784256810233706 | Acc 57.75510204081633\n",
      "Batch 43: loss 0.6783687680266625 | Acc 59.693877551020414\n",
      "Batch 44: loss 0.6783419441093098 | Acc 58.877551020408156\n",
      "Batch 45: loss 0.6781936433580187 | Acc 62.244897959183675\n",
      "Batch 46: loss 0.6781209383321845 | Acc 60.40816326530612\n",
      "Batch 47: loss 0.6780813184190304 | Acc 58.877551020408156\n",
      "Batch 48: loss 0.6779687342544397 | Acc 60.816326530612244\n",
      "Batch 49: loss 0.6778879275127333 | Acc 60.30612244897959\n",
      "Epoch 1: Training loss 0.6778879275127333 | Acc: 59.54810495626821\n",
      "Epoch 1: Validation loss 0.6486769591059004 | Accuracy 79.3119533527697 | AUC 0.49852179654687273\n",
      "Batch 1: loss 0.6730209589004517 | Acc 60.40816326530612\n",
      "Batch 2: loss 0.6757865250110626 | Acc 58.36734693877551\n",
      "Batch 3: loss 0.6767855087916056 | Acc 57.75510204081633\n",
      "Batch 4: loss 0.6768154352903366 | Acc 57.75510204081633\n",
      "Batch 5: loss 0.676145589351654 | Acc 60.10204081632653\n",
      "Batch 6: loss 0.6747827033201853 | Acc 63.06122448979592\n",
      "Batch 7: loss 0.6745705945151192 | Acc 60.10204081632653\n",
      "Batch 8: loss 0.6744122207164764 | Acc 60.816326530612244\n",
      "Batch 9: loss 0.6747006906403435 | Acc 58.77551020408164\n",
      "Batch 10: loss 0.6748744130134583 | Acc 58.673469387755105\n",
      "Batch 11: loss 0.6743556911295111 | Acc 61.93877551020408\n",
      "Batch 12: loss 0.6738801151514053 | Acc 62.244897959183675\n",
      "Batch 13: loss 0.6738620217029865 | Acc 59.897959183673464\n",
      "Batch 14: loss 0.6738833699907575 | Acc 59.897959183673464\n",
      "Batch 15: loss 0.6741170485814413 | Acc 58.16326530612245\n",
      "Batch 16: loss 0.6740950904786587 | Acc 59.897959183673464\n",
      "Batch 17: loss 0.6743742578169879 | Acc 56.83673469387755\n",
      "Batch 18: loss 0.6743867662217882 | Acc 59.183673469387756\n",
      "Batch 19: loss 0.67418814019153 | Acc 61.42857142857143\n",
      "Batch 20: loss 0.6743351459503174 | Acc 57.85714285714286\n",
      "Batch 21: loss 0.6741712661016555 | Acc 60.204081632653065\n",
      "Batch 22: loss 0.6742914671247656 | Acc 57.95918367346938\n",
      "Batch 23: loss 0.6741519103879514 | Acc 60.51020408163266\n",
      "Batch 24: loss 0.6740899160504341 | Acc 60.51020408163266\n",
      "Batch 25: loss 0.6741497087478637 | Acc 57.95918367346938\n",
      "Batch 26: loss 0.6742664048304925 | Acc 57.75510204081633\n",
      "Batch 27: loss 0.6743905345598856 | Acc 56.734693877551024\n",
      "Batch 28: loss 0.6744009469236646 | Acc 58.265306122448976\n",
      "Batch 29: loss 0.6742890559393784 | Acc 60.0\n",
      "Batch 30: loss 0.6743148366610209 | Acc 58.877551020408156\n",
      "Batch 31: loss 0.6742600029514682 | Acc 59.285714285714285\n",
      "Batch 32: loss 0.6741889454424381 | Acc 59.693877551020414\n",
      "Batch 33: loss 0.6742009899832986 | Acc 59.285714285714285\n",
      "Batch 34: loss 0.6742281545610989 | Acc 58.46938775510204\n",
      "Batch 35: loss 0.6742928147315979 | Acc 57.85714285714286\n",
      "Batch 36: loss 0.674209690756268 | Acc 59.693877551020414\n",
      "Batch 37: loss 0.6741545780284984 | Acc 58.877551020408156\n",
      "Batch 38: loss 0.6740839763691551 | Acc 59.591836734693885\n",
      "Batch 39: loss 0.6740367412567139 | Acc 59.591836734693885\n",
      "Batch 40: loss 0.673970565199852 | Acc 60.40816326530612\n",
      "Batch 41: loss 0.6740014102400803 | Acc 58.265306122448976\n",
      "Batch 42: loss 0.6740206366493589 | Acc 57.75510204081633\n",
      "Batch 43: loss 0.6739560698353967 | Acc 61.0204081632653\n",
      "Batch 44: loss 0.6738734529777006 | Acc 60.30612244897959\n",
      "Batch 45: loss 0.6738927960395813 | Acc 58.57142857142858\n",
      "Batch 46: loss 0.6738399759582852 | Acc 59.795918367346935\n",
      "Batch 47: loss 0.6738382514486921 | Acc 59.08163265306122\n",
      "Batch 48: loss 0.6737772474686304 | Acc 60.71428571428571\n",
      "Batch 49: loss 0.6738781259984387 | Acc 55.306122448979586\n",
      "Epoch 2: Training loss 0.6738781259984387 | Acc: 59.29612661391087\n",
      "Epoch 2: Validation loss 0.6495531746319362 | Accuracy 78.99708454810495 | AUC 0.5249919121515279\n",
      "Batch 1: loss 0.6681010723114014 | Acc 60.30612244897959\n",
      "Batch 2: loss 0.6714080274105072 | Acc 57.75510204081633\n",
      "Batch 3: loss 0.6725442210833231 | Acc 57.3469387755102\n",
      "Batch 4: loss 0.6725458800792694 | Acc 57.75510204081633\n",
      "Batch 5: loss 0.6721907734870911 | Acc 59.897959183673464\n",
      "Batch 6: loss 0.6711460053920746 | Acc 62.142857142857146\n",
      "Batch 7: loss 0.6706865174429757 | Acc 60.51020408163266\n",
      "Batch 8: loss 0.6704347506165504 | Acc 60.816326530612244\n",
      "Batch 9: loss 0.6706915100415548 | Acc 58.97959183673469\n",
      "Batch 10: loss 0.6710814237594604 | Acc 58.877551020408156\n",
      "Batch 11: loss 0.6707289327274669 | Acc 61.3265306122449\n",
      "Batch 12: loss 0.6703536957502365 | Acc 61.224489795918366\n",
      "Batch 13: loss 0.67014479637146 | Acc 60.204081632653065\n",
      "Batch 14: loss 0.6700166761875153 | Acc 59.591836734693885\n",
      "Batch 15: loss 0.6702910184860229 | Acc 57.75510204081633\n",
      "Batch 16: loss 0.6704448573291302 | Acc 59.38775510204082\n",
      "Batch 17: loss 0.6706617649863748 | Acc 56.83673469387755\n",
      "Batch 18: loss 0.6706764267550575 | Acc 59.183673469387756\n",
      "Batch 19: loss 0.6705433224376879 | Acc 61.0204081632653\n",
      "Batch 20: loss 0.6705321609973908 | Acc 58.77551020408164\n",
      "Batch 21: loss 0.6703449998583112 | Acc 60.0\n",
      "Batch 22: loss 0.6704165854237296 | Acc 58.97959183673469\n",
      "Batch 23: loss 0.6702379921208257 | Acc 61.0204081632653\n",
      "Batch 24: loss 0.670300746957461 | Acc 59.38775510204082\n",
      "Batch 25: loss 0.6704106330871582 | Acc 57.244897959183675\n",
      "Batch 26: loss 0.67058432331452 | Acc 57.3469387755102\n",
      "Batch 27: loss 0.6706757170182688 | Acc 57.14285714285714\n",
      "Batch 28: loss 0.6707218970571246 | Acc 58.46938775510204\n",
      "Batch 29: loss 0.6706250022197592 | Acc 60.51020408163266\n",
      "Batch 30: loss 0.670540859301885 | Acc 60.71428571428571\n",
      "Batch 31: loss 0.6705732230217226 | Acc 58.36734693877551\n",
      "Batch 32: loss 0.6704543828964233 | Acc 61.63265306122449\n",
      "Batch 33: loss 0.6704427816651084 | Acc 59.08163265306122\n",
      "Batch 34: loss 0.6704377724843866 | Acc 57.95918367346938\n",
      "Batch 35: loss 0.6704070159367153 | Acc 59.183673469387756\n",
      "Batch 36: loss 0.67030245396826 | Acc 60.816326530612244\n",
      "Batch 37: loss 0.6703028662784679 | Acc 58.06122448979592\n",
      "Batch 38: loss 0.6703092691145445 | Acc 59.38775510204082\n",
      "Batch 39: loss 0.6702855840707437 | Acc 60.0\n",
      "Batch 40: loss 0.6701719462871552 | Acc 60.91836734693877\n",
      "Batch 41: loss 0.6701502276629936 | Acc 60.71428571428571\n",
      "Batch 42: loss 0.6701885291508266 | Acc 58.46938775510204\n",
      "Batch 43: loss 0.6700606249099554 | Acc 61.73469387755102\n",
      "Batch 44: loss 0.6700015257705342 | Acc 60.816326530612244\n",
      "Batch 45: loss 0.6700012630886502 | Acc 60.40816326530612\n",
      "Batch 46: loss 0.6699843108654022 | Acc 59.693877551020414\n",
      "Batch 47: loss 0.6699288030888172 | Acc 60.10204081632653\n",
      "Batch 48: loss 0.6698679129282633 | Acc 61.0204081632653\n",
      "Batch 49: loss 0.6698249931238136 | Acc 60.0\n",
      "Epoch 3: Training loss 0.6698249931238136 | Acc: 59.5689296126614\n"
     ]
    }
   ],
   "source": [
    "mtrainer.run_train(4, train_query_loader, train_support_loader, val_loader, lr=5e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
