{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from abc import abstractmethod\n",
    "# from models.metaclassifier.base import MetaModelBase\n",
    "\n",
    "class MetaModelBase(nn.Module):\n",
    "    def __init__(self, imgtxt_encoder, class_prototype_aggregator):\n",
    "        super().__init__()\n",
    "        self.encoder = imgtxt_encoder\n",
    "        self.class_prototype_aggregator = class_prototype_aggregator\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def set_class_prototype_details(self, class_labels, support_images, support_label_inds):\n",
    "        image_embeddings = self.encoder.embed_image(support_images, pool=True) # (B, D)\n",
    "        image_embeddings = image_embeddings.unsqueeze(1).expand(-1, support_label_inds.shape[1], -1)\n",
    "\n",
    "        self.class_prototypes = self.class_prototype_aggregator(image_embeddings, support_label_inds)\n",
    "    \n",
    "    def update_support_and_classify(self, class_labels, support_images, support_label_inds, query_images):\n",
    "        self.set_class_prototype_details(class_labels, support_images, support_label_inds)\n",
    "        return self.forward(query_images)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, query_images):\n",
    "        return query_images\n",
    "    \n",
    "    def loss(self, predictions, label_inds):\n",
    "        return self.loss_fn(predictions, label_inds.float())\n",
    "\n",
    "\n",
    "class ProtoNet(MetaModelBase):\n",
    "    def __init__(self, imgtxt_encoder, class_prototype_aggregator, distance_func, scale=1.0, trainable_base=True):\n",
    "        super(ProtoNet, self).__init__(imgtxt_encoder, class_prototype_aggregator)\n",
    "        self.distance_func = distance_func\n",
    "        self.scale = nn.Parameter(torch.tensor(scale))\n",
    "        self.set_trainable(trainable_base, trainable_base, include_text_bb=False, include_logit_scale=False)\n",
    "    \n",
    "    def set_trainable(self, trainable):\n",
    "        self.encoder.set_trainable(trainable, trainable, include_text_bb=False, include_logit_scale=False)\n",
    "\n",
    "    def forward(self, query_images):\n",
    "        query_image_embeddings = self.encoder.embed_image(query_images, pool=True)\n",
    "        query_image_embeddings = query_image_embeddings.unsqueeze(1).expand(-1, self.class_prototypes.shape[0], -1)\n",
    "        return -self.distance_func(self.class_prototypes, query_image_embeddings) * self.scale\n",
    "    \n",
    "class RelationNet(MetaModelBase):\n",
    "    def __init__(self, imgtxt_encoder, embed_dim, class_prototype_aggregator, fc_hidden_size=16, activation=nn.ReLU, dropout=0.3):\n",
    "        super(RelationNet, self).__init__(imgtxt_encoder, class_prototype_aggregator)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(embed_dim*2, fc_hidden_size),\n",
    "            activation(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fc_hidden_size, 1)\n",
    "        )\n",
    "        self.encoder.set_trainable(False, False)\n",
    "\n",
    "    def forward(self, query_images):\n",
    "        query_image_embeddings = self.encoder.embed_image(query_images, pool=True)\n",
    "        query_image_embeddings = query_image_embeddings.unsqueeze(1).expand(-1, self.class_prototypes.shape[0], -1)\n",
    "        class_prototypes = self.class_prototypes.repeat(query_image_embeddings.shape[0], 1, 1)\n",
    "       \n",
    "        out = self.cls(torch.cat((class_prototypes, query_image_embeddings), dim=2))\n",
    "        return out.squeeze(2) # NxLx1 -> NxL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from utils.device import get_device\n",
    "from utils.data import get_query_and_support_ids\n",
    "from utils.sampling import FewShotBatchSampler\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "from models.embedding.dataset import Dataset\n",
    "\n",
    "img_info = pd.read_pickle('data/vindr_cxr_split_labels.pkl')\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(img_info, 'data/vindr_train_query_set.pkl')\n",
    "\n",
    "IMG_PATH = 'datasets/vindr-cxr-png'\n",
    "\n",
    "def meta_training_loader(dataset, shots, n_ways=None, include_query=False):\n",
    "    return DataLoader(dataset, batch_sampler=FewShotBatchSampler(dataset.get_class_indicators(), shots, n_ways=n_ways, include_query=include_query))\n",
    "\n",
    "def meta_training_dataset(img_info, split):\n",
    "    img_ids = img_info[img_info['meta_split'] == split]['image_id'].to_list()\n",
    "    return Dataset(IMG_PATH, img_info, img_ids, VINDR_CXR_LABELS, VINDR_SPLIT[split], mean_std=MEAN_STDS['chestmnist'])\n",
    "\n",
    "num_shots = 5\n",
    "train_query_dataset = Dataset(IMG_PATH, img_info, query_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "train_query_loader = meta_training_loader(train_query_dataset, num_shots)\n",
    "train_support_dataset = Dataset(IMG_PATH, img_info, support_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "train_support_loader = meta_training_loader(train_support_dataset, num_shots)\n",
    "\n",
    "val_dataset = meta_training_dataset(img_info, 'val')\n",
    "val_loader = meta_training_loader(val_dataset, num_shots, include_query=True)\n",
    "\n",
    "test_dataset = meta_training_dataset(img_info, 'test')\n",
    "test_loader = meta_training_loader(test_dataset, num_shots, include_query=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models with attention\n",
    "### Run experiments on proposed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.attention.model import LabelImageAttention, LabelImagePrototypeModel\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "\n",
    "from utils.prototype import class_prototype_inf\n",
    "from models.metaclassifier.trainer import MetaTrainer\n",
    "from models.metaclassifier.model import ClsModel\n",
    "\n",
    "device =  get_device()\n",
    "\n",
    "encoder = torch.load('models/embedding/model/imgtext_model_trained.pth')\n",
    "encoder.text_model.device = device\n",
    "attention = torch.load('models/attention/model/attention-model-8h4l.pth')\n",
    "model = ClsModel(encoder, attention, 512, class_prototype_inf, fc_hidden_size=64)\n",
    "mtrainer = MetaTrainer(model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80.09955201592834, 0.46334128083966913, 0.6701876840940336)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.model, test_loader, test_dataset.class_labels())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.35860058309035, 0.5253407048569427, 0.671541600567954)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.model, val_loader, val_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrainer.model.attn_model.set_trainable(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 0.6870514154434204 | Acc 60.0\n",
      "Batch 2: loss 0.6872147917747498 | Acc 58.57142857142858\n",
      "Batch 3: loss 0.6868315935134888 | Acc 60.30612244897959\n",
      "Batch 4: loss 0.6869679391384125 | Acc 58.57142857142858\n",
      "Batch 5: loss 0.6870496034622192 | Acc 58.57142857142858\n",
      "Batch 6: loss 0.687052438656489 | Acc 58.97959183673469\n",
      "Batch 7: loss 0.686966095651899 | Acc 59.795918367346935\n",
      "Batch 8: loss 0.6868915781378746 | Acc 59.897959183673464\n",
      "Batch 9: loss 0.6867133577664694 | Acc 61.3265306122449\n",
      "Batch 10: loss 0.686601585149765 | Acc 60.91836734693877\n",
      "Batch 11: loss 0.6865592382170937 | Acc 60.204081632653065\n",
      "Batch 12: loss 0.6865238746007284 | Acc 60.204081632653065\n",
      "Batch 13: loss 0.6864938919360821 | Acc 60.204081632653065\n",
      "Batch 14: loss 0.686628269297736 | Acc 57.244897959183675\n",
      "Batch 15: loss 0.6864869594573975 | Acc 62.34693877551021\n",
      "Batch 16: loss 0.6865179091691971 | Acc 59.08163265306122\n",
      "Batch 17: loss 0.6866225109380835 | Acc 57.3469387755102\n",
      "Batch 18: loss 0.6867111722628275 | Acc 57.44897959183673\n",
      "Batch 19: loss 0.6866316732607389 | Acc 61.42857142857143\n",
      "Batch 20: loss 0.6866142392158509 | Acc 60.0\n",
      "Batch 21: loss 0.6866389626548404 | Acc 58.877551020408156\n",
      "Batch 22: loss 0.6865945702249353 | Acc 60.816326530612244\n",
      "Batch 23: loss 0.6865977463514908 | Acc 59.48979591836735\n",
      "Batch 24: loss 0.6865909472107887 | Acc 59.795918367346935\n",
      "Batch 25: loss 0.6866651844978332 | Acc 57.14285714285714\n",
      "Batch 26: loss 0.6866145661244025 | Acc 61.224489795918366\n",
      "Batch 27: loss 0.6866393685340881 | Acc 58.673469387755105\n",
      "Batch 28: loss 0.6866596128259387 | Acc 58.77551020408164\n",
      "Batch 29: loss 0.6866757623080549 | Acc 58.877551020408156\n",
      "Batch 30: loss 0.6867088913917542 | Acc 58.16326530612245\n",
      "Batch 31: loss 0.686709871215205 | Acc 59.38775510204082\n",
      "Batch 32: loss 0.6867059245705605 | Acc 59.591836734693885\n",
      "Batch 33: loss 0.6866787075996399 | Acc 60.61224489795919\n",
      "Batch 34: loss 0.6866530635777641 | Acc 60.61224489795919\n",
      "Batch 35: loss 0.6866775904382978 | Acc 58.36734693877551\n",
      "Batch 36: loss 0.686618897649977 | Acc 62.244897959183675\n",
      "Batch 37: loss 0.6866786157762682 | Acc 56.63265306122449\n",
      "Batch 38: loss 0.6867249749208751 | Acc 57.14285714285714\n",
      "Batch 39: loss 0.6867669545687162 | Acc 57.244897959183675\n",
      "Batch 40: loss 0.6867447659373284 | Acc 60.51020408163266\n",
      "Batch 41: loss 0.6867103925565394 | Acc 61.224489795918366\n",
      "Batch 42: loss 0.6866942644119263 | Acc 60.30612244897959\n",
      "Batch 43: loss 0.6866626240486322 | Acc 61.224489795918366\n",
      "Batch 44: loss 0.6866888471625068 | Acc 57.95918367346938\n",
      "Batch 45: loss 0.6867121656735738 | Acc 58.06122448979592\n",
      "Batch 46: loss 0.6866737008094788 | Acc 61.73469387755102\n",
      "Batch 47: loss 0.6866550255329051 | Acc 60.61224489795919\n",
      "Batch 48: loss 0.6866613750656446 | Acc 59.08163265306122\n",
      "Batch 49: loss 0.6866642808427617 | Acc 59.285714285714285\n",
      "Epoch 1: Training loss 0.6866642808427617 | Acc: 59.51270304039984\n",
      "Epoch 1: Validation loss 0.6716127582958766 | Accuracy 79.24198250728861 | AUC 0.5\n",
      "Batch 1: loss 0.6868806481361389 | Acc 59.183673469387756\n",
      "Batch 2: loss 0.6869191825389862 | Acc 59.08163265306122\n",
      "Batch 3: loss 0.6864915291468302 | Acc 60.816326530612244\n",
      "Batch 4: loss 0.6865494549274445 | Acc 59.38775510204082\n",
      "Batch 5: loss 0.6867394566535949 | Acc 58.36734693877551\n",
      "Batch 6: loss 0.6870085000991821 | Acc 57.244897959183675\n",
      "Batch 7: loss 0.687089570931026 | Acc 58.265306122448976\n",
      "Batch 8: loss 0.6869850978255272 | Acc 60.0\n",
      "Batch 9: loss 0.6868519186973572 | Acc 60.61224489795919\n",
      "Batch 10: loss 0.6867764055728912 | Acc 60.204081632653065\n",
      "Batch 11: loss 0.686721617525274 | Acc 60.10204081632653\n",
      "Batch 12: loss 0.6866629322369894 | Acc 60.30612244897959\n",
      "Batch 13: loss 0.6866491116010226 | Acc 59.693877551020414\n",
      "Batch 14: loss 0.6867483598845345 | Acc 57.6530612244898\n",
      "Batch 15: loss 0.6865697662035625 | Acc 62.857142857142854\n",
      "Batch 16: loss 0.6865739449858665 | Acc 59.48979591836735\n",
      "Batch 17: loss 0.6866783043917488 | Acc 57.244897959183675\n",
      "Batch 18: loss 0.6867537432246738 | Acc 57.6530612244898\n",
      "Batch 19: loss 0.6866941891218487 | Acc 60.816326530612244\n",
      "Batch 20: loss 0.6866755843162536 | Acc 59.897959183673464\n",
      "Batch 21: loss 0.6867106301443917 | Acc 58.46938775510204\n",
      "Batch 22: loss 0.686653958125548 | Acc 61.0204081632653\n",
      "Batch 23: loss 0.6866326513497726 | Acc 60.10204081632653\n",
      "Batch 24: loss 0.686671515305837 | Acc 58.265306122448976\n",
      "Batch 25: loss 0.6866947793960572 | Acc 58.673469387755105\n",
      "Batch 26: loss 0.6866682767868042 | Acc 60.30612244897959\n",
      "Batch 27: loss 0.6867245237032572 | Acc 57.44897959183673\n",
      "Batch 28: loss 0.6867823026009968 | Acc 57.244897959183675\n",
      "Batch 29: loss 0.6867984512756611 | Acc 58.673469387755105\n",
      "Batch 30: loss 0.6868186970551808 | Acc 58.46938775510204\n",
      "Batch 31: loss 0.6868351005738781 | Acc 58.57142857142858\n",
      "Batch 32: loss 0.6868553329259157 | Acc 58.36734693877551\n",
      "Batch 33: loss 0.6868223295067296 | Acc 60.61224489795919\n",
      "Batch 34: loss 0.6867935412070331 | Acc 60.51020408163266\n",
      "Batch 35: loss 0.6868220925331116 | Acc 57.95918367346938\n",
      "Batch 36: loss 0.686771023604605 | Acc 61.63265306122449\n",
      "Batch 37: loss 0.6868239141799308 | Acc 56.734693877551024\n",
      "Batch 38: loss 0.6868534731237512 | Acc 57.75510204081633\n",
      "Batch 39: loss 0.6869075176043388 | Acc 56.42857142857143\n",
      "Batch 40: loss 0.6868495732545853 | Acc 62.142857142857146\n",
      "Batch 41: loss 0.6868153766887944 | Acc 61.0204081632653\n",
      "Batch 42: loss 0.6868032330558413 | Acc 59.897959183673464\n",
      "Batch 43: loss 0.6868061631224877 | Acc 59.08163265306122\n",
      "Batch 44: loss 0.6867698986421932 | Acc 61.3265306122449\n",
      "Batch 45: loss 0.6867890291743808 | Acc 58.16326530612245\n",
      "Batch 46: loss 0.6867275056631669 | Acc 62.95918367346939\n",
      "Batch 47: loss 0.6867467000129375 | Acc 58.16326530612245\n",
      "Batch 48: loss 0.6867520598073801 | Acc 58.97959183673469\n",
      "Batch 49: loss 0.6867555890764508 | Acc 59.08163265306122\n",
      "Epoch 2: Training loss 0.6867555890764508 | Acc: 59.3252811328613\n",
      "Epoch 2: Validation loss 0.6713847603116717 | Accuracy 79.37026239067058 | AUC 0.5\n",
      "Batch 1: loss 0.6871587634086609 | Acc 58.77551020408164\n",
      "Batch 2: loss 0.6872757375240326 | Acc 58.46938775510204\n",
      "Batch 3: loss 0.6866890788078308 | Acc 60.91836734693877\n",
      "Batch 4: loss 0.6867278218269348 | Acc 59.183673469387756\n",
      "Batch 5: loss 0.6868604063987732 | Acc 58.46938775510204\n",
      "Batch 6: loss 0.6869226197401682 | Acc 58.673469387755105\n",
      "Batch 7: loss 0.6869892988886152 | Acc 58.46938775510204\n",
      "Batch 8: loss 0.6868827939033508 | Acc 60.10204081632653\n",
      "Batch 9: loss 0.6866694755024381 | Acc 61.63265306122449\n",
      "Batch 10: loss 0.6865613162517548 | Acc 60.816326530612244\n",
      "Batch 11: loss 0.6865225488489325 | Acc 60.10204081632653\n",
      "Batch 12: loss 0.686496689915657 | Acc 60.0\n",
      "Batch 13: loss 0.6864747450901911 | Acc 60.0\n",
      "Batch 14: loss 0.686629227229527 | Acc 56.83673469387755\n",
      "Batch 15: loss 0.6864707628885905 | Acc 62.55102040816326\n",
      "Batch 16: loss 0.6864984445273876 | Acc 59.08163265306122\n",
      "Batch 17: loss 0.6866149727035972 | Acc 57.04081632653061\n",
      "Batch 18: loss 0.6866880622175005 | Acc 57.75510204081633\n",
      "Batch 19: loss 0.6866462230682373 | Acc 60.40816326530612\n",
      "Batch 20: loss 0.6866006940603256 | Acc 60.61224489795919\n",
      "Batch 21: loss 0.6866676693870908 | Acc 57.6530612244898\n",
      "Batch 22: loss 0.6866466050798242 | Acc 60.0\n",
      "Batch 23: loss 0.6866137074387592 | Acc 60.40816326530612\n",
      "Batch 24: loss 0.6866292407115301 | Acc 58.97959183673469\n",
      "Batch 25: loss 0.6866811323165893 | Acc 57.75510204081633\n",
      "Batch 26: loss 0.6866355263269864 | Acc 60.91836734693877\n",
      "Batch 27: loss 0.6866513534828469 | Acc 58.877551020408156\n",
      "Batch 28: loss 0.6866884338004249 | Acc 58.06122448979592\n",
      "Batch 29: loss 0.6867283459367424 | Acc 57.85714285714286\n",
      "Batch 30: loss 0.6867708067099253 | Acc 57.6530612244898\n",
      "Batch 31: loss 0.6867902663446241 | Acc 58.46938775510204\n",
      "Batch 32: loss 0.6868035849183798 | Acc 58.673469387755105\n",
      "Batch 33: loss 0.6867304697181239 | Acc 62.34693877551021\n",
      "Batch 34: loss 0.6867424214587492 | Acc 58.77551020408164\n",
      "Batch 35: loss 0.6867446984563556 | Acc 59.183673469387756\n",
      "Batch 36: loss 0.6866966767443551 | Acc 61.530612244897966\n",
      "Batch 37: loss 0.6867170043893762 | Acc 58.36734693877551\n",
      "Batch 38: loss 0.6867279802498064 | Acc 58.77551020408164\n",
      "Batch 39: loss 0.6867524706400358 | Acc 58.06122448979592\n",
      "Batch 40: loss 0.6867089629173279 | Acc 61.530612244897966\n",
      "Batch 41: loss 0.6867173677537499 | Acc 58.877551020408156\n",
      "Batch 42: loss 0.6867197439784095 | Acc 59.183673469387756\n",
      "Batch 43: loss 0.6867238186126532 | Acc 59.08163265306122\n",
      "Batch 44: loss 0.6867098361253738 | Acc 60.10204081632653\n",
      "Batch 45: loss 0.686736626095242 | Acc 57.75510204081633\n",
      "Batch 46: loss 0.6867092692333719 | Acc 60.91836734693877\n",
      "Batch 47: loss 0.6867332357041379 | Acc 57.85714285714286\n",
      "Batch 48: loss 0.6867070533335209 | Acc 60.91836734693877\n",
      "Batch 49: loss 0.6866722921935879 | Acc 61.530612244897966\n",
      "Epoch 3: Training loss 0.6866722921935879 | Acc: 59.38775510204083\n",
      "Epoch 3: Validation loss 0.6712896704673768 | Accuracy 79.32361516034985 | AUC 0.5\n",
      "Batch 1: loss 0.6864181756973267 | Acc 59.693877551020414\n",
      "Batch 2: loss 0.6866144835948944 | Acc 59.183673469387756\n",
      "Batch 3: loss 0.6864698926607767 | Acc 60.0\n",
      "Batch 4: loss 0.6865940690040588 | Acc 58.97959183673469\n",
      "Batch 5: loss 0.6867313742637634 | Acc 58.57142857142858\n",
      "Batch 6: loss 0.6867441336313883 | Acc 59.183673469387756\n",
      "Batch 7: loss 0.6868318404470172 | Acc 58.46938775510204\n",
      "Batch 8: loss 0.6866909116506577 | Acc 60.61224489795919\n",
      "Batch 9: loss 0.6865024831559923 | Acc 61.530612244897966\n",
      "Batch 10: loss 0.6863988876342774 | Acc 60.91836734693877\n",
      "Batch 11: loss 0.6863569996573708 | Acc 60.30612244897959\n",
      "Batch 12: loss 0.6863154470920563 | Acc 60.40816326530612\n",
      "Batch 13: loss 0.6863044683749859 | Acc 60.0\n",
      "Batch 14: loss 0.686469418661935 | Acc 56.83673469387755\n",
      "Batch 15: loss 0.6863287170728047 | Acc 62.34693877551021\n",
      "Batch 16: loss 0.6863631270825863 | Acc 59.08163265306122\n",
      "Batch 17: loss 0.686486163560082 | Acc 57.04081632653061\n",
      "Batch 18: loss 0.6865648494826423 | Acc 57.75510204081633\n",
      "Batch 19: loss 0.6864775670202155 | Acc 61.63265306122449\n",
      "Batch 20: loss 0.6864620387554169 | Acc 60.0\n",
      "Batch 21: loss 0.6865155356270927 | Acc 58.16326530612245\n",
      "Batch 22: loss 0.686481697992845 | Acc 60.51020408163266\n",
      "Batch 23: loss 0.6864747638287751 | Acc 59.795918367346935\n",
      "Batch 24: loss 0.6865341116984686 | Acc 57.75510204081633\n",
      "Batch 25: loss 0.6865697598457337 | Acc 58.36734693877551\n",
      "Batch 26: loss 0.6865358765308673 | Acc 60.61224489795919\n",
      "Batch 27: loss 0.6866038353354843 | Acc 57.14285714285714\n",
      "Batch 28: loss 0.6866556448595864 | Acc 57.55102040816327\n",
      "Batch 29: loss 0.6866875373083969 | Acc 58.16326530612245\n",
      "Batch 30: loss 0.6867146511872609 | Acc 58.265306122448976\n",
      "Batch 31: loss 0.6867018015153946 | Acc 59.795918367346935\n",
      "Batch 32: loss 0.6866798624396324 | Acc 60.204081632653065\n",
      "Batch 33: loss 0.686637692379229 | Acc 61.12244897959184\n",
      "Batch 34: loss 0.68664210859467 | Acc 59.183673469387756\n",
      "Batch 35: loss 0.6866665618760245 | Acc 58.265306122448976\n",
      "Batch 36: loss 0.6866698927349515 | Acc 59.183673469387756\n",
      "Batch 37: loss 0.686705054463567 | Acc 57.6530612244898\n",
      "Batch 38: loss 0.6867154789598364 | Acc 58.77551020408164\n",
      "Batch 39: loss 0.6867071145620102 | Acc 59.693877551020414\n",
      "Batch 40: loss 0.6866576611995697 | Acc 61.836734693877546\n",
      "Batch 41: loss 0.6866337308069554 | Acc 60.61224489795919\n",
      "Batch 42: loss 0.6866636219478789 | Acc 57.75510204081633\n",
      "Batch 43: loss 0.6866240778634715 | Acc 61.530612244897966\n",
      "Batch 44: loss 0.6866276413202286 | Acc 59.183673469387756\n",
      "Batch 45: loss 0.6866837528016833 | Acc 56.12244897959183\n",
      "Batch 46: loss 0.686661764331486 | Acc 60.61224489795919\n",
      "Batch 47: loss 0.6866524777513869 | Acc 59.897959183673464\n",
      "Batch 48: loss 0.686646856367588 | Acc 59.693877551020414\n",
      "Batch 49: loss 0.6866446782131584 | Acc 59.48979591836735\n",
      "Epoch 4: Training loss 0.6866446782131584 | Acc: 59.377342773844205\n",
      "Epoch 4: Validation loss 0.6711220264434814 | Accuracy 79.37026239067055 | AUC 0.5\n",
      "Best epoch:  2\n"
     ]
    }
   ],
   "source": [
    "# model seems to have a tendency to overfit, improving performance on training seems to quickly lead to a degradation for test and val\n",
    "mtrainer.run_train(4, train_query_loader, train_support_loader, val_loader,  train_query_dataset.class_labels(), val_dataset.class_labels(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.42857142857143, 0.5, 0.671340092590877)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, val_loader, val_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80.19910403185662, 0.5, 0.6707499230780253)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, test_loader, test_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.best_model.cls.state_dict(), 'models/metaclassifier/model/comb2/cls_weights-64.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.best_model.attn_model, 'models/metaclassifier/model/comb2/attention-model-8h4l.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototypical Network with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.attention.model import LabelImageAttention, LabelImagePrototypeModel\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "\n",
    "from utils.prototype import class_prototype_inf\n",
    "from models.metaclassifier.base import euclidean_distance\n",
    "from models.metaclassifier.trainer import MetaTrainer\n",
    "from models.metaclassifier.model import ProtoNetAttention\n",
    "\n",
    "device =  get_device()\n",
    "\n",
    "encoder = torch.load('models/embedding/model/imgtext_model_trained.pth')\n",
    "encoder.text_model.device = device\n",
    "attention = torch.load('models/attention/model/attention-model-8h4l.pth')\n",
    "# imgtxt_encoder, attn_model, class_prototype_aggregator, distance_func\n",
    "model = ProtoNetAttention(encoder, attention, class_prototype_inf, euclidean_distance)\n",
    "mtrainer = MetaTrainer(model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naomileow/Documents/school/CS6240/project/utils/prototype.py:55: UserWarning: The operator 'aten::nonzero' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:11.)\n",
      "  classes_count = torch.nonzero(label_inds)[:,1].bincount()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80.39820806371328, 0.5402599822857049, 0.6664394343771586)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(model, test_loader, test_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.37026239067053, 0.5161583489962621, 0.6703500832830157)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(model, val_loader, val_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 0.645804762840271 | Acc 59.795918367346935\n",
      "Batch 2: loss 0.64562126994133 | Acc 58.877551020408156\n",
      "Batch 3: loss 0.6446925004323324 | Acc 60.51020408163266\n",
      "Batch 4: loss 0.6453973799943924 | Acc 59.285714285714285\n",
      "Batch 5: loss 0.6450785756111145 | Acc 58.673469387755105\n",
      "Batch 6: loss 0.6450744767983755 | Acc 59.38775510204082\n",
      "Batch 7: loss 0.6451364670481 | Acc 58.57142857142858\n",
      "Batch 8: loss 0.6450803205370903 | Acc 60.71428571428571\n",
      "Batch 9: loss 0.6445973383055793 | Acc 61.530612244897966\n",
      "Batch 10: loss 0.6443249225616455 | Acc 60.71428571428571\n",
      "Batch 11: loss 0.6443740454587069 | Acc 59.591836734693885\n",
      "Batch 12: loss 0.6442251602808634 | Acc 60.10204081632653\n",
      "Batch 13: loss 0.6441494272305415 | Acc 60.0\n",
      "Batch 14: loss 0.6442059363637652 | Acc 57.44897959183673\n",
      "Batch 15: loss 0.6439297715822856 | Acc 62.755102040816325\n",
      "Batch 16: loss 0.6437373459339142 | Acc 59.08163265306122\n",
      "Batch 17: loss 0.6442396991393146 | Acc 56.93877551020409\n",
      "Batch 18: loss 0.644419295920266 | Acc 57.6530612244898\n",
      "Batch 19: loss 0.6443859276018644 | Acc 61.224489795918366\n",
      "Batch 20: loss 0.644442817568779 | Acc 60.204081632653065\n",
      "Batch 21: loss 0.644468279111953 | Acc 58.36734693877551\n",
      "Batch 22: loss 0.6442746682600542 | Acc 60.61224489795919\n",
      "Batch 23: loss 0.6440734267234802 | Acc 59.48979591836735\n",
      "Batch 24: loss 0.6440364544590315 | Acc 58.57142857142858\n",
      "Batch 25: loss 0.6440093660354614 | Acc 59.08163265306122\n",
      "Batch 26: loss 0.6440682090245761 | Acc 59.693877551020414\n",
      "Batch 27: loss 0.6444474480770253 | Acc 58.57142857142858\n",
      "Batch 28: loss 0.6445617356470653 | Acc 58.265306122448976\n",
      "Batch 29: loss 0.6447201946686054 | Acc 59.48979591836735\n",
      "Batch 30: loss 0.6448805093765259 | Acc 59.285714285714285\n",
      "Batch 31: loss 0.6448275543028309 | Acc 60.30612244897959\n",
      "Batch 32: loss 0.6447042729705572 | Acc 59.38775510204082\n",
      "Batch 33: loss 0.6446310606869784 | Acc 61.224489795918366\n",
      "Batch 34: loss 0.6444737595670363 | Acc 61.3265306122449\n",
      "Batch 35: loss 0.6446235792977469 | Acc 59.183673469387756\n",
      "Batch 36: loss 0.644515100452635 | Acc 58.97959183673469\n",
      "Batch 37: loss 0.6445720469629442 | Acc 58.97959183673469\n",
      "Batch 38: loss 0.6446787918868818 | Acc 56.63265306122449\n",
      "Batch 39: loss 0.6446952407176678 | Acc 59.183673469387756\n",
      "Batch 40: loss 0.6447015598416328 | Acc 59.693877551020414\n",
      "Batch 41: loss 0.6445464331929277 | Acc 63.469387755102034\n",
      "Batch 42: loss 0.644495780978884 | Acc 59.08163265306122\n",
      "Batch 43: loss 0.6445212960243225 | Acc 58.57142857142858\n",
      "Batch 44: loss 0.6444095495072278 | Acc 62.55102040816326\n",
      "Batch 45: loss 0.6444143811861675 | Acc 57.44897959183673\n",
      "Batch 46: loss 0.6443672724392103 | Acc 60.61224489795919\n",
      "Batch 47: loss 0.644300141233079 | Acc 59.08163265306122\n",
      "Batch 48: loss 0.6441943148771921 | Acc 58.77551020408164\n",
      "Batch 49: loss 0.644330764303402 | Acc 58.16326530612245\n",
      "Epoch 1: Training loss 0.644330764303402 | Acc: 59.53352769679299\n",
      "Epoch 1: Validation loss 0.6641835195677621 | Accuracy 79.12536443148687 | AUC 0.5295648644525054\n",
      "Batch 1: loss 0.6464527249336243 | Acc 59.897959183673464\n",
      "Batch 2: loss 0.6458544433116913 | Acc 58.97959183673469\n",
      "Batch 3: loss 0.6446337898572286 | Acc 60.0\n",
      "Batch 4: loss 0.6461596041917801 | Acc 58.77551020408164\n",
      "Batch 5: loss 0.6460518360137939 | Acc 58.57142857142858\n",
      "Batch 6: loss 0.6460611820220947 | Acc 59.38775510204082\n",
      "Batch 7: loss 0.6462189384869167 | Acc 58.16326530612245\n",
      "Batch 8: loss 0.6465961188077927 | Acc 60.51020408163266\n",
      "Batch 9: loss 0.6457337074809604 | Acc 61.224489795918366\n",
      "Batch 10: loss 0.6456472396850585 | Acc 60.71428571428571\n",
      "Batch 11: loss 0.6451704556291754 | Acc 60.51020408163266\n",
      "Batch 12: loss 0.6448740611473719 | Acc 59.285714285714285\n",
      "Batch 13: loss 0.645025005707374 | Acc 59.795918367346935\n",
      "Batch 14: loss 0.6451449223927089 | Acc 57.44897959183673\n",
      "Batch 15: loss 0.6449779907862345 | Acc 62.34693877551021\n",
      "Batch 16: loss 0.644949484616518 | Acc 59.48979591836735\n",
      "Batch 17: loss 0.6451979805441463 | Acc 56.53061224489796\n",
      "Batch 18: loss 0.6452026930120256 | Acc 57.85714285714286\n",
      "Batch 19: loss 0.645241097400063 | Acc 61.836734693877546\n",
      "Batch 20: loss 0.6450112730264663 | Acc 60.51020408163266\n",
      "Batch 21: loss 0.6450335355032057 | Acc 57.75510204081633\n",
      "Batch 22: loss 0.6448475230823864 | Acc 61.0204081632653\n",
      "Batch 23: loss 0.6448500571043595 | Acc 59.38775510204082\n",
      "Batch 24: loss 0.6447729443510374 | Acc 59.183673469387756\n",
      "Batch 25: loss 0.6447567558288574 | Acc 58.265306122448976\n",
      "Batch 26: loss 0.6445562747808603 | Acc 61.12244897959184\n",
      "Batch 27: loss 0.6447264772874338 | Acc 58.265306122448976\n",
      "Batch 28: loss 0.6448971884591239 | Acc 56.83673469387755\n",
      "Batch 29: loss 0.6452014322938591 | Acc 57.75510204081633\n",
      "Batch 30: loss 0.6452295184135437 | Acc 58.77551020408164\n",
      "Batch 31: loss 0.6452498012973417 | Acc 59.285714285714285\n",
      "Batch 32: loss 0.6454197894781828 | Acc 58.673469387755105\n",
      "Batch 33: loss 0.6453623265931101 | Acc 59.693877551020414\n",
      "Batch 34: loss 0.6451991393285639 | Acc 61.12244897959184\n",
      "Batch 35: loss 0.6451753411974226 | Acc 56.53061224489796\n",
      "Batch 36: loss 0.6451225678126017 | Acc 59.693877551020414\n",
      "Batch 37: loss 0.6452148524490563 | Acc 58.77551020408164\n",
      "Batch 38: loss 0.6452572518273404 | Acc 59.48979591836735\n",
      "Batch 39: loss 0.6452626158029605 | Acc 58.97959183673469\n",
      "Batch 40: loss 0.6452720627188683 | Acc 60.51020408163266\n",
      "Batch 41: loss 0.6451658359388026 | Acc 62.04081632653061\n",
      "Batch 42: loss 0.6454526327905201 | Acc 57.6530612244898\n",
      "Batch 43: loss 0.6454908057700756 | Acc 59.08163265306122\n",
      "Batch 44: loss 0.645400112325495 | Acc 60.71428571428571\n",
      "Batch 45: loss 0.6453922775056627 | Acc 57.6530612244898\n",
      "Batch 46: loss 0.6452509229597838 | Acc 60.91836734693877\n",
      "Batch 47: loss 0.6452864966493972 | Acc 59.183673469387756\n",
      "Batch 48: loss 0.6451647480328878 | Acc 60.40816326530612\n",
      "Batch 49: loss 0.64504068603321 | Acc 61.0204081632653\n",
      "Epoch 2: Training loss 0.64504068603321 | Acc: 59.421074552269886\n",
      "Epoch 2: Validation loss 0.6642832006726946 | Accuracy 79.5218658892128 | AUC 0.5293396045695373\n",
      "Best epoch:  2\n"
     ]
    }
   ],
   "source": [
    "mtrainer.run_train(2, train_query_loader, train_support_loader, val_loader,  train_query_dataset.class_labels(), val_dataset.class_labels(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(1.0008, device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(mtrainer.best_model.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80.38825286212047, 0.5417728137567698, 0.6569410795118751)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, test_loader, test_dataset.class_labels())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiments on baseline models without attention\n",
    "### RelationNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.attention.model import LabelImageAttention, LabelImagePrototypeModel\n",
    "from models.embedding.model import ImageTextEmbedding, TextEncoder, ImageEncoder\n",
    "\n",
    "from utils.prototype import class_prototype_inf\n",
    "from models.metaclassifier.trainer import MetaTrainer\n",
    "from models.metaclassifier.baselines import RelationNet\n",
    "\n",
    "device =  get_device()\n",
    "\n",
    "encoder = torch.load('imgtext_model_trained.pth')\n",
    "encoder.text_model.device = device\n",
    "base_model = RelationNet(encoder, 512, class_prototype_inf, fc_hidden_size=64)\n",
    "btrainer = MetaTrainer(base_model, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76.40816326530613, 0.4814422659229693, 0.6890060526984079)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btrainer.run_eval(btrainer.model, val_loader, val_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75.85863613738178, 0.46753415772972007, 0.6887184134343776)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btrainer.run_eval(btrainer.model, test_loader, test_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 0.6922392845153809 | Acc 55.81632653061225\n",
      "Batch 2: loss 0.6922772824764252 | Acc 56.42857142857143\n",
      "Batch 3: loss 0.6921975612640381 | Acc 56.53061224489796\n",
      "Batch 4: loss 0.6921016573905945 | Acc 56.42857142857143\n",
      "Batch 5: loss 0.6920598268508911 | Acc 56.93877551020409\n",
      "Batch 6: loss 0.6919763882954916 | Acc 57.95918367346938\n",
      "Batch 7: loss 0.6918906910078866 | Acc 57.14285714285714\n",
      "Batch 8: loss 0.6917958334088326 | Acc 58.57142857142858\n",
      "Batch 9: loss 0.6916449202431573 | Acc 59.897959183673464\n",
      "Batch 10: loss 0.6915410220623016 | Acc 58.97959183673469\n",
      "Batch 11: loss 0.6914029825817455 | Acc 59.897959183673464\n",
      "Batch 12: loss 0.6912817309300104 | Acc 59.48979591836735\n",
      "Batch 13: loss 0.6911432926471417 | Acc 59.38775510204082\n",
      "Batch 14: loss 0.6910162568092346 | Acc 59.38775510204082\n",
      "Batch 15: loss 0.6908564249674479 | Acc 60.40816326530612\n",
      "Batch 16: loss 0.6907406598329544 | Acc 58.97959183673469\n",
      "Batch 17: loss 0.6905901607345132 | Acc 59.285714285714285\n",
      "Batch 18: loss 0.6904586421118842 | Acc 59.48979591836735\n",
      "Batch 19: loss 0.6903580364428068 | Acc 57.95918367346938\n",
      "Batch 20: loss 0.6902426958084107 | Acc 58.77551020408164\n",
      "Batch 21: loss 0.6900684095564342 | Acc 59.591836734693885\n",
      "Batch 22: loss 0.6899705122817646 | Acc 58.06122448979592\n",
      "Batch 23: loss 0.6898851887039517 | Acc 57.55102040816327\n",
      "Batch 24: loss 0.6897051880757014 | Acc 62.04081632653061\n",
      "Batch 25: loss 0.6895659399032593 | Acc 59.795918367346935\n",
      "Batch 26: loss 0.6894316283556131 | Acc 59.285714285714285\n",
      "Batch 27: loss 0.6893438741012856 | Acc 57.55102040816327\n",
      "Batch 28: loss 0.6892368580613818 | Acc 58.673469387755105\n",
      "Batch 29: loss 0.6891068713418369 | Acc 58.97959183673469\n",
      "Batch 30: loss 0.6889615774154663 | Acc 60.204081632653065\n",
      "Batch 31: loss 0.688858893609816 | Acc 57.95918367346938\n",
      "Batch 32: loss 0.6887626629322767 | Acc 58.673469387755105\n",
      "Batch 33: loss 0.688580516612891 | Acc 61.0204081632653\n",
      "Batch 34: loss 0.6885147988796234 | Acc 56.93877551020409\n",
      "Batch 35: loss 0.6882957662854876 | Acc 63.9795918367347\n",
      "Batch 36: loss 0.6881766865650812 | Acc 58.97959183673469\n",
      "Batch 37: loss 0.6881095235412186 | Acc 57.85714285714286\n",
      "Batch 38: loss 0.6878698421152014 | Acc 62.857142857142854\n",
      "Batch 39: loss 0.6877533717033191 | Acc 58.36734693877551\n",
      "Batch 40: loss 0.6875804081559181 | Acc 59.897959183673464\n",
      "Batch 41: loss 0.6874986363620292 | Acc 57.3469387755102\n",
      "Batch 42: loss 0.6873420945235661 | Acc 61.0204081632653\n",
      "Batch 43: loss 0.6871476686278055 | Acc 61.12244897959184\n",
      "Batch 44: loss 0.6869287084449421 | Acc 63.469387755102034\n",
      "Batch 45: loss 0.6868225296338399 | Acc 58.97959183673469\n",
      "Batch 46: loss 0.6866591883742291 | Acc 60.71428571428571\n",
      "Batch 47: loss 0.6864511180431285 | Acc 62.55102040816326\n",
      "Batch 48: loss 0.6863410274187723 | Acc 59.183673469387756\n",
      "Batch 49: loss 0.6861897439372783 | Acc 60.61224489795919\n",
      "Epoch 1: Training loss 0.6861897439372783 | Acc: 59.2044981257809\n",
      "Epoch 1: Validation loss 0.6591783268111092 | Accuracy 78.68221574344024 | AUC 0.5174450021592617\n",
      "Batch 1: loss 0.6794984936714172 | Acc 60.51020408163266\n",
      "Batch 2: loss 0.679203987121582 | Acc 60.0\n",
      "Batch 3: loss 0.6792896588643392 | Acc 60.0\n",
      "Batch 4: loss 0.6793586313724518 | Acc 59.38775510204082\n",
      "Batch 5: loss 0.6795257568359375 | Acc 57.95918367346938\n",
      "Batch 6: loss 0.6796328822771708 | Acc 58.265306122448976\n",
      "Batch 7: loss 0.6800575852394104 | Acc 57.14285714285714\n",
      "Batch 8: loss 0.6798605695366859 | Acc 58.57142857142858\n",
      "Batch 9: loss 0.67949288421207 | Acc 59.591836734693885\n",
      "Batch 10: loss 0.6793319642543793 | Acc 58.877551020408156\n",
      "Batch 11: loss 0.678920713337985 | Acc 60.0\n",
      "Batch 12: loss 0.67886454363664 | Acc 58.06122448979592\n",
      "Batch 13: loss 0.6787226429352393 | Acc 59.591836734693885\n",
      "Batch 14: loss 0.6785929203033447 | Acc 59.38775510204082\n",
      "Batch 15: loss 0.6783170580863953 | Acc 60.40816326530612\n",
      "Batch 16: loss 0.6781174838542938 | Acc 59.48979591836735\n",
      "Batch 17: loss 0.6780138822162852 | Acc 58.673469387755105\n",
      "Batch 18: loss 0.6778813401858012 | Acc 59.48979591836735\n",
      "Batch 19: loss 0.6779164671897888 | Acc 57.55102040816327\n",
      "Batch 20: loss 0.6778457194566727 | Acc 58.77551020408164\n",
      "Batch 21: loss 0.6777306028774807 | Acc 59.183673469387756\n",
      "Batch 22: loss 0.6777736002748663 | Acc 57.55102040816327\n",
      "Batch 23: loss 0.6777204119640848 | Acc 58.877551020408156\n",
      "Batch 24: loss 0.6774991825222969 | Acc 61.42857142857143\n",
      "Batch 25: loss 0.6772192907333374 | Acc 61.3265306122449\n",
      "Batch 26: loss 0.6770845055580139 | Acc 59.08163265306122\n",
      "Batch 27: loss 0.6770436675460251 | Acc 58.06122448979592\n",
      "Batch 28: loss 0.6769895809037345 | Acc 57.95918367346938\n",
      "Batch 29: loss 0.6767991370168226 | Acc 60.0\n",
      "Batch 30: loss 0.6766861975193024 | Acc 60.10204081632653\n",
      "Batch 31: loss 0.676718202329451 | Acc 57.75510204081633\n",
      "Batch 32: loss 0.6766557916998863 | Acc 58.36734693877551\n",
      "Batch 33: loss 0.6763838222532561 | Acc 60.61224489795919\n",
      "Batch 34: loss 0.6764291437233195 | Acc 57.04081632653061\n",
      "Batch 35: loss 0.6760616336550032 | Acc 64.08163265306122\n",
      "Batch 36: loss 0.6758561962180667 | Acc 61.530612244897966\n",
      "Batch 37: loss 0.6757375307985254 | Acc 60.71428571428571\n",
      "Batch 38: loss 0.675583585312492 | Acc 62.244897959183675\n",
      "Batch 39: loss 0.6754604134804163 | Acc 60.30612244897959\n",
      "Batch 40: loss 0.6753481760621071 | Acc 60.0\n",
      "Batch 41: loss 0.6753720728362479 | Acc 58.265306122448976\n",
      "Batch 42: loss 0.6752108818008786 | Acc 60.30612244897959\n",
      "Batch 43: loss 0.6750412818997406 | Acc 60.61224489795919\n",
      "Batch 44: loss 0.6748815354975787 | Acc 60.30612244897959\n",
      "Batch 45: loss 0.6747330930497911 | Acc 59.897959183673464\n",
      "Batch 46: loss 0.6747071069219838 | Acc 59.897959183673464\n",
      "Batch 47: loss 0.6744581143906776 | Acc 63.775510204081634\n",
      "Batch 48: loss 0.6743760332465172 | Acc 59.38775510204082\n",
      "Batch 49: loss 0.6743285449183717 | Acc 59.183673469387756\n",
      "Epoch 2: Training loss 0.6743285449183717 | Acc: 59.5835068721366\n",
      "Epoch 2: Validation loss 0.6308378747531346 | Accuracy 78.7521865889213 | AUC 0.5215574423686749\n",
      "Batch 1: loss 0.6671068668365479 | Acc 61.530612244897966\n",
      "Batch 2: loss 0.6695881187915802 | Acc 59.693877551020414\n",
      "Batch 3: loss 0.6696506142616272 | Acc 61.224489795918366\n",
      "Batch 4: loss 0.6696416437625885 | Acc 58.877551020408156\n",
      "Batch 5: loss 0.6704630374908447 | Acc 57.75510204081633\n",
      "Batch 6: loss 0.6708731055259705 | Acc 58.77551020408164\n",
      "Batch 7: loss 0.6717420986720494 | Acc 57.244897959183675\n",
      "Batch 8: loss 0.6715336218476295 | Acc 59.591836734693885\n",
      "Batch 9: loss 0.6710280643569099 | Acc 60.204081632653065\n",
      "Batch 10: loss 0.670782345533371 | Acc 60.51020408163266\n",
      "Batch 11: loss 0.6697549494830045 | Acc 61.224489795918366\n",
      "Batch 12: loss 0.6696996539831161 | Acc 59.693877551020414\n",
      "Batch 13: loss 0.6693907150855432 | Acc 60.40816326530612\n",
      "Batch 14: loss 0.6693734441484723 | Acc 61.12244897959184\n",
      "Batch 15: loss 0.6690911293029785 | Acc 61.224489795918366\n",
      "Batch 16: loss 0.6690889820456505 | Acc 60.30612244897959\n",
      "Batch 17: loss 0.6689454106723561 | Acc 59.38775510204082\n",
      "Batch 18: loss 0.6689656575520834 | Acc 60.10204081632653\n",
      "Batch 19: loss 0.6692209933933458 | Acc 57.3469387755102\n",
      "Batch 20: loss 0.6690206527709961 | Acc 60.0\n",
      "Batch 21: loss 0.6688638187590099 | Acc 60.816326530612244\n",
      "Batch 22: loss 0.668828934431076 | Acc 58.77551020408164\n",
      "Batch 23: loss 0.668866929800614 | Acc 59.38775510204082\n",
      "Batch 24: loss 0.6685812075932821 | Acc 61.836734693877546\n",
      "Batch 25: loss 0.6684433007240296 | Acc 60.816326530612244\n",
      "Batch 26: loss 0.6681980926256913 | Acc 61.0204081632653\n",
      "Batch 27: loss 0.6680872197504397 | Acc 60.204081632653065\n",
      "Batch 28: loss 0.6680277990443366 | Acc 59.38775510204082\n",
      "Batch 29: loss 0.667841033688907 | Acc 62.44897959183674\n",
      "Batch 30: loss 0.6678104221820831 | Acc 59.693877551020414\n",
      "Batch 31: loss 0.6681316129622921 | Acc 56.93877551020409\n",
      "Batch 32: loss 0.6682349387556314 | Acc 57.44897959183673\n",
      "Batch 33: loss 0.668085596778176 | Acc 60.40816326530612\n",
      "Batch 34: loss 0.6683073517154244 | Acc 57.244897959183675\n",
      "Batch 35: loss 0.6681495359965733 | Acc 61.0204081632653\n",
      "Batch 36: loss 0.6681797206401825 | Acc 60.30612244897959\n",
      "Batch 37: loss 0.6679497055105261 | Acc 63.469387755102034\n",
      "Batch 38: loss 0.6677134052703255 | Acc 63.87755102040816\n",
      "Batch 39: loss 0.6676883330711951 | Acc 59.795918367346935\n",
      "Batch 40: loss 0.6675750225782394 | Acc 60.30612244897959\n",
      "Batch 41: loss 0.6677043379806891 | Acc 57.44897959183673\n",
      "Batch 42: loss 0.667630136013031 | Acc 61.63265306122449\n",
      "Batch 43: loss 0.6675477679385695 | Acc 60.40816326530612\n",
      "Batch 44: loss 0.6673743331974203 | Acc 63.57142857142857\n",
      "Batch 45: loss 0.6673522737291124 | Acc 59.693877551020414\n",
      "Batch 46: loss 0.6672070894552313 | Acc 61.93877551020408\n",
      "Batch 47: loss 0.6669787510912469 | Acc 64.38775510204081\n",
      "Batch 48: loss 0.666894386212031 | Acc 59.48979591836735\n",
      "Batch 49: loss 0.6668269123349871 | Acc 62.244897959183675\n",
      "Epoch 3: Training loss 0.6668269123349871 | Acc: 60.249895876718035\n",
      "Epoch 3: Validation loss 0.6146493826593672 | Accuracy 77.87755102040818 | AUC 0.5236970305311497\n",
      "Batch 1: loss 0.6645023822784424 | Acc 61.530612244897966\n",
      "Batch 2: loss 0.666167676448822 | Acc 60.816326530612244\n",
      "Batch 3: loss 0.6659182707468668 | Acc 62.34693877551021\n",
      "Batch 4: loss 0.6658838391304016 | Acc 59.897959183673464\n",
      "Batch 5: loss 0.6661906957626342 | Acc 59.897959183673464\n",
      "Batch 6: loss 0.6667752464612325 | Acc 59.795918367346935\n",
      "Batch 7: loss 0.6679234334400722 | Acc 59.285714285714285\n",
      "Batch 8: loss 0.6671867147088051 | Acc 60.30612244897959\n",
      "Batch 9: loss 0.6666208836767409 | Acc 61.73469387755102\n",
      "Batch 10: loss 0.6662196159362793 | Acc 61.73469387755102\n",
      "Batch 11: loss 0.6653882806951349 | Acc 61.0204081632653\n",
      "Batch 12: loss 0.6650811582803726 | Acc 61.73469387755102\n",
      "Batch 13: loss 0.6648334035506616 | Acc 60.40816326530612\n",
      "Batch 14: loss 0.6646990392889295 | Acc 61.224489795918366\n",
      "Batch 15: loss 0.664291787147522 | Acc 63.775510204081634\n",
      "Batch 16: loss 0.6643046028912067 | Acc 60.61224489795919\n",
      "Batch 17: loss 0.664245580925661 | Acc 59.183673469387756\n",
      "Batch 18: loss 0.6643294029765658 | Acc 61.42857142857143\n",
      "Batch 19: loss 0.6647240143073233 | Acc 58.16326530612245\n",
      "Batch 20: loss 0.6646039873361588 | Acc 60.0\n",
      "Batch 21: loss 0.664353316738492 | Acc 63.57142857142857\n",
      "Batch 22: loss 0.6642938174984672 | Acc 60.816326530612244\n",
      "Batch 23: loss 0.6642630540806315 | Acc 61.530612244897966\n",
      "Batch 24: loss 0.6639002313216528 | Acc 63.06122448979592\n",
      "Batch 25: loss 0.6638231682777405 | Acc 59.48979591836735\n",
      "Batch 26: loss 0.6635776528945336 | Acc 62.142857142857146\n",
      "Batch 27: loss 0.6636184829252737 | Acc 60.204081632653065\n",
      "Batch 28: loss 0.6635727712086269 | Acc 60.71428571428571\n",
      "Batch 29: loss 0.6632150637692419 | Acc 64.38775510204081\n",
      "Batch 30: loss 0.663139541943868 | Acc 61.530612244897966\n",
      "Batch 31: loss 0.6634142341152314 | Acc 57.85714285714286\n",
      "Batch 32: loss 0.6636482607573271 | Acc 57.95918367346938\n",
      "Batch 33: loss 0.663311318917708 | Acc 63.6734693877551\n",
      "Batch 34: loss 0.6634208419743706 | Acc 59.48979591836735\n",
      "Batch 35: loss 0.6631177595683506 | Acc 63.163265306122454\n",
      "Batch 36: loss 0.6631718907091353 | Acc 60.61224489795919\n",
      "Batch 37: loss 0.6631984324068636 | Acc 62.142857142857146\n",
      "Batch 38: loss 0.6629828481297744 | Acc 63.06122448979592\n",
      "Batch 39: loss 0.6628193305088923 | Acc 63.06122448979592\n",
      "Batch 40: loss 0.6625381708145142 | Acc 63.775510204081634\n",
      "Batch 41: loss 0.6626030061303115 | Acc 60.10204081632653\n",
      "Batch 42: loss 0.6624367308048975 | Acc 63.469387755102034\n",
      "Batch 43: loss 0.6622579028440077 | Acc 63.36734693877551\n",
      "Batch 44: loss 0.6621849699453874 | Acc 61.42857142857143\n",
      "Batch 45: loss 0.6621556798617045 | Acc 61.63265306122449\n",
      "Batch 46: loss 0.6621394572050675 | Acc 62.142857142857146\n",
      "Batch 47: loss 0.6620904638412151 | Acc 61.93877551020408\n",
      "Batch 48: loss 0.6618497011562189 | Acc 63.57142857142857\n",
      "Batch 49: loss 0.6617987800617607 | Acc 62.65306122448979\n",
      "Epoch 4: Training loss 0.6617987800617607 | Acc: 61.376509787588496\n",
      "Epoch 4: Validation loss 0.6034654668399266 | Accuracy 76.89795918367345 | AUC 0.525221066447877\n",
      "Best epoch:  2\n"
     ]
    }
   ],
   "source": [
    "btrainer.run_train(4, train_query_loader, train_support_loader, val_loader,  train_query_dataset.class_labels(), val_dataset.class_labels(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78.69387755102042, 0.5204702109376685, 0.6312191026551383)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btrainer.run_eval(btrainer.best_model, val_loader, val_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79.25335988053757, 0.510349916356135, 0.6320822573289638)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "btrainer.run_eval(btrainer.best_model, test_loader, test_dataset.class_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(btrainer.best_model.cls.state_dict(), 'relnet_weights-64.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
