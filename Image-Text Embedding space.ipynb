{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def load_pretrained_resnet(img_channels, num_classes, save_path, fc_bias=True):\n",
    "    model = torchvision.models.resnet50(num_classes=num_classes, weights=None)\n",
    "    if fc_bias == False:\n",
    "        model.fc = nn.Linear(2048, num_classes, bias=False)\n",
    "    model.conv1 = torch.nn.Conv2d(img_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    return model\n",
    "\n",
    "def resnet_backbone(model):\n",
    "    return torch.nn.Sequential(*(list(model.children())[:-2]))\n",
    "\n",
    "def load_medclip_retrained_resnet(path):\n",
    "    return resnet_backbone(load_pretrained_resnet(1, 512, path, False))\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, backbone, embed_dims, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.proj = nn.Linear(2048, embed_dims)\n",
    "        if freeze_backbone:\n",
    "            self.set_backbone_trainable(False)\n",
    "    \n",
    "    def set_backbone_trainable(self, trainable):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = trainable\n",
    "\n",
    "    def forward(self, input):\n",
    "        # B, C, H, W\n",
    "        img = self.backbone(input)\n",
    "        # B, C, H, W -> B, H, W, C -> B, D, H, W\n",
    "        return self.proj(img.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dims, device='cpu', freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.backbone = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.proj = nn.Linear(768, embed_dims)\n",
    "        self.device = device\n",
    "        if freeze_backbone:\n",
    "            self.set_backbone_trainable(False)\n",
    "\n",
    "    def set_backbone_trainable(self, trainable):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = trainable\n",
    "    \n",
    "    def forward(self, input):\n",
    "        tokens = self.tokenizer(input, max_length=77, return_tensors='pt', padding='max_length').to(self.device)\n",
    "        out = self.backbone(**tokens)\n",
    "        enc = out['pooler_output']\n",
    "        # enc = out['last_hidden_state'][:, 0]\n",
    "        return self.proj(enc)\n",
    "\n",
    "class ImageTextEmbedding(nn.Module):\n",
    "    def __init__(self, img_backbone, embed_dims, logit_scale_init_value=0.1, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.text_model = TextEncoder(embed_dims, device)\n",
    "        self.img_model = ImageEncoder(img_backbone, embed_dims)\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1/logit_scale_init_value)))\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def embed_text(self, text):\n",
    "        text_emb = self.text_model(text)\n",
    "        return text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    def embed_image(self, image, pool=False):\n",
    "        img_emb = self.img_model(image) # B, D, H, W\n",
    "        if pool:\n",
    "            img_emb = self.flatten(self.gap(img_emb)) # B, D\n",
    "        return img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    \n",
    "    def compute_logits(self, text_emb, img_emb):\n",
    "        self.logit_scale.data = torch.clamp(self.logit_scale.data, 0, 4.6052)\n",
    "        logit_scale = self.logit_scale.exp()        \n",
    "        if len(img_emb.shape) == 4:\n",
    "            logits_per_image = logit_scale * torch.matmul(img_emb.permute(2,3,0,1), text_emb.t())\n",
    "        else:\n",
    "            logits_per_image = logit_scale * torch.matmul(img_emb, text_emb.t())\n",
    "        \n",
    "        if len(img_emb.shape) == 4:\n",
    "            logits_per_text = logits_per_image.permute(0,1,3,2) # HxWxBxB\n",
    "        else:\n",
    "            logits_per_text = logits_per_image.t()\n",
    "        return logits_per_text, logits_per_image\n",
    "        \n",
    "    def forward(self, text, img, pool=False):\n",
    "        text_emb = self.embed_text(text)\n",
    "        img_emb = self.embed_image(img, pool)\n",
    "\n",
    "        return text_emb, img_emb\n",
    "    \n",
    "    def contrastive_logit_loss(self, logits_per_text, logits_per_image, labels):\n",
    "         # Image-label contrastive loss, which is similar to classification loss, except using the computed logits\n",
    "        itl = self.criterion(logits_per_image, labels)\n",
    "        til = self.criterion(logits_per_text, labels.t())\n",
    "        return (itl+til) / 2\n",
    "    \n",
    "    def loss(self, text_emb, img_emb, labels):\n",
    "        # text_embed should be an NxD matrix where N is the number of classes, so each row is the text embedding for the ith class\n",
    "        # image embed: BxD\n",
    "        # labels is an BxN indicator matrix with 1 for each class an image belongs to\n",
    "        logits_per_text, logits_per_image = self.compute_logits(text_emb, img_emb)\n",
    "        \n",
    "        return self.contrastive_logit_loss(logits_per_text, logits_per_image, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from utils.metrics import AverageMeter, calculate_auc, multilabel_accuracy\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, class_labels, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.class_labels = class_labels\n",
    "    \n",
    "    def run_train(self, epochs, dataloader, val_dataloader):\n",
    "        model = self.model.to(self.device)\n",
    "        best_epoch = None\n",
    "        best_acc = None\n",
    "        # optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            loss_meter = AverageMeter()\n",
    "            for i, (images, class_inds) in enumerate(dataloader):\n",
    "                images, class_inds = images.to(self.device), class_inds.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                text_embeddings, image_embeddings = model(self.class_labels, images, pool=True)\n",
    "                loss = model.loss(text_embeddings, image_embeddings, class_inds)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_meter.update(loss.item(), len(class_inds))\n",
    "                print(f\"Batch {i+1}: loss {loss_meter.average()}\")\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Training loss {loss_meter.average()}\")\n",
    "            val_acc, val_auc, val_loss = self.run_eval(model, val_dataloader)\n",
    "            print(f\"Epoch {epoch+1}: Validation loss {val_loss} | Accuracy {val_acc} | AUC {val_auc}\")\n",
    "            \n",
    "            if best_acc is None or val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                self.best_model = copy.deepcopy(model)\n",
    "                best_epoch = epoch\n",
    "        print('Best epoch: ', best_epoch+1)\n",
    "\n",
    "    def run_eval(self, model, dataloader):\n",
    "        model.eval()\n",
    "        model = model.to(self.device)\n",
    "        \n",
    "        loss_meter = AverageMeter()\n",
    "        auc_meter = AverageMeter()\n",
    "        acc_meter = AverageMeter()\n",
    "        with torch.no_grad():\n",
    "            for images, class_inds in dataloader:\n",
    "                images, class_inds = images.to(self.device), class_inds.to(self.device)\n",
    "                text_embeddings, image_embeddings = model(self.class_labels, images, pool=True)\n",
    "\n",
    "                logits_per_text, logits_per_image = model.compute_logits(text_embeddings, image_embeddings)\n",
    "        \n",
    "                loss = model.contrastive_logit_loss(logits_per_text, logits_per_image, class_inds)\n",
    "                loss_meter.update(loss.item(), len(class_inds))\n",
    "\n",
    "                auc = calculate_auc(logits_per_image, class_inds)\n",
    "                auc_meter.update(auc, len(class_inds))\n",
    "                \n",
    "                acc = multilabel_accuracy(logits_per_image, class_inds)\n",
    "                acc_meter.update(acc, len(class_inds))\n",
    "        return acc_meter.average(), auc_meter.average(), loss_meter.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "from utils.device import get_device\n",
    "from models.embedding.dataset import Dataset\n",
    "\n",
    "def get_query_and_support_ids(img_info, split_file):\n",
    "    with open(split_file, 'rb') as fp:\n",
    "        cxr_train_query = pickle.load(fp)\n",
    "    query_image_ids = []\n",
    "    for ids in cxr_train_query.values():\n",
    "        query_image_ids.extend(ids)\n",
    "    support_image_ids = img_info[(img_info['meta_split'] == 'train') & ~img_info['image_id'].isin(query_image_ids)]['image_id'].to_list()\n",
    "    return query_image_ids, support_image_ids\n",
    "\n",
    "img_info = pd.read_pickle('data/vindr_cxr_split_labels.pkl')\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(img_info, 'data/vindr_train_query_set.pkl')\n",
    "# support_image_ids = img_info[(img_info['meta_split'] == 'train') & ~img_info['image_id'].isin(query_image_ids)]['image_id'].to_list()\n",
    "\n",
    "IMG_PATH = 'datasets/vindr-cxr-png'\n",
    "batch_size = 10*14\n",
    "query_dataset = Dataset(IMG_PATH, img_info, query_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "query_loader = DataLoader(dataset=query_dataset, batch_size=batch_size, shuffle=True)\n",
    "support_dataset = Dataset(IMG_PATH, img_info, support_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "support_loader = DataLoader(dataset=support_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "PROJ_SIZE = 512\n",
    "device = get_device()\n",
    "# backbone = resnet_backbone(load_pretrained_resnet(1, 14, 'models/backbone/pretrained/cxr_backbone_bal.pkl'))\n",
    "backbone = load_medclip_retrained_resnet('models/backbone/pretrained/medclip_resnet50.pkl')\n",
    "model = ImageTextEmbedding(backbone, PROJ_SIZE, device=device)\n",
    "# model.img_model.set_backbone_trainable(False)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "mtrainer = Trainer(model, support_dataset.class_labels(), device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.img_model.set_backbone_trainable(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 29.59238624572754\n",
      "Batch 2: loss 30.447845458984375\n",
      "Batch 3: loss 30.15230369567871\n",
      "Batch 4: loss 30.436224937438965\n",
      "Batch 5: loss 30.034178924560546\n",
      "Batch 6: loss 30.263222694396973\n",
      "Batch 7: loss 30.309550421578543\n",
      "Batch 8: loss 30.489644527435303\n",
      "Batch 9: loss 30.431108262803818\n",
      "Batch 10: loss 30.47095890045166\n",
      "Batch 11: loss 30.54041359641335\n",
      "Batch 12: loss 30.450597127278645\n",
      "Batch 13: loss 30.575787470890926\n",
      "Batch 14: loss 30.61762046813965\n",
      "Batch 15: loss 30.584911346435547\n",
      "Batch 16: loss 30.794479608535767\n",
      "Batch 17: loss 30.74137586705825\n",
      "Batch 18: loss 30.793212042914497\n",
      "Batch 19: loss 30.75236671849301\n",
      "Batch 20: loss 30.649649906158448\n",
      "Batch 21: loss 30.642054603213357\n",
      "Batch 22: loss 30.66928984902122\n",
      "Batch 23: loss 30.58221195055091\n",
      "Batch 24: loss 30.52550967534383\n",
      "Batch 25: loss 30.536274337768553\n",
      "Batch 26: loss 30.5052149112408\n",
      "Batch 27: loss 30.453746725011754\n",
      "Batch 28: loss 30.426480633871897\n",
      "Batch 29: loss 30.39460530774347\n",
      "Batch 30: loss 30.258544476826987\n",
      "Batch 31: loss 30.23040174668835\n",
      "Batch 32: loss 30.295980036258698\n",
      "Batch 33: loss 30.292732180971086\n",
      "Batch 34: loss 30.284801595351276\n",
      "Batch 35: loss 30.238455854143414\n",
      "Batch 36: loss 30.152061727311875\n",
      "Batch 37: loss 30.132911424379092\n",
      "Batch 38: loss 30.092320592779863\n",
      "Batch 39: loss 30.1579465621557\n",
      "Batch 40: loss 30.127256202697755\n",
      "Batch 41: loss 30.19531361649676\n",
      "Batch 42: loss 30.242367608206614\n",
      "Batch 43: loss 30.198792435402094\n",
      "Batch 44: loss 30.166151436892424\n",
      "Batch 45: loss 30.160627195570203\n",
      "Batch 46: loss 30.17089084956957\n",
      "Batch 47: loss 30.09471881135981\n",
      "Batch 48: loss 30.133195122083027\n",
      "Batch 49: loss 30.07163433152802\n",
      "Batch 50: loss 30.082334442138674\n",
      "Batch 51: loss 30.15714428471584\n",
      "Batch 52: loss 30.23597108400785\n",
      "Batch 53: loss 30.266361308547687\n",
      "Batch 54: loss 30.311633427937824\n",
      "Batch 55: loss 30.33466710177335\n",
      "Batch 56: loss 30.32653318132673\n",
      "Batch 57: loss 30.321883285254764\n",
      "Batch 58: loss 30.375950320013636\n",
      "Batch 59: loss 30.447955955893306\n",
      "Batch 60: loss 30.436240259806315\n",
      "Batch 61: loss 30.428299919503633\n",
      "Batch 62: loss 30.374035312283425\n",
      "Batch 63: loss 30.372622535342263\n",
      "Batch 64: loss 30.35865506529808\n",
      "Batch 65: loss 30.39174443758451\n",
      "Batch 66: loss 30.421058279095273\n",
      "Batch 67: loss 30.476998314928654\n",
      "Batch 68: loss 30.468936751870547\n",
      "Batch 69: loss 30.453922188800313\n",
      "Batch 70: loss 30.45179102761405\n",
      "Batch 71: loss 30.471146811901683\n",
      "Batch 72: loss 30.459203693601822\n",
      "Batch 73: loss 30.461582445118527\n",
      "Batch 74: loss 30.448498210391485\n",
      "Batch 75: loss 30.43227444966634\n",
      "Batch 76: loss 30.41144273155614\n",
      "Batch 77: loss 30.382928773954315\n",
      "Batch 78: loss 30.379016436063328\n",
      "Batch 79: loss 30.36807118186468\n",
      "Batch 80: loss 30.371588778495788\n",
      "Batch 81: loss 30.372769508832768\n",
      "Batch 82: loss 30.428794139769018\n",
      "Batch 83: loss 30.426403436316065\n",
      "Batch 84: loss 30.45374209540231\n",
      "Batch 85: loss 30.499505099128275\n",
      "Batch 86: loss 30.471172643262285\n",
      "Batch 87: loss 30.467800820010833\n",
      "Batch 88: loss 30.446986198425293\n",
      "Batch 89: loss 30.464139766907422\n",
      "Batch 90: loss 30.433132298787434\n",
      "Batch 91: loss 30.46391355074369\n",
      "Batch 92: loss 30.44695760892785\n",
      "Batch 93: loss 30.46647075940204\n"
     ]
    }
   ],
   "source": [
    "mtrainer.run_train(10, support_loader, query_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65.24489801951817, 0.7983050188223917, 130.08445739746094)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, query_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65.24489819662911, 0.7767362713978241, 136.79983215332032)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.model, query_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.model, 'imgtext_model_trained_proj.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
