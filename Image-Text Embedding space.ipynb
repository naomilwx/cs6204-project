{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def load_pretrained_resnet(img_channels, num_classes, save_path, fc_bias=True):\n",
    "    model = torchvision.models.resnet50(num_classes=num_classes, weights=None)\n",
    "    if fc_bias == False:\n",
    "        model.fc = nn.Linear(2048, num_classes, bias=False)\n",
    "    model.conv1 = torch.nn.Conv2d(img_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    return model\n",
    "\n",
    "def resnet_backbone(model):\n",
    "    return torch.nn.Sequential(*(list(model.children())[:-2]))\n",
    "\n",
    "def load_medclip_retrained_resnet(path):\n",
    "    return resnet_backbone(load_pretrained_resnet(1, 512, path, False))\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, backbone, embed_dims, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.proj = nn.Linear(2048, embed_dims)\n",
    "        if freeze_backbone:\n",
    "            self.set_backbone_trainable(False)\n",
    "    \n",
    "    def set_backbone_trainable(self, trainable):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = trainable\n",
    "\n",
    "    def forward(self, input):\n",
    "        # B, C, H, W\n",
    "        img = self.backbone(input)\n",
    "        # B, C, H, W -> B, H, W, C -> B, D, H, W\n",
    "        return self.proj(img.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dims, device='cpu', freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.backbone = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.proj = nn.Linear(768, embed_dims)\n",
    "        self.device = device\n",
    "        if freeze_backbone:\n",
    "            self.set_backbone_trainable(False)\n",
    "\n",
    "    def set_backbone_trainable(self, trainable):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = trainable\n",
    "    \n",
    "    def forward(self, input):\n",
    "        tokens = self.tokenizer(input, max_length=77, return_tensors='pt', padding='max_length').to(self.device)\n",
    "        out = self.backbone(**tokens)\n",
    "        enc = out['pooler_output']\n",
    "        # enc = out['last_hidden_state'][:, 0]\n",
    "        return self.proj(enc)\n",
    "\n",
    "class ImageTextEmbedding(nn.Module):\n",
    "    def __init__(self, img_backbone, embed_dims, logit_scale_init_value=0.1, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.text_model = TextEncoder(embed_dims, device)\n",
    "        self.img_model = ImageEncoder(img_backbone, embed_dims)\n",
    "        self.logit_scale = nn.Parameter(torch.log(torch.tensor(1/logit_scale_init_value)))\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def embed_text(self, text):\n",
    "        text_emb = self.text_model(text)\n",
    "        return text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    def embed_image(self, image, pool=False):\n",
    "        img_emb = self.img_model(image) # B, D, H, W\n",
    "        if pool:\n",
    "            img_emb = self.flatten(self.gap(img_emb)) # B, D\n",
    "        return img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    \n",
    "    def compute_logits(self, text_emb, img_emb):\n",
    "        self.logit_scale.data = torch.clamp(self.logit_scale.data, 0, 4.6052)\n",
    "        logit_scale = self.logit_scale.exp()        \n",
    "        if len(img_emb.shape) == 4:\n",
    "            logits_per_image = logit_scale * torch.matmul(img_emb.permute(2,3,0,1), text_emb.t())\n",
    "        else:\n",
    "            logits_per_image = logit_scale * torch.matmul(img_emb, text_emb.t())\n",
    "        \n",
    "        if len(img_emb.shape) == 4:\n",
    "            logits_per_text = logits_per_image.permute(0,1,3,2) # HxWxBxB\n",
    "        else:\n",
    "            logits_per_text = logits_per_image.t()\n",
    "        return logits_per_text, logits_per_image\n",
    "        \n",
    "    def forward(self, text, img, pool=False):\n",
    "        text_emb = self.embed_text(text)\n",
    "        img_emb = self.embed_image(img, pool)\n",
    "\n",
    "        return text_emb, img_emb\n",
    "    \n",
    "    def contrastive_logit_loss(self, logits_per_text, logits_per_image, labels):\n",
    "         # Image-label contrastive loss, which is similar to classification loss, except using the computed logits\n",
    "        itl = self.criterion(logits_per_image, labels)\n",
    "        til = self.criterion(logits_per_text, labels.t())\n",
    "        return (itl+til) / 2\n",
    "    \n",
    "    def loss(self, text_emb, img_emb, labels):\n",
    "        # text_embed should be an NxD matrix where N is the number of classes, so each row is the text embedding for the ith class\n",
    "        # image embed: BxD\n",
    "        # labels is an BxN indicator matrix with 1 for each class an image belongs to\n",
    "        logits_per_text, logits_per_image = self.compute_logits(text_emb, img_emb)\n",
    "        \n",
    "        return self.contrastive_logit_loss(logits_per_text, logits_per_image, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from utils.metrics import AverageMeter, calculate_auc, multilabel_accuracy\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, class_labels, device='cpu'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.class_labels = class_labels\n",
    "    \n",
    "    def run_train(self, epochs, dataloader, val_dataloader):\n",
    "        model = self.model.to(self.device)\n",
    "        best_epoch = None\n",
    "        best_acc = None\n",
    "        # optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            loss_meter = AverageMeter()\n",
    "            for i, (images, class_inds) in enumerate(dataloader):\n",
    "                images, class_inds = images.to(self.device), class_inds.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                text_embeddings, image_embeddings = model(self.class_labels, images, pool=True)\n",
    "                loss = model.loss(text_embeddings, image_embeddings, class_inds)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                loss_meter.update(loss.item(), len(class_inds))\n",
    "                print(f\"Batch {i+1}: loss {loss_meter.average()}\")\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Training loss {loss_meter.average()}\")\n",
    "            val_acc, val_auc, val_loss = self.run_eval(model, val_dataloader)\n",
    "            print(f\"Epoch {epoch+1}: Validation loss {val_loss} | Accuracy {val_acc} | AUC {val_auc}\")\n",
    "            \n",
    "            if best_acc is None or val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                self.best_model = copy.deepcopy(model)\n",
    "                best_epoch = epoch\n",
    "        print('Best epoch: ', best_epoch+1)\n",
    "\n",
    "    def run_eval(self, model, dataloader):\n",
    "        model.eval()\n",
    "        model = model.to(self.device)\n",
    "        \n",
    "        loss_meter = AverageMeter()\n",
    "        auc_meter = AverageMeter()\n",
    "        acc_meter = AverageMeter()\n",
    "        with torch.no_grad():\n",
    "            for images, class_inds in dataloader:\n",
    "                images, class_inds = images.to(self.device), class_inds.to(self.device)\n",
    "                text_embeddings, image_embeddings = model(self.class_labels, images, pool=True)\n",
    "\n",
    "                logits_per_text, logits_per_image = model.compute_logits(text_embeddings, image_embeddings)\n",
    "        \n",
    "                loss = model.contrastive_logit_loss(logits_per_text, logits_per_image, class_inds)\n",
    "                loss_meter.update(loss.item(), len(class_inds))\n",
    "\n",
    "                auc = calculate_auc(logits_per_image, class_inds)\n",
    "                auc_meter.update(auc, len(class_inds))\n",
    "                \n",
    "                acc = multilabel_accuracy(logits_per_image, class_inds)\n",
    "                acc_meter.update(acc, len(class_inds))\n",
    "        return acc_meter.average(), auc_meter.average(), loss_meter.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "from utils.device import get_device\n",
    "from models.embedding.dataset import Dataset\n",
    "\n",
    "# with open('data/vindr_train_query_set.pkl', 'rb') as fp:\n",
    "#     cxr_train_query = pickle.load(fp)\n",
    "\n",
    "# query_image_ids = []\n",
    "# for ids in cxr_train_query.values():\n",
    "#     query_image_ids.extend(ids)\n",
    "\n",
    "def get_query_and_support_ids(img_info, split_file):\n",
    "    with open(split_file, 'rb') as fp:\n",
    "        cxr_train_query = pickle.load(fp)\n",
    "    query_image_ids = []\n",
    "    for ids in cxr_train_query.values():\n",
    "        query_image_ids.extend(ids)\n",
    "    support_image_ids = img_info[(img_info['meta_split'] == 'train') & ~img_info['image_id'].isin(query_image_ids)]['image_id'].to_list()\n",
    "    return query_image_ids, support_image_ids\n",
    "\n",
    "img_info = pd.read_pickle('data/vindr_cxr_split_labels.pkl')\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(img_info, 'data/vindr_train_query_set.pkl')\n",
    "# support_image_ids = img_info[(img_info['meta_split'] == 'train') & ~img_info['image_id'].isin(query_image_ids)]['image_id'].to_list()\n",
    "\n",
    "IMG_PATH = 'datasets/vindr-cxr-png'\n",
    "batch_size = 10*14\n",
    "query_dataset = Dataset(IMG_PATH, img_info, query_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "query_loader = DataLoader(dataset=query_dataset, batch_size=batch_size, shuffle=True)\n",
    "support_dataset = Dataset(IMG_PATH, img_info, support_image_ids, VINDR_CXR_LABELS, VINDR_SPLIT['train'], mean_std=MEAN_STDS['chestmnist'])\n",
    "support_loader = DataLoader(dataset=support_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "PROJ_SIZE = 512\n",
    "device = get_device()\n",
    "# backbone = resnet_backbone(load_pretrained_resnet(1, 14, 'models/backbone/pretrained/cxr_backbone_bal.pkl'))\n",
    "backbone = load_medclip_retrained_resnet('models/backbone/pretrained/medclip_resnet50.pkl')\n",
    "model = ImageTextEmbedding(backbone, PROJ_SIZE, device=device)\n",
    "# model.img_model.set_backbone_trainable(False)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "mtrainer = Trainer(model, support_dataset.class_labels(), device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bb = torchvision.models.resnet50()\n",
    "bb.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model_baseline = ImageTextEmbedding(resnet_backbone(bb), PROJ_SIZE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65.24489822387696, 0.46013339092210115, 127.54764556884766)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.model, query_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 33.69576644897461\n",
      "Batch 2: loss 35.70352745056152\n",
      "Batch 3: loss 36.39114507039388\n",
      "Batch 4: loss 37.13524913787842\n",
      "Batch 5: loss 37.716374206542966\n",
      "Batch 6: loss 37.22264162699381\n",
      "Batch 7: loss 36.68990707397461\n",
      "Batch 8: loss 36.2630820274353\n",
      "Batch 9: loss 35.733789655897354\n",
      "Batch 10: loss 35.663933181762694\n",
      "Batch 11: loss 35.68782286210494\n",
      "Batch 12: loss 35.78476587931315\n",
      "Batch 13: loss 35.8788701570951\n",
      "Batch 14: loss 35.878697531563894\n",
      "Batch 15: loss 35.65770772298177\n",
      "Batch 16: loss 35.810307025909424\n",
      "Batch 17: loss 35.901333304012525\n",
      "Batch 18: loss 36.104226430257164\n",
      "Batch 19: loss 35.97945122969778\n",
      "Batch 20: loss 35.83519229888916\n",
      "Batch 21: loss 35.72511182512556\n",
      "Batch 22: loss 35.65146879716353\n",
      "Batch 23: loss 35.60612769748854\n",
      "Batch 24: loss 35.58307123184204\n",
      "Batch 25: loss 35.528944702148436\n",
      "Batch 26: loss 35.56490604694073\n",
      "Batch 27: loss 35.5359984503852\n",
      "Batch 28: loss 35.60614095415388\n",
      "Batch 29: loss 35.529578504891234\n",
      "Batch 30: loss 35.58520647684733\n",
      "Batch 31: loss 35.43843576985021\n",
      "Batch 32: loss 35.49918681383133\n",
      "Batch 33: loss 35.527032274188414\n",
      "Batch 34: loss 35.409088134765625\n",
      "Batch 35: loss 35.28324083600725\n",
      "Batch 36: loss 35.23486794365777\n",
      "Batch 37: loss 35.24905673877613\n",
      "Batch 38: loss 35.215861772236075\n",
      "Batch 39: loss 35.1839362902519\n",
      "Batch 40: loss 35.20705299377441\n",
      "Batch 41: loss 35.10209148686106\n",
      "Batch 42: loss 35.038130215236116\n",
      "Batch 43: loss 34.97690941566645\n",
      "Batch 44: loss 35.06771568818526\n",
      "Batch 45: loss 35.121047761705185\n",
      "Batch 46: loss 35.14209759753683\n",
      "Batch 47: loss 35.034737201447186\n",
      "Batch 48: loss 34.97111928462982\n",
      "Batch 49: loss 35.014954897822165\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/naomileow/Documents/school/CS6240/project/Image-Text Embedding space.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Image-Text%20Embedding%20space.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mtrainer\u001b[39m.\u001b[39;49mrun_train(\u001b[39m10\u001b[39;49m, support_loader, query_loader)\n",
      "\u001b[1;32m/Users/naomileow/Documents/school/CS6240/project/Image-Text Embedding space.ipynb Cell 6\u001b[0m in \u001b[0;36mTrainer.run_train\u001b[0;34m(self, epochs, dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Image-Text%20Embedding%20space.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Image-Text%20Embedding%20space.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss_meter \u001b[39m=\u001b[39m AverageMeter()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Image-Text%20Embedding%20space.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (images, class_inds) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Image-Text%20Embedding%20space.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     images, class_inds \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), class_inds\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/naomileow/Documents/school/CS6240/project/Image-Text%20Embedding%20space.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Documents/school/CS6240/project/models/embedding/dataset.py:40\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     38\u001b[0m class_inds \u001b[39m=\u001b[39m info[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses]\u001b[39m.\u001b[39mto_numpy()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     39\u001b[0m img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_root, info[\u001b[39m'\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 40\u001b[0m image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms(io\u001b[39m.\u001b[39;49mimread(img_path))\n\u001b[1;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m image, class_inds\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/skimage/io/_io.py:53\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     50\u001b[0m         plugin \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtifffile\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[39mwith\u001b[39;00m file_or_url_context(fname) \u001b[39mas\u001b[39;00m fname:\n\u001b[0;32m---> 53\u001b[0m     img \u001b[39m=\u001b[39m call_plugin(\u001b[39m'\u001b[39;49m\u001b[39mimread\u001b[39;49m\u001b[39m'\u001b[39;49m, fname, plugin\u001b[39m=\u001b[39;49mplugin, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mplugin_args)\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(img, \u001b[39m'\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/skimage/io/manage_plugins.py:207\u001b[0m, in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCould not find the plugin \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    205\u001b[0m                            (plugin, kind))\n\u001b[0;32m--> 207\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/skimage/io/_plugins/imageio_plugin.py:15\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@wraps\u001b[39m(imageio_imread)\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimread\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 15\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(imageio_imread(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/imageio/v2.py:227\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m imopen_args[\u001b[39m\"\u001b[39m\u001b[39mlegacy_mode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[39mwith\u001b[39;00m imopen(uri, \u001b[39m\"\u001b[39m\u001b[39mri\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mimopen_args) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m--> 227\u001b[0m     result \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39;49mread(index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    229\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/imageio/core/legacy_plugin_wrapper.py:148\u001b[0m, in \u001b[0;36mLegacyPlugin.read\u001b[0;34m(self, index, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n\u001b[1;32m    147\u001b[0m reader \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlegacy_get_reader(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m reader\u001b[39m.\u001b[39;49mget_data(index)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/imageio/core/format.py:435\u001b[0m, in \u001b[0;36mFormat.Reader.get_data\u001b[0;34m(self, index, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_BaseReaderWriter_last_index \u001b[39m=\u001b[39m index\n\u001b[1;32m    434\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     im, meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data(index, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    436\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(index)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/imageio/plugins/pillow_legacy.py:395\u001b[0m, in \u001b[0;36mPNGFormat.Reader._get_data\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_data\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m--> 395\u001b[0m     im, info \u001b[39m=\u001b[39m PillowFormat\u001b[39m.\u001b[39;49mReader\u001b[39m.\u001b[39;49m_get_data(\u001b[39mself\u001b[39;49m, index)\n\u001b[1;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mignoregamma\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    397\u001b[0m         \u001b[39m# The gamma value in the file represents the gamma factor for the\u001b[39;00m\n\u001b[1;32m    398\u001b[0m         \u001b[39m# hardware on the system where the file was created, and is meant\u001b[39;00m\n\u001b[1;32m    399\u001b[0m         \u001b[39m# to be able to match the colors with the system on which the\u001b[39;00m\n\u001b[1;32m    400\u001b[0m         \u001b[39m# image is shown. See also issue #366\u001b[39;00m\n\u001b[1;32m    401\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/imageio/plugins/pillow_legacy.py:343\u001b[0m, in \u001b[0;36mPillowFormat.Reader._get_data\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_im\u001b[39m.\u001b[39mpalette\u001b[39m.\u001b[39mrawmode_saved \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_im\u001b[39m.\u001b[39mpalette\u001b[39m.\u001b[39mrawmode\n\u001b[1;32m    342\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_im\u001b[39m.\u001b[39mgetdata()[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 343\u001b[0m im \u001b[39m=\u001b[39m pil_get_frame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_im, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_kwargs)\n\u001b[1;32m    344\u001b[0m \u001b[39mreturn\u001b[39;00m im, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_im\u001b[39m.\u001b[39minfo\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/imageio/plugins/pillow_legacy.py:781\u001b[0m, in \u001b[0;36mpil_get_frame\u001b[0;34m(im, is_gray, as_gray, mode, dtype)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mformat \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPNG\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m im\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m         dtype \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39muint16\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 781\u001b[0m     frame \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(frame, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    783\u001b[0m \u001b[39mreturn\u001b[39;00m frame\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/PIL/Image.py:687\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    685\u001b[0m     new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtobytes(\u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    686\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 687\u001b[0m     new[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtobytes()\n\u001b[1;32m    688\u001b[0m \u001b[39mreturn\u001b[39;00m new\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/PIL/Image.py:749\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[39mif\u001b[39;00m s \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    747\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mencoder error \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m in tobytes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 749\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mtrainer.run_train(10, support_loader, query_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, query_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
