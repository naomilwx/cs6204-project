{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medmnist\n",
    "from medmnist import INFO\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "# chestmnist, retinamnist\n",
    "def get_image_mean_std(dataname):\n",
    "    info = INFO[dataname]\n",
    "    DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((224, 224))\n",
    "            ])\n",
    "\n",
    "    train_dataset = DataClass(split='train', transform=transform, download=True)\n",
    "\n",
    "    train_loader = data.DataLoader(dataset=train_dataset, batch_size=8192)\n",
    "    total = info['n_samples']['train']\n",
    "    mean = torch.zeros(info['n_channels'])\n",
    "    std = torch.zeros(info['n_channels'])\n",
    "    for images, _ in train_loader:\n",
    "        num_img = len(images)\n",
    "        m, s = images.mean([0,2,3]), images.std([0,2,3])\n",
    "        mean += num_img * m / total\n",
    "        std += np.sqrt(num_img/total) * s\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/naomileow/.medmnist/pathmnist.npz\n",
      "pathmnist tensor([0.7405, 0.5330, 0.7058]) tensor([0.3920, 0.5636, 0.3959])\n",
      "Downloading https://zenodo.org/record/6496656/files/chestmnist.npz?download=1 to /Users/naomileow/.medmnist/chestmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a4e26d9d65463885431a1112f1c12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82802576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chestmnist tensor([0.4936]) tensor([0.7392])\n",
      "Downloading https://zenodo.org/record/6496656/files/dermamnist.npz?download=1 to /Users/naomileow/.medmnist/dermamnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a9f387e2264897bdff7ae73a39c3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19725078 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dermamnist tensor([0.7631, 0.5381, 0.5614]) tensor([0.1354, 0.1530, 0.1679])\n",
      "Downloading https://zenodo.org/record/6496656/files/octmnist.npz?download=1 to /Users/naomileow/.medmnist/octmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7097f8dabc6248908b4ff79621ca9174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54938180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "octmnist tensor([0.1889]) tensor([0.6606])\n",
      "Downloading https://zenodo.org/record/6496656/files/pneumoniamnist.npz?download=1 to /Users/naomileow/.medmnist/pneumoniamnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0ae89427524cf7806d3cbcd5289d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4170669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pneumoniamnist tensor([0.5719]) tensor([0.1651])\n",
      "Downloading https://zenodo.org/record/6496656/files/retinamnist.npz?download=1 to /Users/naomileow/.medmnist/retinamnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f60fc6b2d63460bb3a2196718b78112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3291041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retinamnist tensor([0.3984, 0.2447, 0.1558]) tensor([0.2952, 0.1970, 0.1470])\n",
      "Downloading https://zenodo.org/record/6496656/files/breastmnist.npz?download=1 to /Users/naomileow/.medmnist/breastmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e47eaa870494073b514256c59621848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/559580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breastmnist tensor([0.3276]) tensor([0.2027])\n",
      "Downloading https://zenodo.org/record/6496656/files/bloodmnist.npz?download=1 to /Users/naomileow/.medmnist/bloodmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b123ec2919ca4a54ba8645f6c203c062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35461855 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloodmnist tensor([0.7943, 0.6597, 0.6962]) tensor([0.2930, 0.3292, 0.1541])\n",
      "Downloading https://zenodo.org/record/6496656/files/tissuemnist.npz?download=1 to /Users/naomileow/.medmnist/tissuemnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f937abd55417400ca0f8ab5a41cc9cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124962739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tissuemnist tensor([0.1020]) tensor([0.4443])\n",
      "Downloading https://zenodo.org/record/6496656/files/organamnist.npz?download=1 to /Users/naomileow/.medmnist/organamnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd87d128acb4dabae5ec87edc43dc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38247903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organamnist tensor([0.4678]) tensor([0.6105])\n",
      "Downloading https://zenodo.org/record/6496656/files/organcmnist.npz?download=1 to /Users/naomileow/.medmnist/organcmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91efcfa71b4f495a9e83122e8df95ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15527535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organcmnist tensor([0.4932]) tensor([0.3762])\n",
      "Downloading https://zenodo.org/record/6496656/files/organsmnist.npz?download=1 to /Users/naomileow/.medmnist/organsmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a442275b7a495d81bd3d380182f0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16528536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organsmnist tensor([0.4950]) tensor([0.3779])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdecb9172087411d972fa4e036525a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32657407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_stds = {}\n",
    "\n",
    "for k in ['pathmnist', 'chestmnist', 'dermamnist', 'octmnist', 'pneumoniamnist', 'retinamnist', 'breastmnist', 'bloodmnist', 'tissuemnist', 'organamnist', 'organcmnist', 'organsmnist']:\n",
    "    mean, std = get_image_mean_std(k)\n",
    "    print(k, mean, std)\n",
    "    mean_stds[k] = {\n",
    "        'mean': mean,\n",
    "        'std': std\n",
    "    }\n",
    "print(mean_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.device import get_device\n",
    "from models.backbone.datasets import MEAN_STDS, DataSets\n",
    "from models.backbone.trainer import Trainer\n",
    "\n",
    "device = get_device()\n",
    "MODEL_SAVE_PATH = 'models/backbone/pretrained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/naomileow/.medmnist/retinamnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/retinamnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/retinamnist.npz\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torch import nn\n",
    "\n",
    "retina_ds = DataSets('retinamnist', MEAN_STDS) # 5 classes\n",
    "backbone = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "backbone.fc = nn.Linear(2048, len(retina_ds.info['label']))\n",
    "\n",
    "batch_size = 256\n",
    "rtrainer = Trainer(backbone, retina_ds, batch_size, device, balance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrainer.run_train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.52, 0.7557038049792416, 1.3845270824432374)\n"
     ]
    }
   ],
   "source": [
    "print(rtrainer.run_eval(rtrainer.best_model, rtrainer.test_loader)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# (0.5125, 0.7247683647010932, 1.464751205444336) for non pretrained, non balanced\n",
    "print(rtrainer.run_eval(rtrainer.best_model, rtrainer.test_loader)) \n",
    "\n",
    "torch.save(rtrainer.best_model.state_dict(), os.path.join(MODEL_SAVE_PATH, 'retina_backbone_pretrained_bal.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "# 'chestmnist', 'pneumoniamnist', 'octmnist',  'retinamnist'\n",
    "chest_ds = DataSets('chestmnist', MEAN_STDS)\n",
    "\n",
    "backbone = torchvision.models.resnet50(num_classes=len(chest_ds.info['label']), pretrained=False)\n",
    "# patch for single channel\n",
    "backbone.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "batch_size = 128\n",
    "ctrainer = Trainer(backbone, chest_ds, batch_size, device)\n",
    "\n",
    "ctrainer.run_train(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# [7996,  1950,  9261, 13914,  3988,  4375,   978,  3705,  3263, 1690,  1799,  1158,  2279,   144]\n",
    "chest_ds = DataSets('chestmnist', mean_stds)\n",
    "\n",
    "backbone = torchvision.models.resnet50(num_classes=len(chest_ds.info['label']), weights=None)\n",
    "backbone.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "backbone.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, 'cxr_backbone.pkl')))\n",
    "\n",
    "batch_size = 256\n",
    "ctrainer = Trainer(backbone, chest_ds, batch_size, device, balance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrainer.run_train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9239799784755875, 0.6329840270919405, 0.7261211995067128)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrainer.run_eval(ctrainer.model, ctrainer.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and save pretrained resnet from medclip\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from medclip import MedCLIPModel, MedCLIPVisionModel\n",
    "\n",
    "# load MedCLIP-ResNet50\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModel)\n",
    "model.from_pretrained()\n",
    "\n",
    "bconv_weight = model.vision_model.model.conv1.weight.mean(dim=1).unsqueeze(1)\n",
    "\n",
    "# The resnet model was trained on CheXpert and MIMIC-CXR\n",
    "backbone = model.vision_model.model\n",
    "backbone.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "backbone.conv1.weight = nn.Parameter(bconv_weight)\n",
    "\n",
    "torch.save(backbone.state_dict(), 'medclip_resnet50.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT, VINDR_SPLIT2\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "\n",
    "from utils.data import get_query_and_support_ids, DatasetConfig\n",
    "from utils.device import get_device\n",
    "from models.embedding.dataset import Dataset\n",
    "from utils.sampling import MultilabelBalancedRandomSampler\n",
    "\n",
    "configs = {\n",
    "    'vindr2': DatasetConfig('datasets/vindr-cxr-png', 'data/vindr_cxr_split_labels2.pkl', 'data/vindr_train_query_set2.pkl', VINDR_CXR_LABELS, VINDR_SPLIT2, MEAN_STDS['chestmnist'])\n",
    "}\n",
    "\n",
    "config = configs['vindr2']\n",
    "\n",
    "batch_size = 10*10\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(config.img_info, config.training_split_path)\n",
    "query_dataset = Dataset(config.img_path, config.img_info, query_image_ids, config.label_names_map, config.classes_split_map['train'], mean_std=config.mean_std)\n",
    "query_loader = DataLoader(dataset=query_dataset, batch_size=batch_size, shuffle=True)\n",
    "support_dataset = Dataset(config.img_path, config.img_info, support_image_ids, config.label_names_map, config.classes_split_map['train'], mean_std=config.mean_std)\n",
    "support_loader = DataLoader(dataset=support_dataset, batch_size=batch_size, sampler=MultilabelBalancedRandomSampler(support_dataset.get_class_indicators()))\n",
    "\n",
    "PROJ_SIZE = 512\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from models.backbone.trainer import DSTrainer\n",
    "from utils.f1_loss import BalAccuracyLoss, MCCLoss, F1Loss\n",
    "from models.embedding.model import load_medclip_retrained_resnet, load_pretrained_resnet\n",
    "\n",
    "# backbone = torchvision.models.resnet50(num_classes=len(config.classes_split_map['train']), weights=None)\n",
    "# backbone.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "backbone = load_pretrained_resnet(1, 512, 'models/backbone/pretrained/medclip_resnet50.pkl', False)\n",
    "backbone.fc = nn.Linear(2048, len(config.classes_split_map['train']), bias=False)\n",
    "\n",
    "mtrainer = DSTrainer(backbone, query_dataset.class_labels(), criterion=BalAccuracyLoss(), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 0.5119678974151611\n",
      "Batch 2: loss 0.5130165815353394\n",
      "Batch 3: loss 0.5083330869674683\n",
      "Batch 4: loss 0.5047162771224976\n",
      "Batch 5: loss 0.5002678632736206\n",
      "Batch 6: loss 0.4915280342102051\n",
      "Batch 7: loss 0.48292696475982666\n",
      "Batch 8: loss 0.4732884168624878\n",
      "Batch 9: loss 0.4661235809326172\n",
      "Batch 10: loss 0.4513899087905884\n",
      "Batch 11: loss 0.4456549286842346\n",
      "Batch 12: loss 0.4385819435119629\n",
      "Batch 13: loss 0.40876710414886475\n",
      "Batch 14: loss 0.41575318574905396\n",
      "Batch 15: loss 0.3964570164680481\n",
      "Batch 16: loss 0.3955634832382202\n",
      "Batch 17: loss 0.38344401121139526\n",
      "Batch 18: loss 0.3853582739830017\n",
      "Batch 19: loss 0.37931305170059204\n",
      "Batch 20: loss 0.3641212582588196\n",
      "Batch 21: loss 0.37644582986831665\n",
      "Batch 22: loss 0.36176663637161255\n",
      "Batch 23: loss 0.3737676739692688\n",
      "Batch 24: loss 0.3661550283432007\n",
      "Batch 25: loss 0.3574559688568115\n",
      "Batch 26: loss 0.3555760383605957\n",
      "Batch 27: loss 0.35296630859375\n",
      "Batch 28: loss 0.3469029664993286\n",
      "Batch 29: loss 0.3210749626159668\n",
      "Batch 30: loss 0.3270200490951538\n",
      "Batch 31: loss 0.31323325634002686\n",
      "Batch 32: loss 0.2776561379432678\n",
      "Batch 33: loss 0.2755095362663269\n",
      "Batch 34: loss 0.2628071904182434\n",
      "Batch 35: loss 0.24770516157150269\n",
      "Batch 36: loss 0.23648393154144287\n",
      "Batch 37: loss 0.23488539457321167\n",
      "Batch 38: loss 0.24094492197036743\n",
      "Batch 39: loss 0.25474345684051514\n",
      "Batch 40: loss 0.24012696743011475\n",
      "Batch 41: loss 0.2341938614845276\n",
      "Batch 42: loss 0.23943662643432617\n",
      "Batch 43: loss 0.22820860147476196\n",
      "Batch 44: loss 0.23696976900100708\n",
      "Batch 45: loss 0.2235502004623413\n",
      "Batch 46: loss 0.23577183485031128\n",
      "Batch 47: loss 0.2134609818458557\n",
      "Batch 48: loss 0.20219582319259644\n",
      "Batch 49: loss 0.20437514781951904\n",
      "Batch 50: loss 0.20174771547317505\n",
      "Batch 51: loss 0.20575976371765137\n",
      "Batch 52: loss 0.16640126705169678\n",
      "Batch 53: loss 0.16980457305908203\n",
      "Batch 54: loss 0.1803736686706543\n",
      "Batch 55: loss 0.17188423871994019\n",
      "Batch 56: loss 0.1688266396522522\n",
      "Batch 57: loss 0.17154008150100708\n",
      "Batch 58: loss 0.15762805938720703\n",
      "Batch 59: loss 0.15687376260757446\n",
      "Batch 60: loss 0.14493072032928467\n",
      "Batch 61: loss 0.15148907899856567\n",
      "Batch 62: loss 0.1701107621192932\n",
      "Batch 63: loss 0.17370635271072388\n",
      "Batch 64: loss 0.1450783610343933\n",
      "Batch 65: loss 0.14726150035858154\n",
      "Batch 66: loss 0.14244288206100464\n",
      "Batch 67: loss 0.1463523507118225\n",
      "Batch 68: loss 0.1541540026664734\n",
      "Batch 69: loss 0.16267889738082886\n",
      "Batch 70: loss 0.15736407041549683\n",
      "Batch 71: loss 0.13455504179000854\n",
      "Batch 72: loss 0.15836769342422485\n",
      "Batch 73: loss 0.12272363901138306\n",
      "Batch 74: loss 0.12145030498504639\n",
      "Batch 75: loss 0.10825282335281372\n",
      "Batch 76: loss 0.14676302671432495\n",
      "Batch 77: loss 0.13795173168182373\n",
      "Batch 78: loss 0.11356180906295776\n",
      "Batch 79: loss 0.12932240962982178\n",
      "Batch 80: loss 0.11950546503067017\n",
      "Batch 81: loss 0.14955759048461914\n",
      "Batch 82: loss 0.11997032165527344\n",
      "Batch 83: loss 0.11792117357254028\n",
      "Batch 84: loss 0.12202823162078857\n",
      "Batch 85: loss 0.11906272172927856\n",
      "Batch 86: loss 0.11221867799758911\n",
      "Batch 87: loss 0.10867547988891602\n",
      "Batch 88: loss 0.10795325040817261\n",
      "Batch 89: loss 0.10965096950531006\n",
      "Batch 90: loss 0.09400540590286255\n",
      "Batch 91: loss 0.10712796449661255\n",
      "Batch 92: loss 0.10513478517532349\n",
      "Batch 93: loss 0.09871375560760498\n",
      "Batch 94: loss 0.0989450216293335\n",
      "Batch 95: loss 0.10617238283157349\n",
      "Batch 96: loss 0.08277720212936401\n",
      "Batch 97: loss 0.09704607725143433\n",
      "Batch 98: loss 0.1226760745048523\n",
      "Batch 99: loss 0.09761476516723633\n",
      "Batch 100: loss 0.09075790643692017\n",
      "Batch 101: loss 0.10053062438964844\n",
      "Batch 102: loss 0.09310436248779297\n",
      "Batch 103: loss 0.10547786951065063\n",
      "Batch 104: loss 0.1041330099105835\n",
      "Batch 105: loss 0.09278476238250732\n",
      "Batch 106: loss 0.0876164436340332\n",
      "Batch 107: loss 0.09372586011886597\n",
      "Batch 108: loss 0.09066969156265259\n",
      "Batch 109: loss 0.07659357786178589\n",
      "Batch 110: loss 0.09257221221923828\n",
      "Batch 111: loss 0.09330624341964722\n",
      "Batch 112: loss 0.06746739149093628\n",
      "Batch 113: loss 0.09759408235549927\n",
      "Batch 114: loss 0.08987289667129517\n",
      "Batch 115: loss 0.07984954118728638\n",
      "Batch 116: loss 0.07587605714797974\n",
      "Batch 117: loss 0.08931571245193481\n",
      "Batch 118: loss 0.08898216485977173\n",
      "Batch 119: loss 0.07234209775924683\n",
      "Batch 120: loss 0.08314210176467896\n",
      "Batch 121: loss 0.09905850887298584\n",
      "Batch 122: loss 0.10432147979736328\n",
      "Batch 123: loss 0.0771753191947937\n",
      "Batch 124: loss 0.09351223707199097\n",
      "Batch 125: loss 0.05896252393722534\n",
      "Batch 126: loss 0.07762295007705688\n",
      "Batch 127: loss 0.07842081785202026\n",
      "Batch 128: loss 0.07001543045043945\n",
      "Batch 129: loss 0.057508766651153564\n",
      "Batch 130: loss 0.07291924953460693\n",
      "Batch 131: loss 0.07568031549453735\n",
      "Batch 132: loss 0.04504966735839844\n",
      "Batch 133: loss 0.06848210096359253\n",
      "Batch 134: loss 0.06271445751190186\n",
      "Batch 135: loss 0.05019080638885498\n",
      "Batch 136: loss 0.06630802154541016\n",
      "Batch 137: loss 0.0661611557006836\n",
      "Batch 138: loss 0.046791255474090576\n",
      "Batch 139: loss 0.06006079912185669\n",
      "Batch 140: loss 0.0630766749382019\n",
      "Batch 141: loss 0.0711597204208374\n",
      "Batch 142: loss 0.05385911464691162\n",
      "Batch 143: loss 0.06957894563674927\n",
      "Batch 144: loss 0.0706360936164856\n",
      "Batch 145: loss 0.08246475458145142\n",
      "Epoch 1: Training loss 0.18959424212460818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torch/functional.py:799: UserWarning: MPS: _unique2 op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performace implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Unique.mm:354.)\n",
      "  output, inverse_indices, counts = torch._unique2(\n",
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchmetrics/utilities/compute.py:52: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:218.)\n",
      "  denom[denom == 0.0] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation loss 0.34194380044937134 | F1 0.5965440750122071 | AUC 0.7955255076626198 | Acc H-Mean 0.6950270412651609\n",
      "Batch 1: loss 0.06137770414352417\n",
      "Batch 2: loss 0.07616311311721802\n",
      "Batch 3: loss 0.050753891468048096\n",
      "Batch 4: loss 0.059242069721221924\n",
      "Batch 5: loss 0.05802994966506958\n",
      "Batch 6: loss 0.06616359949111938\n",
      "Batch 7: loss 0.07716786861419678\n",
      "Batch 8: loss 0.05368441343307495\n",
      "Batch 9: loss 0.05364042520523071\n",
      "Batch 10: loss 0.060742199420928955\n",
      "Batch 11: loss 0.06535154581069946\n",
      "Batch 12: loss 0.05269354581832886\n",
      "Batch 13: loss 0.04498434066772461\n",
      "Batch 14: loss 0.074970543384552\n",
      "Batch 15: loss 0.062256693840026855\n",
      "Batch 16: loss 0.05268007516860962\n",
      "Batch 17: loss 0.06480079889297485\n",
      "Batch 18: loss 0.06918841600418091\n",
      "Batch 19: loss 0.055506765842437744\n",
      "Batch 20: loss 0.056970953941345215\n",
      "Batch 21: loss 0.04814988374710083\n",
      "Batch 22: loss 0.05216604471206665\n",
      "Batch 23: loss 0.04417693614959717\n",
      "Batch 24: loss 0.05861949920654297\n",
      "Batch 25: loss 0.05966430902481079\n",
      "Batch 26: loss 0.04516202211380005\n",
      "Batch 27: loss 0.041934311389923096\n",
      "Batch 28: loss 0.056013405323028564\n",
      "Batch 29: loss 0.04750633239746094\n",
      "Batch 30: loss 0.0576593279838562\n",
      "Batch 31: loss 0.046666502952575684\n",
      "Batch 32: loss 0.03775137662887573\n",
      "Batch 33: loss 0.04059332609176636\n",
      "Batch 34: loss 0.03951340913772583\n",
      "Batch 35: loss 0.03306370973587036\n",
      "Batch 36: loss 0.045195937156677246\n",
      "Batch 37: loss 0.04364520311355591\n",
      "Batch 38: loss 0.04462391138076782\n",
      "Batch 39: loss 0.043147265911102295\n",
      "Batch 40: loss 0.044514238834381104\n",
      "Batch 41: loss 0.045063018798828125\n",
      "Batch 42: loss 0.024513065814971924\n",
      "Batch 43: loss 0.042142271995544434\n",
      "Batch 44: loss 0.049679458141326904\n",
      "Batch 45: loss 0.04301244020462036\n",
      "Batch 46: loss 0.0340767502784729\n",
      "Batch 47: loss 0.032470524311065674\n",
      "Batch 48: loss 0.048011958599090576\n",
      "Batch 49: loss 0.03994178771972656\n",
      "Batch 50: loss 0.03223949670791626\n",
      "Batch 51: loss 0.04964572191238403\n",
      "Batch 52: loss 0.02907770872116089\n",
      "Batch 53: loss 0.04390525817871094\n",
      "Batch 54: loss 0.06439906358718872\n",
      "Batch 55: loss 0.04455679655075073\n",
      "Batch 56: loss 0.035319507122039795\n",
      "Batch 57: loss 0.041726648807525635\n",
      "Batch 58: loss 0.03739696741104126\n",
      "Batch 59: loss 0.03118044137954712\n",
      "Batch 60: loss 0.02948051691055298\n",
      "Batch 61: loss 0.04150944948196411\n",
      "Batch 62: loss 0.04927337169647217\n",
      "Batch 63: loss 0.031163573265075684\n",
      "Batch 64: loss 0.04067838191986084\n",
      "Batch 65: loss 0.037823498249053955\n",
      "Batch 66: loss 0.043143272399902344\n",
      "Batch 67: loss 0.045488834381103516\n",
      "Batch 68: loss 0.023694217205047607\n",
      "Batch 69: loss 0.02787017822265625\n",
      "Batch 70: loss 0.03140789270401001\n",
      "Batch 71: loss 0.03296983242034912\n",
      "Batch 72: loss 0.041065990924835205\n",
      "Batch 73: loss 0.031244099140167236\n",
      "Batch 74: loss 0.036894381046295166\n",
      "Batch 75: loss 0.03738212585449219\n",
      "Batch 76: loss 0.02818983793258667\n",
      "Batch 77: loss 0.034236013889312744\n",
      "Batch 78: loss 0.03420180082321167\n",
      "Batch 79: loss 0.02901238203048706\n",
      "Batch 80: loss 0.034692347049713135\n",
      "Batch 81: loss 0.02371901273727417\n",
      "Batch 82: loss 0.03952842950820923\n",
      "Batch 83: loss 0.029754638671875\n",
      "Batch 84: loss 0.03454303741455078\n",
      "Batch 85: loss 0.026170730590820312\n",
      "Batch 86: loss 0.01879364252090454\n",
      "Batch 87: loss 0.02517068386077881\n",
      "Batch 88: loss 0.01929861307144165\n",
      "Batch 89: loss 0.01929527521133423\n",
      "Batch 90: loss 0.02520895004272461\n",
      "Batch 91: loss 0.02045494318008423\n",
      "Batch 92: loss 0.028519630432128906\n",
      "Batch 93: loss 0.023616373538970947\n",
      "Batch 94: loss 0.02678781747817993\n",
      "Batch 95: loss 0.02893465757369995\n",
      "Batch 96: loss 0.025029540061950684\n",
      "Batch 97: loss 0.02300947904586792\n",
      "Batch 98: loss 0.024360358715057373\n",
      "Batch 99: loss 0.023929178714752197\n",
      "Batch 100: loss 0.026052653789520264\n",
      "Batch 101: loss 0.02738368511199951\n",
      "Batch 102: loss 0.05036896467208862\n",
      "Batch 103: loss 0.03803497552871704\n",
      "Batch 104: loss 0.04166680574417114\n",
      "Batch 105: loss 0.030895531177520752\n",
      "Batch 106: loss 0.017824947834014893\n",
      "Batch 107: loss 0.024963200092315674\n",
      "Batch 108: loss 0.03414887189865112\n",
      "Batch 109: loss 0.026622474193572998\n",
      "Batch 110: loss 0.019750773906707764\n",
      "Batch 111: loss 0.02739793062210083\n",
      "Batch 112: loss 0.02024775743484497\n",
      "Batch 113: loss 0.020500242710113525\n",
      "Batch 114: loss 0.03199213743209839\n",
      "Batch 115: loss 0.016407489776611328\n",
      "Batch 116: loss 0.033356666564941406\n",
      "Batch 117: loss 0.02296006679534912\n",
      "Batch 118: loss 0.017483294010162354\n",
      "Batch 119: loss 0.029113292694091797\n",
      "Batch 120: loss 0.036671221256256104\n",
      "Batch 121: loss 0.03500467538833618\n",
      "Batch 122: loss 0.020059943199157715\n",
      "Batch 123: loss 0.014042854309082031\n",
      "Batch 124: loss 0.031144320964813232\n",
      "Batch 125: loss 0.03214234113693237\n",
      "Batch 126: loss 0.027833163738250732\n",
      "Batch 127: loss 0.03001326322555542\n",
      "Batch 128: loss 0.02899777889251709\n",
      "Batch 129: loss 0.021705806255340576\n",
      "Batch 130: loss 0.025954723358154297\n",
      "Batch 131: loss 0.02247750759124756\n",
      "Batch 132: loss 0.01799541711807251\n",
      "Batch 133: loss 0.019486725330352783\n",
      "Batch 134: loss 0.012409389019012451\n",
      "Batch 135: loss 0.015391826629638672\n",
      "Batch 136: loss 0.015487253665924072\n",
      "Batch 137: loss 0.01869678497314453\n",
      "Batch 138: loss 0.02578216791152954\n",
      "Batch 139: loss 0.023305892944335938\n",
      "Batch 140: loss 0.028930723667144775\n",
      "Batch 141: loss 0.01714181900024414\n",
      "Batch 142: loss 0.028398990631103516\n",
      "Batch 143: loss 0.021137237548828125\n",
      "Batch 144: loss 0.012400031089782715\n",
      "Batch 145: loss 0.02121371030807495\n",
      "Epoch 2: Training loss 0.036582222345016666\n",
      "Epoch 2: Validation loss 0.3910371422767639 | F1 0.5567512750625611 | AUC 0.7884963793664028 | Acc H-Mean 0.6584938145363319\n",
      "Batch 1: loss 0.012977182865142822\n",
      "Batch 2: loss 0.0267065167427063\n",
      "Batch 3: loss 0.026409626007080078\n",
      "Batch 4: loss 0.023222625255584717\n",
      "Batch 5: loss 0.014118194580078125\n",
      "Batch 6: loss 0.019752681255340576\n",
      "Batch 7: loss 0.0243874192237854\n",
      "Batch 8: loss 0.014805853366851807\n",
      "Batch 9: loss 0.022496521472930908\n",
      "Batch 10: loss 0.023752570152282715\n",
      "Batch 11: loss 0.017080605030059814\n",
      "Batch 12: loss 0.02467590570449829\n",
      "Batch 13: loss 0.02396923303604126\n",
      "Batch 14: loss 0.02212655544281006\n",
      "Batch 15: loss 0.03542602062225342\n",
      "Batch 16: loss 0.02115422487258911\n",
      "Batch 17: loss 0.015996277332305908\n",
      "Batch 18: loss 0.029304862022399902\n",
      "Batch 19: loss 0.014156877994537354\n",
      "Batch 20: loss 0.011640369892120361\n",
      "Batch 21: loss 0.01772022247314453\n",
      "Batch 22: loss 0.017885208129882812\n",
      "Batch 23: loss 0.02359539270401001\n",
      "Batch 24: loss 0.023824691772460938\n",
      "Batch 25: loss 0.016785800457000732\n",
      "Batch 26: loss 0.038969576358795166\n",
      "Batch 27: loss 0.02044612169265747\n",
      "Batch 28: loss 0.02818208932876587\n",
      "Batch 29: loss 0.012765288352966309\n",
      "Batch 30: loss 0.018718302249908447\n",
      "Batch 31: loss 0.011094272136688232\n",
      "Batch 32: loss 0.011019527912139893\n",
      "Batch 33: loss 0.01664954423904419\n",
      "Batch 34: loss 0.02262979745864868\n",
      "Batch 35: loss 0.027037978172302246\n",
      "Batch 36: loss 0.020436763763427734\n",
      "Batch 37: loss 0.015491068363189697\n",
      "Batch 38: loss 0.018638014793395996\n",
      "Batch 39: loss 0.011539280414581299\n",
      "Batch 40: loss 0.024288177490234375\n",
      "Batch 41: loss 0.01695537567138672\n",
      "Batch 42: loss 0.009925425052642822\n",
      "Batch 43: loss 0.030615389347076416\n",
      "Batch 44: loss 0.01200324296951294\n",
      "Batch 45: loss 0.012873232364654541\n",
      "Batch 46: loss 0.01687479019165039\n",
      "Batch 47: loss 0.011216163635253906\n",
      "Batch 48: loss 0.012785255908966064\n",
      "Batch 49: loss 0.008789420127868652\n",
      "Batch 50: loss 0.01872241497039795\n",
      "Batch 51: loss 0.0091170072555542\n",
      "Batch 52: loss 0.01824474334716797\n",
      "Batch 53: loss 0.010042190551757812\n",
      "Batch 54: loss 0.011173784732818604\n",
      "Batch 55: loss 0.019137561321258545\n",
      "Batch 56: loss 0.01835763454437256\n",
      "Batch 57: loss 0.015355288982391357\n",
      "Batch 58: loss 0.017078399658203125\n",
      "Batch 59: loss 0.02044123411178589\n",
      "Batch 60: loss 0.01705312728881836\n",
      "Batch 61: loss 0.010792434215545654\n",
      "Batch 62: loss 0.019296348094940186\n",
      "Batch 63: loss 0.009370505809783936\n",
      "Batch 64: loss 0.01036292314529419\n",
      "Batch 65: loss 0.027396976947784424\n",
      "Batch 66: loss 0.019247591495513916\n",
      "Batch 67: loss 0.010668754577636719\n",
      "Batch 68: loss 0.018868982791900635\n",
      "Batch 69: loss 0.014000892639160156\n",
      "Batch 70: loss 0.016617119312286377\n",
      "Batch 71: loss 0.018327414989471436\n",
      "Batch 72: loss 0.03363138437271118\n",
      "Batch 73: loss 0.012577354907989502\n",
      "Batch 74: loss 0.009136736392974854\n",
      "Batch 75: loss 0.014726340770721436\n",
      "Batch 76: loss 0.01739102602005005\n",
      "Batch 77: loss 0.012246131896972656\n",
      "Batch 78: loss 0.01473778486251831\n",
      "Batch 79: loss 0.006768763065338135\n",
      "Batch 80: loss 0.01112222671508789\n",
      "Batch 81: loss 0.010498583316802979\n",
      "Batch 82: loss 0.013841032981872559\n",
      "Batch 83: loss 0.010781824588775635\n",
      "Batch 84: loss 0.01303088665008545\n",
      "Batch 85: loss 0.0063103437423706055\n",
      "Batch 86: loss 0.024947166442871094\n",
      "Batch 87: loss 0.007139384746551514\n",
      "Batch 88: loss 0.014512240886688232\n",
      "Batch 89: loss 0.014458239078521729\n",
      "Batch 90: loss 0.022196829319000244\n",
      "Batch 91: loss 0.019008219242095947\n",
      "Batch 92: loss 0.01463085412979126\n",
      "Batch 93: loss 0.008967220783233643\n",
      "Batch 94: loss 0.006323516368865967\n",
      "Batch 95: loss 0.00690847635269165\n",
      "Batch 96: loss 0.006199657917022705\n",
      "Batch 97: loss 0.013899683952331543\n",
      "Batch 98: loss 0.0077228546142578125\n",
      "Batch 99: loss 0.008938372135162354\n",
      "Batch 100: loss 0.007876455783843994\n",
      "Batch 101: loss 0.007883846759796143\n",
      "Batch 102: loss 0.01820218563079834\n",
      "Batch 103: loss 0.014018714427947998\n",
      "Batch 104: loss 0.009008228778839111\n",
      "Batch 105: loss 0.011551260948181152\n",
      "Batch 106: loss 0.008100330829620361\n",
      "Batch 107: loss 0.008880436420440674\n",
      "Batch 108: loss 0.009617030620574951\n",
      "Batch 109: loss 0.0140572190284729\n",
      "Batch 110: loss 0.01129072904586792\n",
      "Batch 111: loss 0.015106022357940674\n",
      "Batch 112: loss 0.012394130229949951\n",
      "Batch 113: loss 0.004607975482940674\n",
      "Batch 114: loss 0.01967722177505493\n",
      "Batch 115: loss 0.00545269250869751\n",
      "Batch 116: loss 0.005823194980621338\n",
      "Batch 117: loss 0.01386404037475586\n",
      "Batch 118: loss 0.009725570678710938\n",
      "Batch 119: loss 0.009754955768585205\n",
      "Batch 120: loss 0.027552425861358643\n",
      "Batch 121: loss 0.02078235149383545\n",
      "Batch 122: loss 0.013555407524108887\n",
      "Batch 123: loss 0.018648505210876465\n",
      "Batch 124: loss 0.012196123600006104\n",
      "Batch 125: loss 0.012739181518554688\n",
      "Batch 126: loss 0.013061344623565674\n",
      "Batch 127: loss 0.012183010578155518\n",
      "Batch 128: loss 0.012388229370117188\n",
      "Batch 129: loss 0.017011940479278564\n",
      "Batch 130: loss 0.00755995512008667\n",
      "Batch 131: loss 0.013498842716217041\n",
      "Batch 132: loss 0.004305720329284668\n",
      "Batch 133: loss 0.011341631412506104\n",
      "Batch 134: loss 0.0223236083984375\n",
      "Batch 135: loss 0.018214046955108643\n",
      "Batch 136: loss 0.01530998945236206\n",
      "Batch 137: loss 0.011406123638153076\n",
      "Batch 138: loss 0.010129928588867188\n",
      "Batch 139: loss 0.009724795818328857\n",
      "Batch 140: loss 0.012180864810943604\n",
      "Batch 141: loss 0.0087319016456604\n",
      "Batch 142: loss 0.009296000003814697\n",
      "Batch 143: loss 0.011202037334442139\n",
      "Batch 144: loss 0.005805373191833496\n",
      "Batch 145: loss 0.02126556634902954\n",
      "Epoch 3: Training loss 0.015413948319136138\n",
      "Epoch 3: Validation loss 0.3758175611495972 | F1 0.5789939999580384 | AUC 0.7959970890943784 | Acc H-Mean 0.6742253934283277\n",
      "Batch 1: loss 0.00864332914352417\n",
      "Batch 2: loss 0.008410453796386719\n",
      "Batch 3: loss 0.012668192386627197\n",
      "Batch 4: loss 0.026468932628631592\n",
      "Batch 5: loss 0.010611891746520996\n",
      "Batch 6: loss 0.017673969268798828\n",
      "Batch 7: loss 0.012307047843933105\n",
      "Batch 8: loss 0.015378832817077637\n",
      "Batch 9: loss 0.010284721851348877\n",
      "Batch 10: loss 0.007843554019927979\n",
      "Batch 11: loss 0.01407390832901001\n",
      "Batch 12: loss 0.009161770343780518\n",
      "Batch 13: loss 0.019811630249023438\n",
      "Batch 14: loss 0.009586870670318604\n",
      "Batch 15: loss 0.004871189594268799\n",
      "Batch 16: loss 0.01782369613647461\n",
      "Batch 17: loss 0.01123046875\n",
      "Batch 18: loss 0.010248005390167236\n",
      "Batch 19: loss 0.01322019100189209\n",
      "Batch 20: loss 0.017572224140167236\n",
      "Batch 21: loss 0.008303463459014893\n",
      "Batch 22: loss 0.00942307710647583\n",
      "Batch 23: loss 0.011004805564880371\n",
      "Batch 24: loss 0.01441127061843872\n",
      "Batch 25: loss 0.012873947620391846\n",
      "Batch 26: loss 0.005421698093414307\n",
      "Batch 27: loss 0.013083457946777344\n",
      "Batch 28: loss 0.022932708263397217\n",
      "Batch 29: loss 0.01425248384475708\n",
      "Batch 30: loss 0.00545579195022583\n",
      "Batch 31: loss 0.008729040622711182\n",
      "Batch 32: loss 0.004909217357635498\n",
      "Batch 33: loss 0.021570861339569092\n",
      "Batch 34: loss 0.003826916217803955\n",
      "Batch 35: loss 0.006812870502471924\n",
      "Batch 36: loss 0.012346625328063965\n",
      "Batch 37: loss 0.014647066593170166\n",
      "Batch 38: loss 0.016146838665008545\n",
      "Batch 39: loss 0.007874786853790283\n",
      "Batch 40: loss 0.0074062347412109375\n",
      "Batch 41: loss 0.012458384037017822\n",
      "Batch 42: loss 0.006251335144042969\n",
      "Batch 43: loss 0.00774306058883667\n",
      "Batch 44: loss 0.012225210666656494\n",
      "Batch 45: loss 0.0035367608070373535\n",
      "Batch 46: loss 0.010231971740722656\n",
      "Batch 47: loss 0.009798526763916016\n",
      "Batch 48: loss 0.009851932525634766\n",
      "Batch 49: loss 0.012153029441833496\n",
      "Batch 50: loss 0.008400440216064453\n",
      "Batch 51: loss 0.006632626056671143\n",
      "Batch 52: loss 0.015569508075714111\n",
      "Batch 53: loss 0.008701503276824951\n",
      "Batch 54: loss 0.005198061466217041\n",
      "Batch 55: loss 0.004862606525421143\n",
      "Batch 56: loss 0.008203685283660889\n",
      "Batch 57: loss 0.0037387609481811523\n",
      "Batch 58: loss 0.004602789878845215\n",
      "Batch 59: loss 0.012353897094726562\n",
      "Batch 60: loss 0.00995481014251709\n",
      "Batch 61: loss 0.0052915215492248535\n",
      "Batch 62: loss 0.008264243602752686\n",
      "Batch 63: loss 0.008485972881317139\n",
      "Batch 64: loss 0.008034706115722656\n",
      "Batch 65: loss 0.013236820697784424\n",
      "Batch 66: loss 0.01570308208465576\n",
      "Batch 67: loss 0.011660575866699219\n",
      "Batch 68: loss 0.0057201385498046875\n",
      "Batch 69: loss 0.01249837875366211\n",
      "Batch 70: loss 0.0037397146224975586\n",
      "Batch 71: loss 0.01177138090133667\n",
      "Batch 72: loss 0.01885354518890381\n",
      "Batch 73: loss 0.01758021116256714\n",
      "Batch 74: loss 0.00833052396774292\n",
      "Batch 75: loss 0.014586329460144043\n",
      "Batch 76: loss 0.0070648193359375\n",
      "Batch 77: loss 0.006679892539978027\n",
      "Batch 78: loss 0.012394487857818604\n",
      "Batch 79: loss 0.007228255271911621\n",
      "Batch 80: loss 0.017253100872039795\n",
      "Batch 81: loss 0.012929022312164307\n",
      "Batch 82: loss 0.007056772708892822\n",
      "Batch 83: loss 0.01106870174407959\n",
      "Batch 84: loss 0.007907092571258545\n",
      "Batch 85: loss 0.01053839921951294\n",
      "Batch 86: loss 0.008725523948669434\n",
      "Batch 87: loss 0.008524894714355469\n",
      "Batch 88: loss 0.013208091259002686\n",
      "Batch 89: loss 0.01354438066482544\n",
      "Batch 90: loss 0.009360134601593018\n",
      "Batch 91: loss 0.008380293846130371\n",
      "Batch 92: loss 0.005144953727722168\n",
      "Batch 93: loss 0.013251304626464844\n",
      "Batch 94: loss 0.013592422008514404\n",
      "Batch 95: loss 0.013969242572784424\n",
      "Batch 96: loss 0.00928419828414917\n",
      "Batch 97: loss 0.019968032836914062\n",
      "Batch 98: loss 0.013522505760192871\n",
      "Batch 99: loss 0.004911899566650391\n",
      "Batch 100: loss 0.008914172649383545\n",
      "Batch 101: loss 0.013325870037078857\n",
      "Batch 102: loss 0.005923926830291748\n",
      "Batch 103: loss 0.012622237205505371\n",
      "Batch 104: loss 0.02069157361984253\n",
      "Batch 105: loss 0.007770895957946777\n",
      "Batch 106: loss 0.020952224731445312\n",
      "Batch 107: loss 0.013017833232879639\n",
      "Batch 108: loss 0.005067229270935059\n",
      "Batch 109: loss 0.013632595539093018\n",
      "Batch 110: loss 0.01694965362548828\n",
      "Batch 111: loss 0.04502248764038086\n",
      "Batch 112: loss 0.004563510417938232\n",
      "Batch 113: loss 0.020978331565856934\n",
      "Batch 114: loss 0.014975368976593018\n",
      "Batch 115: loss 0.007923007011413574\n",
      "Batch 116: loss 0.009567737579345703\n",
      "Batch 117: loss 0.010099589824676514\n",
      "Batch 118: loss 0.007333576679229736\n",
      "Batch 119: loss 0.02307182550430298\n",
      "Batch 120: loss 0.02608698606491089\n",
      "Batch 121: loss 0.005719184875488281\n",
      "Batch 122: loss 0.008410751819610596\n",
      "Batch 123: loss 0.007963359355926514\n",
      "Batch 124: loss 0.005553722381591797\n",
      "Batch 125: loss 0.017311275005340576\n",
      "Batch 126: loss 0.006075859069824219\n",
      "Batch 127: loss 0.007994651794433594\n",
      "Batch 128: loss 0.0038385391235351562\n",
      "Batch 129: loss 0.007924437522888184\n",
      "Batch 130: loss 0.01671081781387329\n",
      "Batch 131: loss 0.013194441795349121\n",
      "Batch 132: loss 0.008526861667633057\n",
      "Batch 133: loss 0.008003592491149902\n",
      "Batch 134: loss 0.011495411396026611\n",
      "Batch 135: loss 0.007062733173370361\n",
      "Batch 136: loss 0.005317509174346924\n",
      "Batch 137: loss 0.00407487154006958\n",
      "Batch 138: loss 0.007550060749053955\n",
      "Batch 139: loss 0.011417865753173828\n",
      "Batch 140: loss 0.007895052433013916\n",
      "Batch 141: loss 0.015056610107421875\n",
      "Batch 142: loss 0.01436614990234375\n",
      "Batch 143: loss 0.014988899230957031\n",
      "Batch 144: loss 0.016353607177734375\n",
      "Batch 145: loss 0.004906833171844482\n",
      "Epoch 4: Training loss 0.011157412388648695\n",
      "Epoch 4: Validation loss 0.45555820465087893 | F1 0.48516238331794737 | AUC 0.7836909129228072 | Acc H-Mean 0.6070298205138019\n",
      "Batch 1: loss 0.017278194427490234\n",
      "Batch 2: loss 0.015163123607635498\n",
      "Batch 3: loss 0.011866390705108643\n",
      "Batch 4: loss 0.005841434001922607\n",
      "Batch 5: loss 0.006255149841308594\n",
      "Batch 6: loss 0.004639804363250732\n",
      "Batch 7: loss 0.005234062671661377\n",
      "Batch 8: loss 0.011879324913024902\n",
      "Batch 9: loss 0.010418474674224854\n",
      "Batch 10: loss 0.017081797122955322\n",
      "Batch 11: loss 0.01426476240158081\n",
      "Batch 12: loss 0.008368492126464844\n",
      "Batch 13: loss 0.0271875262260437\n",
      "Batch 14: loss 0.007640659809112549\n",
      "Batch 15: loss 0.006279170513153076\n",
      "Batch 16: loss 0.019279539585113525\n",
      "Batch 17: loss 0.00970369577407837\n",
      "Batch 18: loss 0.024409949779510498\n",
      "Batch 19: loss 0.0031037330627441406\n",
      "Batch 20: loss 0.009031593799591064\n",
      "Batch 21: loss 0.010620474815368652\n",
      "Batch 22: loss 0.010841548442840576\n",
      "Batch 23: loss 0.010342955589294434\n",
      "Batch 24: loss 0.0067043304443359375\n",
      "Batch 25: loss 0.006335794925689697\n",
      "Batch 26: loss 0.009056448936462402\n",
      "Batch 27: loss 0.0024440884590148926\n",
      "Batch 28: loss 0.004567921161651611\n",
      "Batch 29: loss 0.003745436668395996\n",
      "Batch 30: loss 0.007226169109344482\n",
      "Batch 31: loss 0.00509721040725708\n",
      "Batch 32: loss 0.012760162353515625\n",
      "Batch 33: loss 0.005025744438171387\n",
      "Batch 34: loss 0.006987273693084717\n",
      "Batch 35: loss 0.00560760498046875\n",
      "Batch 36: loss 0.009507238864898682\n",
      "Batch 37: loss 0.012225508689880371\n",
      "Batch 38: loss 0.004604339599609375\n",
      "Batch 39: loss 0.0022826790809631348\n",
      "Batch 40: loss 0.003986358642578125\n",
      "Batch 41: loss 0.009339690208435059\n",
      "Batch 42: loss 0.009670615196228027\n",
      "Batch 43: loss 0.002950727939605713\n",
      "Batch 44: loss 0.002156555652618408\n",
      "Batch 45: loss 0.004544556140899658\n",
      "Batch 46: loss 0.003481924533843994\n",
      "Batch 47: loss 0.008973300457000732\n",
      "Batch 48: loss 0.017985165119171143\n",
      "Batch 49: loss 0.004709064960479736\n",
      "Batch 50: loss 0.003998935222625732\n",
      "Batch 51: loss 0.013463020324707031\n",
      "Batch 52: loss 0.006491184234619141\n",
      "Batch 53: loss 0.008207976818084717\n",
      "Batch 54: loss 0.006504058837890625\n",
      "Batch 55: loss 0.017019569873809814\n",
      "Batch 56: loss 0.010009944438934326\n",
      "Batch 57: loss 0.00830298662185669\n",
      "Batch 58: loss 0.01329880952835083\n",
      "Batch 59: loss 0.006703376770019531\n",
      "Batch 60: loss 0.0067234039306640625\n",
      "Batch 61: loss 0.006963551044464111\n",
      "Batch 62: loss 0.006577670574188232\n",
      "Batch 63: loss 0.002154827117919922\n",
      "Batch 64: loss 0.012991607189178467\n",
      "Batch 65: loss 0.01178741455078125\n",
      "Batch 66: loss 0.004718601703643799\n",
      "Batch 67: loss 0.006310999393463135\n",
      "Batch 68: loss 0.010215938091278076\n",
      "Batch 69: loss 0.005239009857177734\n",
      "Batch 70: loss 0.015127360820770264\n",
      "Batch 71: loss 0.005309879779815674\n",
      "Batch 72: loss 0.00646740198135376\n",
      "Batch 73: loss 0.004769325256347656\n",
      "Batch 74: loss 0.004195988178253174\n",
      "Batch 75: loss 0.005307376384735107\n",
      "Batch 76: loss 0.00595170259475708\n",
      "Batch 77: loss 0.00282973051071167\n",
      "Batch 78: loss 0.0022963285446166992\n",
      "Batch 79: loss 0.009579658508300781\n",
      "Batch 80: loss 0.01328200101852417\n",
      "Batch 81: loss 0.01838397979736328\n",
      "Batch 82: loss 0.025946319103240967\n",
      "Batch 83: loss 0.009333133697509766\n",
      "Batch 84: loss 0.013803064823150635\n",
      "Batch 85: loss 0.01097249984741211\n",
      "Batch 86: loss 0.004762470722198486\n",
      "Batch 87: loss 0.00871974229812622\n",
      "Batch 88: loss 0.011670112609863281\n",
      "Batch 89: loss 0.005965590476989746\n",
      "Batch 90: loss 0.005053341388702393\n",
      "Batch 91: loss 0.002131640911102295\n",
      "Batch 92: loss 0.006423532962799072\n",
      "Batch 93: loss 0.00617218017578125\n",
      "Batch 94: loss 0.006120562553405762\n",
      "Batch 95: loss 0.01050424575805664\n",
      "Batch 96: loss 0.013543903827667236\n",
      "Batch 97: loss 0.010627329349517822\n",
      "Batch 98: loss 0.011376559734344482\n",
      "Batch 99: loss 0.007623553276062012\n",
      "Batch 100: loss 0.012169241905212402\n",
      "Batch 101: loss 0.006757795810699463\n",
      "Batch 102: loss 0.005679905414581299\n",
      "Batch 103: loss 0.01020956039428711\n",
      "Batch 104: loss 0.008873462677001953\n",
      "Batch 105: loss 0.005593836307525635\n",
      "Batch 106: loss 0.018746912479400635\n",
      "Batch 107: loss 0.0015308260917663574\n",
      "Batch 108: loss 0.0077571868896484375\n",
      "Batch 109: loss 0.009473681449890137\n",
      "Batch 110: loss 0.0033507943153381348\n",
      "Batch 111: loss 0.010313034057617188\n",
      "Batch 112: loss 0.005523085594177246\n",
      "Batch 113: loss 0.005640387535095215\n",
      "Batch 114: loss 0.018644511699676514\n",
      "Batch 115: loss 0.00423961877822876\n",
      "Batch 116: loss 0.00379866361618042\n",
      "Batch 117: loss 0.005605101585388184\n",
      "Batch 118: loss 0.007323324680328369\n",
      "Batch 119: loss 0.010687410831451416\n",
      "Batch 120: loss 0.008643507957458496\n",
      "Batch 121: loss 0.00363081693649292\n",
      "Batch 122: loss 0.0034692883491516113\n",
      "Batch 123: loss 0.007100105285644531\n",
      "Batch 124: loss 0.01658916473388672\n",
      "Batch 125: loss 0.009349226951599121\n",
      "Batch 126: loss 0.013223350048065186\n",
      "Batch 127: loss 0.013333141803741455\n",
      "Batch 128: loss 0.0157281756401062\n",
      "Batch 129: loss 0.005068778991699219\n",
      "Batch 130: loss 0.008065223693847656\n",
      "Batch 131: loss 0.00356215238571167\n",
      "Batch 132: loss 0.015162467956542969\n",
      "Batch 133: loss 0.012283802032470703\n",
      "Batch 134: loss 0.0036478042602539062\n",
      "Batch 135: loss 0.0054291486740112305\n",
      "Batch 136: loss 0.007904589176177979\n",
      "Batch 137: loss 0.007892012596130371\n",
      "Batch 138: loss 0.010239660739898682\n",
      "Batch 139: loss 0.009444057941436768\n",
      "Batch 140: loss 0.004954516887664795\n",
      "Batch 141: loss 0.0061196088790893555\n",
      "Batch 142: loss 0.00611114501953125\n",
      "Batch 143: loss 0.017663300037384033\n",
      "Batch 144: loss 0.01017606258392334\n",
      "Batch 145: loss 0.01184767484664917\n",
      "Epoch 5: Training loss 0.008751169744793915\n",
      "Epoch 5: Validation loss 0.4381293773651123 | F1 0.5213154315948486 | AUC 0.7982373851266413 | Acc H-Mean 0.637673705925917\n",
      "Batch 1: loss 0.0022829174995422363\n",
      "Batch 2: loss 0.004634737968444824\n",
      "Batch 3: loss 0.008990824222564697\n",
      "Batch 4: loss 0.008220493793487549\n",
      "Batch 5: loss 0.010389149188995361\n",
      "Batch 6: loss 0.0058789849281311035\n",
      "Batch 7: loss 0.009657442569732666\n",
      "Batch 8: loss 0.0024042129516601562\n",
      "Batch 9: loss 0.005249500274658203\n",
      "Batch 10: loss 0.01019275188446045\n",
      "Batch 11: loss 0.0016651153564453125\n",
      "Batch 12: loss 0.018186569213867188\n",
      "Batch 13: loss 0.0063800811767578125\n",
      "Batch 14: loss 0.004669070243835449\n",
      "Batch 15: loss 0.0039017796516418457\n",
      "Batch 16: loss 0.009254634380340576\n",
      "Batch 17: loss 0.009926855564117432\n",
      "Batch 18: loss 0.013410627841949463\n",
      "Batch 19: loss 0.008554160594940186\n",
      "Batch 20: loss 0.009454905986785889\n",
      "Batch 21: loss 0.012220382690429688\n",
      "Batch 22: loss 0.0016592741012573242\n",
      "Batch 23: loss 0.0036602020263671875\n",
      "Batch 24: loss 0.009860992431640625\n",
      "Batch 25: loss 0.010369479656219482\n",
      "Batch 26: loss 0.0016430020332336426\n",
      "Batch 27: loss 0.010882735252380371\n",
      "Batch 28: loss 0.009147942066192627\n",
      "Batch 29: loss 0.01108551025390625\n",
      "Batch 30: loss 0.006245613098144531\n",
      "Batch 31: loss 0.00439530611038208\n",
      "Batch 32: loss 0.006411552429199219\n",
      "Batch 33: loss 0.0016808509826660156\n",
      "Batch 34: loss 0.006244957447052002\n",
      "Batch 35: loss 0.006931960582733154\n",
      "Batch 36: loss 0.006212115287780762\n",
      "Batch 37: loss 0.0013772845268249512\n",
      "Batch 38: loss 0.006935298442840576\n",
      "Batch 39: loss 0.011520206928253174\n",
      "Batch 40: loss 0.0048528313636779785\n",
      "Batch 41: loss 0.0015290379524230957\n",
      "Batch 42: loss 0.006440460681915283\n",
      "Batch 43: loss 0.002723872661590576\n",
      "Batch 44: loss 0.010454177856445312\n",
      "Batch 45: loss 0.00526195764541626\n",
      "Batch 46: loss 0.0032405853271484375\n",
      "Batch 47: loss 0.0017933845520019531\n",
      "Batch 48: loss 0.012520313262939453\n",
      "Batch 49: loss 0.0025070905685424805\n",
      "Batch 50: loss 0.0060697197914123535\n",
      "Batch 51: loss 0.016087710857391357\n",
      "Batch 52: loss 0.009286701679229736\n",
      "Batch 53: loss 0.0039198994636535645\n",
      "Batch 54: loss 0.005589783191680908\n",
      "Batch 55: loss 0.0015890002250671387\n",
      "Batch 56: loss 0.0032064318656921387\n",
      "Batch 57: loss 0.004989326000213623\n",
      "Batch 58: loss 0.005545973777770996\n",
      "Batch 59: loss 0.007069408893585205\n",
      "Batch 60: loss 0.011265575885772705\n",
      "Batch 61: loss 0.00789409875869751\n",
      "Batch 62: loss 0.006638705730438232\n",
      "Batch 63: loss 0.008926928043365479\n",
      "Batch 64: loss 0.008493602275848389\n",
      "Batch 65: loss 0.01235276460647583\n",
      "Batch 66: loss 0.008717358112335205\n",
      "Batch 67: loss 0.011325240135192871\n",
      "Batch 68: loss 0.0029277801513671875\n",
      "Batch 69: loss 0.007297098636627197\n",
      "Batch 70: loss 0.004732906818389893\n",
      "Batch 71: loss 0.0016235113143920898\n",
      "Batch 72: loss 0.004895031452178955\n",
      "Batch 73: loss 0.005218327045440674\n",
      "Batch 74: loss 0.010795295238494873\n",
      "Batch 75: loss 0.0015106797218322754\n",
      "Batch 76: loss 0.0016425848007202148\n",
      "Batch 77: loss 0.006935596466064453\n",
      "Batch 78: loss 0.013559877872467041\n",
      "Batch 79: loss 0.010945141315460205\n",
      "Batch 80: loss 0.011508762836456299\n",
      "Batch 81: loss 0.012098491191864014\n",
      "Batch 82: loss 0.0018097758293151855\n",
      "Batch 83: loss 0.011437714099884033\n",
      "Batch 84: loss 0.0024848580360412598\n",
      "Batch 85: loss 0.009283840656280518\n",
      "Batch 86: loss 0.0032671093940734863\n",
      "Batch 87: loss 0.005632281303405762\n",
      "Batch 88: loss 0.0049138665199279785\n",
      "Batch 89: loss 0.0020691752433776855\n",
      "Batch 90: loss 0.008932948112487793\n",
      "Batch 91: loss 0.00393599271774292\n",
      "Batch 92: loss 0.020661652088165283\n",
      "Batch 93: loss 0.011986315250396729\n",
      "Batch 94: loss 0.0036380887031555176\n",
      "Batch 95: loss 0.013509273529052734\n",
      "Batch 96: loss 0.007389843463897705\n",
      "Batch 97: loss 0.008278071880340576\n",
      "Batch 98: loss 0.008355915546417236\n",
      "Batch 99: loss 0.010221302509307861\n",
      "Batch 100: loss 0.003918647766113281\n",
      "Batch 101: loss 0.012919425964355469\n",
      "Batch 102: loss 0.00447613000869751\n",
      "Batch 103: loss 0.01223534345626831\n",
      "Batch 104: loss 0.0014975666999816895\n",
      "Batch 105: loss 0.0016937851905822754\n",
      "Batch 106: loss 0.007002413272857666\n",
      "Batch 107: loss 0.004283905029296875\n",
      "Batch 108: loss 0.0013791918754577637\n",
      "Batch 109: loss 0.0013283491134643555\n",
      "Batch 110: loss 0.005089759826660156\n",
      "Batch 111: loss 0.0037443041801452637\n",
      "Batch 112: loss 0.006373941898345947\n",
      "Batch 113: loss 0.0016109347343444824\n",
      "Batch 114: loss 0.0032458901405334473\n",
      "Batch 115: loss 0.013175547122955322\n",
      "Batch 116: loss 0.008833229541778564\n",
      "Batch 117: loss 0.0034894943237304688\n",
      "Batch 118: loss 0.011470019817352295\n",
      "Batch 119: loss 0.003914237022399902\n",
      "Batch 120: loss 0.00880342721939087\n",
      "Batch 121: loss 0.004504561424255371\n",
      "Batch 122: loss 0.011803030967712402\n",
      "Batch 123: loss 0.002745211124420166\n",
      "Batch 124: loss 0.002899169921875\n",
      "Batch 125: loss 0.013409793376922607\n",
      "Batch 126: loss 0.0027486681938171387\n",
      "Batch 127: loss 0.004175007343292236\n",
      "Batch 128: loss 0.008684635162353516\n",
      "Batch 129: loss 0.008172810077667236\n",
      "Batch 130: loss 0.004468739032745361\n",
      "Batch 131: loss 0.008645415306091309\n",
      "Batch 132: loss 0.0061956048011779785\n",
      "Batch 133: loss 0.011274516582489014\n",
      "Batch 134: loss 0.009052276611328125\n",
      "Batch 135: loss 0.0037897229194641113\n",
      "Batch 136: loss 0.001738905906677246\n",
      "Batch 137: loss 0.009928703308105469\n",
      "Batch 138: loss 0.005315840244293213\n",
      "Batch 139: loss 0.002771914005279541\n",
      "Batch 140: loss 0.011091232299804688\n",
      "Batch 141: loss 0.001438438892364502\n",
      "Batch 142: loss 0.003839850425720215\n",
      "Batch 143: loss 0.006642162799835205\n",
      "Batch 144: loss 0.0013696551322937012\n",
      "Batch 145: loss 0.019408881664276123\n",
      "Epoch 6: Training loss 0.006834821785453042\n",
      "Epoch 6: Validation loss 0.470628035068512 | F1 0.48204272985458374 | AUC 0.7876830471337696 | Acc H-Mean 0.6074173441228737\n",
      "Batch 1: loss 0.0033845901489257812\n",
      "Batch 2: loss 0.003238677978515625\n",
      "Batch 3: loss 0.0028789639472961426\n",
      "Batch 4: loss 0.0009954571723937988\n",
      "Batch 5: loss 0.013341128826141357\n",
      "Batch 6: loss 0.0031218528747558594\n",
      "Batch 7: loss 0.0032347440719604492\n",
      "Batch 8: loss 0.00363236665725708\n",
      "Batch 9: loss 0.0073601603507995605\n",
      "Batch 10: loss 0.0029691457748413086\n",
      "Batch 11: loss 0.003717482089996338\n",
      "Batch 12: loss 0.00357741117477417\n",
      "Batch 13: loss 0.006692469120025635\n",
      "Batch 14: loss 0.003657698631286621\n",
      "Batch 15: loss 0.0034637451171875\n",
      "Batch 16: loss 0.005534708499908447\n",
      "Batch 17: loss 0.00472182035446167\n",
      "Batch 18: loss 0.0051784515380859375\n",
      "Batch 19: loss 0.005810141563415527\n",
      "Batch 20: loss 0.007275640964508057\n",
      "Batch 21: loss 0.01499176025390625\n",
      "Batch 22: loss 0.010099291801452637\n",
      "Batch 23: loss 0.007213413715362549\n",
      "Batch 24: loss 0.0098114013671875\n",
      "Batch 25: loss 0.0015420913696289062\n",
      "Batch 26: loss 0.010605692863464355\n",
      "Batch 27: loss 0.016456127166748047\n",
      "Batch 28: loss 0.006314277648925781\n",
      "Batch 29: loss 0.009181976318359375\n",
      "Batch 30: loss 0.007920145988464355\n",
      "Batch 31: loss 0.004340708255767822\n",
      "Batch 32: loss 0.0018717646598815918\n",
      "Batch 33: loss 0.006024539470672607\n",
      "Batch 34: loss 0.0050209760665893555\n",
      "Batch 35: loss 0.003712475299835205\n",
      "Batch 36: loss 0.005856692790985107\n",
      "Batch 37: loss 0.0010511279106140137\n",
      "Batch 38: loss 0.016293704509735107\n",
      "Batch 39: loss 0.004431605339050293\n",
      "Batch 40: loss 0.0056247711181640625\n",
      "Batch 41: loss 0.0030608773231506348\n",
      "Batch 42: loss 0.009227931499481201\n",
      "Batch 43: loss 0.005939662456512451\n",
      "Batch 44: loss 0.001362621784210205\n",
      "Batch 45: loss 0.0010784268379211426\n",
      "Batch 46: loss 0.00518423318862915\n",
      "Batch 47: loss 0.019629180431365967\n",
      "Batch 48: loss 0.0031214356422424316\n",
      "Batch 49: loss 0.009399771690368652\n",
      "Batch 50: loss 0.002746403217315674\n",
      "Batch 51: loss 0.0034807324409484863\n",
      "Batch 52: loss 0.003550708293914795\n",
      "Batch 53: loss 0.00601959228515625\n",
      "Batch 54: loss 0.004673004150390625\n",
      "Batch 55: loss 0.014677584171295166\n",
      "Batch 56: loss 0.005127310752868652\n",
      "Batch 57: loss 0.006764233112335205\n",
      "Batch 58: loss 0.010133802890777588\n",
      "Batch 59: loss 0.002842426300048828\n",
      "Batch 60: loss 0.01023179292678833\n",
      "Batch 61: loss 0.011283695697784424\n",
      "Batch 62: loss 0.0033624768257141113\n",
      "Batch 63: loss 0.007168173789978027\n",
      "Batch 64: loss 0.00446242094039917\n",
      "Batch 65: loss 0.011461913585662842\n",
      "Batch 66: loss 0.01644378900527954\n",
      "Batch 67: loss 0.015404284000396729\n",
      "Batch 68: loss 0.0031042098999023438\n",
      "Batch 69: loss 0.013151466846466064\n",
      "Batch 70: loss 0.010004997253417969\n",
      "Batch 71: loss 0.0059716105461120605\n",
      "Batch 72: loss 0.01431339979171753\n",
      "Batch 73: loss 0.0031006932258605957\n",
      "Batch 74: loss 0.013220131397247314\n",
      "Batch 75: loss 0.013244450092315674\n",
      "Batch 76: loss 0.000944674015045166\n",
      "Batch 77: loss 0.002925872802734375\n",
      "Batch 78: loss 0.009596705436706543\n",
      "Batch 79: loss 0.018073737621307373\n",
      "Batch 80: loss 0.0059743523597717285\n",
      "Batch 81: loss 0.003126204013824463\n",
      "Batch 82: loss 0.015033900737762451\n",
      "Batch 83: loss 0.0022878646850585938\n",
      "Batch 84: loss 0.014334022998809814\n",
      "Batch 85: loss 0.010708391666412354\n",
      "Batch 86: loss 0.004976451396942139\n",
      "Batch 87: loss 0.0026363134384155273\n",
      "Batch 88: loss 0.006743431091308594\n",
      "Batch 89: loss 0.008475661277770996\n",
      "Batch 90: loss 0.0065094828605651855\n",
      "Batch 91: loss 0.0034650564193725586\n",
      "Batch 92: loss 0.011388301849365234\n",
      "Batch 93: loss 0.0030518174171447754\n",
      "Batch 94: loss 0.004329383373260498\n",
      "Batch 95: loss 0.007517516613006592\n",
      "Batch 96: loss 0.008182883262634277\n",
      "Batch 97: loss 0.009678661823272705\n",
      "Batch 98: loss 0.0036674141883850098\n",
      "Batch 99: loss 0.004744529724121094\n",
      "Batch 100: loss 0.0011122822761535645\n",
      "Batch 101: loss 0.011269152164459229\n",
      "Batch 102: loss 0.008235156536102295\n",
      "Batch 103: loss 0.001186370849609375\n",
      "Batch 104: loss 0.005098879337310791\n",
      "Batch 105: loss 0.003981411457061768\n",
      "Batch 106: loss 0.006008505821228027\n",
      "Batch 107: loss 0.0027487874031066895\n",
      "Batch 108: loss 0.006537795066833496\n",
      "Batch 109: loss 0.0061397552490234375\n",
      "Batch 110: loss 0.0039007067680358887\n",
      "Batch 111: loss 0.007282257080078125\n",
      "Batch 112: loss 0.006068050861358643\n",
      "Batch 113: loss 0.008080482482910156\n",
      "Batch 114: loss 0.00549846887588501\n",
      "Batch 115: loss 0.008644580841064453\n",
      "Batch 116: loss 0.004446804523468018\n",
      "Batch 117: loss 0.0012602806091308594\n",
      "Batch 118: loss 0.006358981132507324\n",
      "Batch 119: loss 0.009383857250213623\n",
      "Batch 120: loss 0.008217990398406982\n",
      "Batch 121: loss 0.0029082298278808594\n",
      "Batch 122: loss 0.00928419828414917\n",
      "Batch 123: loss 0.0013183355331420898\n",
      "Batch 124: loss 0.006231367588043213\n",
      "Batch 125: loss 0.027852356433868408\n",
      "Batch 126: loss 0.0030118823051452637\n",
      "Batch 127: loss 0.00105208158493042\n",
      "Batch 128: loss 0.00658947229385376\n",
      "Batch 129: loss 0.005924999713897705\n",
      "Batch 130: loss 0.0019183754920959473\n",
      "Batch 131: loss 0.005874991416931152\n",
      "Batch 132: loss 0.00103759765625\n",
      "Batch 133: loss 0.00277787446975708\n",
      "Batch 134: loss 0.0013399124145507812\n",
      "Batch 135: loss 0.0012755990028381348\n",
      "Batch 136: loss 0.0032404661178588867\n",
      "Batch 137: loss 0.007051527500152588\n",
      "Batch 138: loss 0.009400725364685059\n",
      "Batch 139: loss 0.0051201581954956055\n",
      "Batch 140: loss 0.007289886474609375\n",
      "Batch 141: loss 0.008646190166473389\n",
      "Batch 142: loss 0.00553661584854126\n",
      "Batch 143: loss 0.008302092552185059\n",
      "Batch 144: loss 0.009292781352996826\n",
      "Batch 145: loss 0.0083656907081604\n",
      "Epoch 7: Training loss 0.006595985986177706\n",
      "Epoch 7: Validation loss 0.4840953230857849 | F1 0.4670467734336853 | AUC 0.7860067988065363 | Acc H-Mean 0.5915004250786526\n",
      "Batch 1: loss 0.007480442523956299\n",
      "Batch 2: loss 0.008152365684509277\n",
      "Batch 3: loss 0.011179327964782715\n",
      "Batch 4: loss 0.006860554218292236\n",
      "Batch 5: loss 0.010720789432525635\n",
      "Batch 6: loss 0.008321166038513184\n",
      "Batch 7: loss 0.004149973392486572\n",
      "Batch 8: loss 0.012733817100524902\n",
      "Batch 9: loss 0.01064145565032959\n",
      "Batch 10: loss 0.000835120677947998\n",
      "Batch 11: loss 0.008601367473602295\n",
      "Batch 12: loss 0.013783514499664307\n",
      "Batch 13: loss 0.009595930576324463\n",
      "Batch 14: loss 0.004741013050079346\n",
      "Batch 15: loss 0.0012068748474121094\n",
      "Batch 16: loss 0.008245468139648438\n",
      "Batch 17: loss 0.012178599834442139\n",
      "Batch 18: loss 0.00554734468460083\n",
      "Batch 19: loss 0.0036191940307617188\n",
      "Batch 20: loss 0.006686091423034668\n",
      "Batch 21: loss 0.004168689250946045\n",
      "Batch 22: loss 0.006465315818786621\n",
      "Batch 23: loss 0.006696879863739014\n",
      "Batch 24: loss 0.008818984031677246\n",
      "Batch 25: loss 0.015490531921386719\n",
      "Batch 26: loss 0.007573127746582031\n",
      "Batch 27: loss 0.0033183693885803223\n",
      "Batch 28: loss 0.005949079990386963\n",
      "Batch 29: loss 0.01610642671585083\n",
      "Batch 30: loss 0.012942671775817871\n",
      "Batch 31: loss 0.00685727596282959\n",
      "Batch 32: loss 0.007789015769958496\n",
      "Batch 33: loss 0.0027900338172912598\n",
      "Batch 34: loss 0.0015602707862854004\n",
      "Batch 35: loss 0.010770976543426514\n",
      "Batch 36: loss 0.00907886028289795\n",
      "Batch 37: loss 0.011217653751373291\n",
      "Batch 38: loss 0.006566405296325684\n",
      "Batch 39: loss 0.011050403118133545\n",
      "Batch 40: loss 0.005177021026611328\n",
      "Batch 41: loss 0.007991373538970947\n",
      "Batch 42: loss 0.007716715335845947\n",
      "Batch 43: loss 0.003981411457061768\n",
      "Batch 44: loss 0.012822329998016357\n",
      "Batch 45: loss 0.005907595157623291\n",
      "Batch 46: loss 0.005935370922088623\n",
      "Batch 47: loss 0.005444824695587158\n",
      "Batch 48: loss 0.0020459890365600586\n",
      "Batch 49: loss 0.01810365915298462\n",
      "Batch 50: loss 0.0011606216430664062\n",
      "Batch 51: loss 0.010895133018493652\n",
      "Batch 52: loss 0.0038361549377441406\n",
      "Batch 53: loss 0.002796649932861328\n",
      "Batch 54: loss 0.015259623527526855\n",
      "Batch 55: loss 0.008509933948516846\n",
      "Batch 56: loss 0.013499081134796143\n",
      "Batch 57: loss 0.003603041172027588\n",
      "Batch 58: loss 0.0016597509384155273\n",
      "Batch 59: loss 0.005863845348358154\n",
      "Batch 60: loss 0.010595202445983887\n",
      "Batch 61: loss 0.002795279026031494\n",
      "Batch 62: loss 0.006144881248474121\n",
      "Batch 63: loss 0.0024366378784179688\n",
      "Batch 64: loss 0.012323558330535889\n",
      "Batch 65: loss 0.009401500225067139\n",
      "Batch 66: loss 0.006462454795837402\n",
      "Batch 67: loss 0.004715621471405029\n",
      "Batch 68: loss 0.008462905883789062\n",
      "Batch 69: loss 0.0015956759452819824\n",
      "Batch 70: loss 0.00429534912109375\n",
      "Batch 71: loss 0.008545219898223877\n",
      "Batch 72: loss 0.003246128559112549\n",
      "Batch 73: loss 0.005847156047821045\n",
      "Batch 74: loss 0.002758204936981201\n",
      "Batch 75: loss 0.00379180908203125\n",
      "Batch 76: loss 0.005795300006866455\n",
      "Batch 77: loss 0.0025537610054016113\n",
      "Batch 78: loss 0.003199279308319092\n",
      "Batch 79: loss 0.005397617816925049\n",
      "Batch 80: loss 0.0038440823554992676\n",
      "Batch 81: loss 0.006604671478271484\n",
      "Batch 82: loss 0.005843698978424072\n",
      "Batch 83: loss 0.005472004413604736\n",
      "Batch 84: loss 0.016848385334014893\n",
      "Batch 85: loss 0.0014055371284484863\n",
      "Batch 86: loss 0.0025493502616882324\n",
      "Batch 87: loss 0.010293304920196533\n",
      "Batch 88: loss 0.0038042068481445312\n",
      "Batch 89: loss 0.005248844623565674\n",
      "Batch 90: loss 0.005884706974029541\n",
      "Batch 91: loss 0.009448349475860596\n",
      "Batch 92: loss 0.007045567035675049\n",
      "Batch 93: loss 0.0010526776313781738\n",
      "Batch 94: loss 0.009170711040496826\n",
      "Batch 95: loss 0.009960949420928955\n",
      "Batch 96: loss 0.011624038219451904\n",
      "Batch 97: loss 0.00396353006362915\n",
      "Batch 98: loss 0.005184173583984375\n",
      "Batch 99: loss 0.0093231201171875\n",
      "Batch 100: loss 0.004429161548614502\n",
      "Batch 101: loss 0.0041318535804748535\n",
      "Batch 102: loss 0.006146609783172607\n",
      "Batch 103: loss 0.002056121826171875\n",
      "Batch 104: loss 0.00639265775680542\n",
      "Batch 105: loss 0.0011873245239257812\n",
      "Batch 106: loss 0.006608009338378906\n",
      "Batch 107: loss 0.014466285705566406\n",
      "Batch 108: loss 0.010832667350769043\n",
      "Batch 109: loss 0.003456413745880127\n",
      "Batch 110: loss 0.013921141624450684\n",
      "Batch 111: loss 0.00745469331741333\n",
      "Batch 112: loss 0.00266873836517334\n",
      "Batch 113: loss 0.00799560546875\n",
      "Batch 114: loss 0.017390429973602295\n",
      "Batch 115: loss 0.003478407859802246\n",
      "Batch 116: loss 0.007179439067840576\n",
      "Batch 117: loss 0.004650115966796875\n",
      "Batch 118: loss 0.006509304046630859\n",
      "Batch 119: loss 0.00311279296875\n",
      "Batch 120: loss 0.00925356149673462\n",
      "Batch 121: loss 0.011376559734344482\n",
      "Batch 122: loss 0.002871394157409668\n",
      "Batch 123: loss 0.013839244842529297\n",
      "Batch 124: loss 0.009171009063720703\n",
      "Batch 125: loss 0.007318675518035889\n",
      "Batch 126: loss 0.004958510398864746\n",
      "Batch 127: loss 0.00339430570602417\n",
      "Batch 128: loss 0.0010055303573608398\n",
      "Batch 129: loss 0.007292747497558594\n",
      "Batch 130: loss 0.007585108280181885\n",
      "Batch 131: loss 0.0032783150672912598\n",
      "Batch 132: loss 0.005442440509796143\n",
      "Batch 133: loss 0.000838935375213623\n",
      "Batch 134: loss 0.002860426902770996\n",
      "Batch 135: loss 0.013990581035614014\n",
      "Batch 136: loss 0.005646884441375732\n",
      "Batch 137: loss 0.005354464054107666\n",
      "Batch 138: loss 0.009946048259735107\n",
      "Batch 139: loss 0.0011457204818725586\n",
      "Batch 140: loss 0.007713675498962402\n",
      "Batch 141: loss 0.00836104154586792\n",
      "Batch 142: loss 0.005340874195098877\n",
      "Batch 143: loss 0.0008855462074279785\n",
      "Batch 144: loss 0.0020998716354370117\n",
      "Batch 145: loss 0.0038447976112365723\n",
      "Epoch 8: Training loss 0.006805430776657801\n",
      "Epoch 8: Validation loss 0.5287832498550415 | F1 0.4269185245037079 | AUC 0.7732435545857227 | Acc H-Mean 0.555334676026433\n",
      "Batch 1: loss 0.004386723041534424\n",
      "Batch 2: loss 0.005369246006011963\n",
      "Batch 3: loss 0.00689774751663208\n",
      "Batch 4: loss 0.0029171109199523926\n",
      "Batch 5: loss 0.008860945701599121\n",
      "Batch 6: loss 0.012041628360748291\n",
      "Batch 7: loss 0.003158390522003174\n",
      "Batch 8: loss 0.0010905861854553223\n",
      "Batch 9: loss 0.0027589797973632812\n",
      "Batch 10: loss 0.005323231220245361\n",
      "Batch 11: loss 0.0006868243217468262\n",
      "Batch 12: loss 0.0006349682807922363\n",
      "Batch 13: loss 0.008223891258239746\n",
      "Batch 14: loss 0.0080452561378479\n",
      "Batch 15: loss 0.007410407066345215\n",
      "Batch 16: loss 0.005052626132965088\n",
      "Batch 17: loss 0.0059975385665893555\n",
      "Batch 18: loss 0.010145068168640137\n",
      "Batch 19: loss 0.0057569146156311035\n",
      "Batch 20: loss 0.013901293277740479\n",
      "Batch 21: loss 0.009945273399353027\n",
      "Batch 22: loss 0.011340498924255371\n",
      "Batch 23: loss 0.009163916110992432\n",
      "Batch 24: loss 0.006511509418487549\n",
      "Batch 25: loss 0.004330456256866455\n",
      "Batch 26: loss 0.0064438581466674805\n",
      "Batch 27: loss 0.004999637603759766\n",
      "Batch 28: loss 0.0016455650329589844\n",
      "Batch 29: loss 0.0010808706283569336\n",
      "Batch 30: loss 0.005292713642120361\n",
      "Batch 31: loss 0.0037056803703308105\n",
      "Batch 32: loss 0.004599571228027344\n",
      "Batch 33: loss 0.010339319705963135\n",
      "Batch 34: loss 0.00813215970993042\n",
      "Batch 35: loss 0.003488898277282715\n",
      "Batch 36: loss 0.0077024102210998535\n",
      "Batch 37: loss 0.0018094182014465332\n",
      "Batch 38: loss 0.012760162353515625\n",
      "Batch 39: loss 0.003989219665527344\n",
      "Batch 40: loss 0.004819750785827637\n",
      "Batch 41: loss 0.0031645894050598145\n",
      "Batch 42: loss 0.010860979557037354\n",
      "Batch 43: loss 0.010181605815887451\n",
      "Batch 44: loss 0.004036307334899902\n",
      "Batch 45: loss 0.006492972373962402\n",
      "Batch 46: loss 0.005577445030212402\n",
      "Batch 47: loss 0.0030954480171203613\n",
      "Batch 48: loss 0.013361036777496338\n",
      "Batch 49: loss 0.005569636821746826\n",
      "Batch 50: loss 0.004310011863708496\n",
      "Batch 51: loss 0.0009654760360717773\n",
      "Batch 52: loss 0.010218977928161621\n",
      "Batch 53: loss 0.003874659538269043\n",
      "Batch 54: loss 0.002833724021911621\n",
      "Batch 55: loss 0.008048415184020996\n",
      "Batch 56: loss 0.0029630661010742188\n",
      "Batch 57: loss 0.010007262229919434\n",
      "Batch 58: loss 0.008317172527313232\n",
      "Batch 59: loss 0.010865092277526855\n",
      "Batch 60: loss 0.01191025972366333\n",
      "Batch 61: loss 0.00310516357421875\n",
      "Batch 62: loss 0.009451091289520264\n",
      "Batch 63: loss 0.004901468753814697\n",
      "Batch 64: loss 0.011085987091064453\n",
      "Batch 65: loss 0.009752631187438965\n",
      "Batch 66: loss 0.007968723773956299\n",
      "Batch 67: loss 0.013140678405761719\n",
      "Batch 68: loss 0.0032854080200195312\n",
      "Batch 69: loss 0.006733715534210205\n",
      "Batch 70: loss 0.011990547180175781\n",
      "Batch 71: loss 0.009256184101104736\n",
      "Batch 72: loss 0.005099475383758545\n",
      "Batch 73: loss 0.001192629337310791\n",
      "Batch 74: loss 0.007289707660675049\n",
      "Batch 75: loss 0.0032212138175964355\n",
      "Batch 76: loss 0.00729900598526001\n",
      "Batch 77: loss 0.0029582977294921875\n",
      "Batch 78: loss 0.012853026390075684\n",
      "Batch 79: loss 0.00519329309463501\n",
      "Batch 80: loss 0.012089729309082031\n",
      "Batch 81: loss 0.005756735801696777\n",
      "Batch 82: loss 0.0007537603378295898\n",
      "Batch 83: loss 0.005404651165008545\n",
      "Batch 84: loss 0.004611313343048096\n",
      "Batch 85: loss 0.007641792297363281\n",
      "Batch 86: loss 0.01291590929031372\n",
      "Batch 87: loss 0.012739956378936768\n",
      "Batch 88: loss 0.003412306308746338\n",
      "Batch 89: loss 0.006522953510284424\n",
      "Batch 90: loss 0.012139379978179932\n",
      "Batch 91: loss 0.008485674858093262\n",
      "Batch 92: loss 0.009266674518585205\n",
      "Batch 93: loss 0.005179703235626221\n",
      "Batch 94: loss 0.009927749633789062\n",
      "Batch 95: loss 0.0022876858711242676\n",
      "Batch 96: loss 0.007328689098358154\n",
      "Batch 97: loss 0.0034236907958984375\n",
      "Batch 98: loss 0.010172843933105469\n",
      "Batch 99: loss 0.0011653900146484375\n",
      "Batch 100: loss 0.0007380247116088867\n",
      "Batch 101: loss 0.0011619329452514648\n",
      "Batch 102: loss 0.0035971403121948242\n",
      "Batch 103: loss 0.009057998657226562\n",
      "Batch 104: loss 0.005581080913543701\n",
      "Batch 105: loss 0.005809366703033447\n",
      "Batch 106: loss 0.00117570161819458\n",
      "Batch 107: loss 0.005003452301025391\n",
      "Batch 108: loss 0.00263059139251709\n",
      "Batch 109: loss 0.0064896345138549805\n",
      "Batch 110: loss 0.007133841514587402\n",
      "Batch 111: loss 0.003280162811279297\n",
      "Batch 112: loss 0.0059490203857421875\n",
      "Batch 113: loss 0.0026717782020568848\n",
      "Batch 114: loss 0.006577849388122559\n",
      "Batch 115: loss 0.012109756469726562\n",
      "Batch 116: loss 0.008404791355133057\n",
      "Batch 117: loss 0.004786312580108643\n",
      "Batch 118: loss 0.004833400249481201\n",
      "Batch 119: loss 0.0006868243217468262\n",
      "Batch 120: loss 0.005035221576690674\n",
      "Batch 121: loss 0.005160868167877197\n",
      "Batch 122: loss 0.01215207576751709\n",
      "Batch 123: loss 0.0015516281127929688\n",
      "Batch 124: loss 0.0033963918685913086\n",
      "Batch 125: loss 0.0030791163444519043\n",
      "Batch 126: loss 0.012376487255096436\n",
      "Batch 127: loss 0.0042615532875061035\n",
      "Batch 128: loss 0.004404544830322266\n",
      "Batch 129: loss 0.0015959739685058594\n",
      "Batch 130: loss 0.007707417011260986\n",
      "Batch 131: loss 0.0017312169075012207\n",
      "Batch 132: loss 0.002697765827178955\n",
      "Batch 133: loss 0.0049942731857299805\n",
      "Batch 134: loss 0.00675123929977417\n",
      "Batch 135: loss 0.0035939812660217285\n",
      "Batch 136: loss 0.00768965482711792\n",
      "Batch 137: loss 0.008703410625457764\n",
      "Batch 138: loss 0.0010358691215515137\n",
      "Batch 139: loss 0.006557166576385498\n",
      "Batch 140: loss 0.004483997821807861\n",
      "Batch 141: loss 0.0028725266456604004\n",
      "Batch 142: loss 0.0013355016708374023\n",
      "Batch 143: loss 0.0018216967582702637\n",
      "Batch 144: loss 0.00700300931930542\n",
      "Batch 145: loss 0.006253421306610107\n",
      "Epoch 9: Training loss 0.006071224250323563\n",
      "Epoch 9: Validation loss 0.46148196458816526 | F1 0.4886802613735199 | AUC 0.7863835927182751 | Acc H-Mean 0.6101804237422793\n",
      "Best epoch:  1\n"
     ]
    }
   ],
   "source": [
    "mtrainer.run_train(9, support_loader, query_loader, lr=1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.4831690788269043 | F1 0.46632564067840576 | AUC 0.77686636362777 | Specificity 0.8896466493606567 | Recall 0.4324343800544739 | Bal Acc 0.6610405445098877 | Acc 0.7259999513626099\n",
      "Loss 0.47481298446655273 | F1 0.4678463339805603 | AUC 0.7806188292567595 | Specificity 0.8718984723091125 | Recall 0.45877307653427124 | Bal Acc 0.6653357744216919 | Acc 0.7379999756813049\n",
      "Loss 0.4411987066268921 | F1 0.49595433473587036 | AUC 0.7779539128360795 | Specificity 0.8714594841003418 | Recall 0.4803504943847656 | Bal Acc 0.6759049892425537 | Acc 0.7439999580383301\n",
      "Loss 0.47626906633377075 | F1 0.47104692459106445 | AUC 0.7963559349410614 | Specificity 0.8575273752212524 | Recall 0.47236400842666626 | Bal Acc 0.6649457216262817 | Acc 0.7269999980926514\n",
      "Loss 0.4233925938606262 | F1 0.5393682718276978 | AUC 0.8172842148198833 | Specificity 0.8938779830932617 | Recall 0.5020822882652283 | Bal Acc 0.6979801654815674 | Acc 0.7700000405311584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.45976848602294923,\n",
       " 0.48810830116271975,\n",
       " 0.7898158510963108,\n",
       " 0.876881992816925,\n",
       " 0.46920084953308105)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.model, query_loader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.3236452341079712 | F1 0.6388580203056335 | AUC 0.8052553663786899 | Specificity 0.8051336407661438 | Recall 0.6420743465423584 | Bal Acc 0.7236039638519287 | Acc 0.7380000352859497\n",
      "Loss 0.35609865188598633 | F1 0.5631943941116333 | AUC 0.7861629817423941 | Specificity 0.8200464248657227 | Recall 0.5798979997634888 | Bal Acc 0.6999722123146057 | Acc 0.7230000495910645\n",
      "Loss 0.3331189751625061 | F1 0.6214109659194946 | AUC 0.819008560788822 | Specificity 0.8354839086532593 | Recall 0.6005048155784607 | Bal Acc 0.7179943323135376 | Acc 0.7569999694824219\n",
      "Loss 0.347114622592926 | F1 0.5806185603141785 | AUC 0.7836986628788286 | Specificity 0.8207861185073853 | Recall 0.5990288257598877 | Bal Acc 0.7099074721336365 | Acc 0.7280000448226929\n",
      "Loss 0.3335912227630615 | F1 0.5932779312133789 | AUC 0.7935558433293111 | Specificity 0.8327345848083496 | Recall 0.6035705208778381 | Bal Acc 0.7181525230407715 | Acc 0.7409999966621399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3387137413024902,\n",
       " 0.5994719743728638,\n",
       " 0.797536283023609,\n",
       " 0.8228369355201721,\n",
       " 0.6050153017044068)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, query_loader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.best_model, 'models/backbone/pretrained/vindr2/trained-backbone-ba.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from models.backbone.trainer import DSTrainer\n",
    "from utils.f1_loss import BalAccuracyLoss, MCCLoss, F1Loss\n",
    "from models.embedding.model import load_medclip_retrained_resnet, load_pretrained_resnet\n",
    "\n",
    "backbone = torch.load('models/backbone/pretrained/vindr2/trained-backbone-ba1.pth')\n",
    "\n",
    "mtrainer = DSTrainer(backbone, query_dataset.class_labels(), criterion=BalAccuracyLoss(), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrainer.run_train(7, support_loader, query_loader, lr=1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.best_model, 'models/backbone/pretrained/vindr2/trained-backbone-ba1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "from models.backbone.trainer import DSTrainer\n",
    "from utils.f1_loss import BalAccuracyLoss\n",
    "\n",
    "backbone = torch.load('models/backbone/pretrained/vindr2/pretrained-ft.pth')\n",
    "\n",
    "mtrainer = DSTrainer(backbone, query_dataset.class_labels(), criterion=BalAccuracyLoss(), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total 13 epochs\n",
    "mtrainer.run_train(3, support_loader, query_loader, lr=1e-4, min_lr=1e-5, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.best_model, 'models/backbone/pretrained/vindr2/pretrained-ft.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torch/functional.py:799: UserWarning: MPS: _unique2 op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performace implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Unique.mm:354.)\n",
      "  output, inverse_indices, counts = torch._unique2(\n",
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchmetrics/utilities/compute.py:52: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:218.)\n",
      "  denom[denom == 0.0] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.3752140998840332 | F1 0.5579749345779419 | AUC 0.6818722377061905 | Specificity 0.6007749438285828 | Recall 0.6888378858566284 | Bal Acc 0.6448063850402832 | Raw Acc 0.6240000128746033\n",
      "Loss 0.3561440706253052 | F1 0.5730926394462585 | AUC 0.7209999003372558 | Specificity 0.6452262997627258 | Recall 0.6919309496879578 | Bal Acc 0.6685786247253418 | Raw Acc 0.6490000486373901\n",
      "Loss 0.3474789261817932 | F1 0.5531376600265503 | AUC 0.7310684379819131 | Specificity 0.6689056754112244 | Recall 0.6753683686256409 | Bal Acc 0.6721370220184326 | Raw Acc 0.652999997138977\n",
      "Loss 0.3710537552833557 | F1 0.5372604131698608 | AUC 0.6892123562738988 | Specificity 0.6180906295776367 | Recall 0.6586120128631592 | Bal Acc 0.638351321220398 | Raw Acc 0.6230000257492065\n",
      "Loss 0.34730011224746704 | F1 0.5868090391159058 | AUC 0.7284682124691239 | Specificity 0.6394455432891846 | Recall 0.7057230472564697 | Bal Acc 0.6725842952728271 | Raw Acc 0.6539999842643738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3594381928443909,\n",
       " 0.5616549372673034,\n",
       " 0.7103242289536764,\n",
       " 0.6344886183738708,\n",
       " 0.6840944528579712)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.model, query_loader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.3633430004119873 | F1 0.5735126733779907 | AUC 0.7259596714303461 | Specificity 0.6672283411026001 | Recall 0.6480233073234558 | Bal Acc 0.6576257944107056 | Raw Acc 0.6639999747276306\n",
      "Loss 0.3383271098136902 | F1 0.5868979096412659 | AUC 0.7238842103494072 | Specificity 0.6317911148071289 | Recall 0.7322815656661987 | Bal Acc 0.6820363402366638 | Raw Acc 0.6549999713897705\n",
      "Loss 0.36487895250320435 | F1 0.5721150636672974 | AUC 0.7073469603005058 | Specificity 0.6365436315536499 | Recall 0.6813758611679077 | Bal Acc 0.6589597463607788 | Raw Acc 0.6499999761581421\n",
      "Loss 0.3419176936149597 | F1 0.5767743587493896 | AUC 0.725889286229896 | Specificity 0.6792066097259521 | Recall 0.6816607713699341 | Bal Acc 0.6804336905479431 | Raw Acc 0.6769999861717224\n",
      "Loss 0.3819623589515686 | F1 0.5113722681999207 | AUC 0.684822142471056 | Specificity 0.6611570715904236 | Recall 0.6062338352203369 | Bal Acc 0.6336954832077026 | Raw Acc 0.6240000128746033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.35808582305908204,\n",
       " 0.5641344547271728,\n",
       " 0.7135804541562424,\n",
       " 0.6551853537559509,\n",
       " 0.6699150681495667)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, query_loader, True)\n",
    "\n",
    "# Loss 0.3633430004119873 | F1 0.5735126733779907 | AUC 0.7259596714303461 | Specificity 0.6672283411026001 | Recall 0.6480233073234558 | Bal Acc 0.6576257944107056 | Raw Acc 0.6639999747276306\n",
    "# Loss 0.3383271098136902 | F1 0.5868979096412659 | AUC 0.7238842103494072 | Specificity 0.6317911148071289 | Recall 0.7322815656661987 | Bal Acc 0.6820363402366638 | Raw Acc 0.6549999713897705\n",
    "# Loss 0.36487895250320435 | F1 0.5721150636672974 | AUC 0.7073469603005058 | Specificity 0.6365436315536499 | Recall 0.6813758611679077 | Bal Acc 0.6589597463607788 | Raw Acc 0.6499999761581421\n",
    "# Loss 0.3419176936149597 | F1 0.5767743587493896 | AUC 0.725889286229896 | Specificity 0.6792066097259521 | Recall 0.6816607713699341 | Bal Acc 0.6804336905479431 | Raw Acc 0.6769999861717224\n",
    "# (0.35808582305908204,\n",
    "#  0.5641344547271728,\n",
    "#  0.7135804541562424,\n",
    "#  0.6551853537559509,\n",
    "#  0.6699150681495667)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3af20c5b7ad2810593c71dd4ba5b7d473da7f58b181d32c1c7ac42846aa4b248"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
