{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medmnist\n",
    "from medmnist import INFO\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "# chestmnist, retinamnist\n",
    "def get_image_mean_std(dataname):\n",
    "    info = INFO[dataname]\n",
    "    DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((224, 224))\n",
    "            ])\n",
    "\n",
    "    train_dataset = DataClass(split='train', transform=transform, download=True)\n",
    "\n",
    "    train_loader = data.DataLoader(dataset=train_dataset, batch_size=8192)\n",
    "    total = info['n_samples']['train']\n",
    "    mean = torch.zeros(info['n_channels'])\n",
    "    std = torch.zeros(info['n_channels'])\n",
    "    for images, _ in train_loader:\n",
    "        num_img = len(images)\n",
    "        m, s = images.mean([0,2,3]), images.std([0,2,3])\n",
    "        mean += num_img * m / total\n",
    "        std += np.sqrt(num_img/total) * s\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/naomileow/.medmnist/pathmnist.npz\n",
      "pathmnist tensor([0.7405, 0.5330, 0.7058]) tensor([0.3920, 0.5636, 0.3959])\n",
      "Downloading https://zenodo.org/record/6496656/files/chestmnist.npz?download=1 to /Users/naomileow/.medmnist/chestmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a4e26d9d65463885431a1112f1c12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82802576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chestmnist tensor([0.4936]) tensor([0.7392])\n",
      "Downloading https://zenodo.org/record/6496656/files/dermamnist.npz?download=1 to /Users/naomileow/.medmnist/dermamnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a9f387e2264897bdff7ae73a39c3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19725078 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dermamnist tensor([0.7631, 0.5381, 0.5614]) tensor([0.1354, 0.1530, 0.1679])\n",
      "Downloading https://zenodo.org/record/6496656/files/octmnist.npz?download=1 to /Users/naomileow/.medmnist/octmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7097f8dabc6248908b4ff79621ca9174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54938180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "octmnist tensor([0.1889]) tensor([0.6606])\n",
      "Downloading https://zenodo.org/record/6496656/files/pneumoniamnist.npz?download=1 to /Users/naomileow/.medmnist/pneumoniamnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0ae89427524cf7806d3cbcd5289d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4170669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pneumoniamnist tensor([0.5719]) tensor([0.1651])\n",
      "Downloading https://zenodo.org/record/6496656/files/retinamnist.npz?download=1 to /Users/naomileow/.medmnist/retinamnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f60fc6b2d63460bb3a2196718b78112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3291041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retinamnist tensor([0.3984, 0.2447, 0.1558]) tensor([0.2952, 0.1970, 0.1470])\n",
      "Downloading https://zenodo.org/record/6496656/files/breastmnist.npz?download=1 to /Users/naomileow/.medmnist/breastmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e47eaa870494073b514256c59621848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/559580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breastmnist tensor([0.3276]) tensor([0.2027])\n",
      "Downloading https://zenodo.org/record/6496656/files/bloodmnist.npz?download=1 to /Users/naomileow/.medmnist/bloodmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b123ec2919ca4a54ba8645f6c203c062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35461855 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloodmnist tensor([0.7943, 0.6597, 0.6962]) tensor([0.2930, 0.3292, 0.1541])\n",
      "Downloading https://zenodo.org/record/6496656/files/tissuemnist.npz?download=1 to /Users/naomileow/.medmnist/tissuemnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f937abd55417400ca0f8ab5a41cc9cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124962739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tissuemnist tensor([0.1020]) tensor([0.4443])\n",
      "Downloading https://zenodo.org/record/6496656/files/organamnist.npz?download=1 to /Users/naomileow/.medmnist/organamnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd87d128acb4dabae5ec87edc43dc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38247903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organamnist tensor([0.4678]) tensor([0.6105])\n",
      "Downloading https://zenodo.org/record/6496656/files/organcmnist.npz?download=1 to /Users/naomileow/.medmnist/organcmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91efcfa71b4f495a9e83122e8df95ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15527535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organcmnist tensor([0.4932]) tensor([0.3762])\n",
      "Downloading https://zenodo.org/record/6496656/files/organsmnist.npz?download=1 to /Users/naomileow/.medmnist/organsmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a442275b7a495d81bd3d380182f0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16528536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organsmnist tensor([0.4950]) tensor([0.3779])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdecb9172087411d972fa4e036525a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32657407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_stds = {}\n",
    "\n",
    "for k in ['pathmnist', 'chestmnist', 'dermamnist', 'octmnist', 'pneumoniamnist', 'retinamnist', 'breastmnist', 'bloodmnist', 'tissuemnist', 'organamnist', 'organcmnist', 'organsmnist']:\n",
    "    mean, std = get_image_mean_std(k)\n",
    "    print(k, mean, std)\n",
    "    mean_stds[k] = {\n",
    "        'mean': mean,\n",
    "        'std': std\n",
    "    }\n",
    "print(mean_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.device import get_device\n",
    "from models.backbone.datasets import MEAN_STDS, DataSets\n",
    "from models.backbone.trainer import Trainer\n",
    "\n",
    "device = get_device()\n",
    "MODEL_SAVE_PATH = 'models/backbone/pretrained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/naomileow/.medmnist/retinamnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/retinamnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/retinamnist.npz\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torch import nn\n",
    "\n",
    "retina_ds = DataSets('retinamnist', MEAN_STDS) # 5 classes\n",
    "backbone = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "backbone.fc = nn.Linear(2048, len(retina_ds.info['label']))\n",
    "\n",
    "batch_size = 256\n",
    "rtrainer = Trainer(backbone, retina_ds, batch_size, device, balance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrainer.run_train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.52, 0.7557038049792416, 1.3845270824432374)\n"
     ]
    }
   ],
   "source": [
    "print(rtrainer.run_eval(rtrainer.best_model, rtrainer.test_loader)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# (0.5125, 0.7247683647010932, 1.464751205444336) for non pretrained, non balanced\n",
    "print(rtrainer.run_eval(rtrainer.best_model, rtrainer.test_loader)) \n",
    "\n",
    "torch.save(rtrainer.best_model.state_dict(), os.path.join(MODEL_SAVE_PATH, 'retina_backbone_pretrained_bal.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "# 'chestmnist', 'pneumoniamnist', 'octmnist',  'retinamnist'\n",
    "chest_ds = DataSets('chestmnist', MEAN_STDS)\n",
    "\n",
    "backbone = torchvision.models.resnet50(num_classes=len(chest_ds.info['label']), pretrained=False)\n",
    "# patch for single channel\n",
    "backbone.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "batch_size = 128\n",
    "ctrainer = Trainer(backbone, chest_ds, batch_size, device)\n",
    "\n",
    "ctrainer.run_train(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# [7996,  1950,  9261, 13914,  3988,  4375,   978,  3705,  3263, 1690,  1799,  1158,  2279,   144]\n",
    "chest_ds = DataSets('chestmnist', mean_stds)\n",
    "\n",
    "backbone = torchvision.models.resnet50(num_classes=len(chest_ds.info['label']), weights=None)\n",
    "backbone.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "backbone.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, 'cxr_backbone.pkl')))\n",
    "\n",
    "batch_size = 256\n",
    "ctrainer = Trainer(backbone, chest_ds, batch_size, device, balance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrainer.run_train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9239799784755875, 0.6329840270919405, 0.7261211995067128)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrainer.run_eval(ctrainer.model, ctrainer.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and save pretrained resnet from medclip\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from medclip import MedCLIPModel, MedCLIPVisionModel\n",
    "\n",
    "# load MedCLIP-ResNet50\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModel)\n",
    "model.from_pretrained()\n",
    "\n",
    "bconv_weight = model.vision_model.model.conv1.weight.mean(dim=1).unsqueeze(1)\n",
    "\n",
    "# The resnet model was trained on CheXpert and MIMIC-CXR\n",
    "backbone = model.vision_model.model\n",
    "backbone.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "backbone.conv1.weight = nn.Parameter(bconv_weight)\n",
    "\n",
    "torch.save(backbone.state_dict(), 'medclip_resnet50.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT, VINDR_SPLIT2\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "\n",
    "from utils.data import get_query_and_support_ids, DatasetConfig\n",
    "from utils.device import get_device\n",
    "from models.embedding.dataset import Dataset\n",
    "from utils.sampling import MultilabelBalancedRandomSampler\n",
    "\n",
    "configs = {\n",
    "    'vindr2': DatasetConfig('datasets/vindr-cxr-png', 'data/vindr_cxr_split_labels2.pkl', 'data/vindr_train_query_set2.pkl', VINDR_CXR_LABELS, VINDR_SPLIT2, MEAN_STDS['chestmnist'])\n",
    "}\n",
    "\n",
    "config = configs['vindr2']\n",
    "\n",
    "batch_size = 10*10\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(config.img_info, config.training_split_path)\n",
    "query_dataset = Dataset(config.img_path, config.img_info, query_image_ids, config.label_names_map, config.classes_split_map['train'], mean_std=config.mean_std)\n",
    "query_loader = DataLoader(dataset=query_dataset, batch_size=batch_size, shuffle=True)\n",
    "support_dataset = Dataset(config.img_path, config.img_info, support_image_ids, config.label_names_map, config.classes_split_map['train'], mean_std=config.mean_std)\n",
    "support_loader = DataLoader(dataset=support_dataset, batch_size=batch_size, sampler=MultilabelBalancedRandomSampler(support_dataset.get_class_indicators()))\n",
    "\n",
    "PROJ_SIZE = 512\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import os\n",
    "import torch\n",
    "from models.backbone.trainer import DSTrainer\n",
    "\n",
    "backbone = torchvision.models.resnet50(num_classes=len(config.classes_split_map['train']), weights=None)\n",
    "backbone.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "mtrainer = DSTrainer(backbone, query_dataset.class_labels(), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 0.7594740390777588\n",
      "Batch 2: loss 0.7567426562309265\n",
      "Batch 3: loss 0.7553663651148478\n",
      "Batch 4: loss 0.7501944750547409\n",
      "Batch 5: loss 0.7435857892036438\n",
      "Batch 6: loss 0.7360113759835561\n",
      "Batch 7: loss 0.7286982706614903\n",
      "Batch 8: loss 0.7190685793757439\n",
      "Batch 9: loss 0.7093484136793349\n",
      "Batch 10: loss 0.7008466958999634\n",
      "Batch 11: loss 0.6924483288418163\n",
      "Batch 12: loss 0.6842299898465475\n",
      "Batch 13: loss 0.6778851472414457\n",
      "Batch 14: loss 0.6741286473614829\n",
      "Batch 15: loss 0.6680373112360637\n",
      "Batch 16: loss 0.6633482836186886\n",
      "Batch 17: loss 0.6590615791432998\n",
      "Batch 18: loss 0.6545084714889526\n",
      "Batch 19: loss 0.6504338791495875\n",
      "Batch 20: loss 0.6483506679534912\n",
      "Batch 21: loss 0.6452367703119913\n",
      "Batch 22: loss 0.6427820920944214\n",
      "Batch 23: loss 0.6403462601744611\n",
      "Batch 24: loss 0.6381940320134163\n",
      "Batch 25: loss 0.6357972574234009\n",
      "Batch 26: loss 0.633175634420835\n",
      "Batch 27: loss 0.6309664536405493\n",
      "Batch 28: loss 0.6286095295633588\n",
      "Batch 29: loss 0.6263313272903706\n",
      "Batch 30: loss 0.6244158486525218\n",
      "Batch 31: loss 0.6216799251494869\n",
      "Batch 32: loss 0.619188928976655\n",
      "Batch 33: loss 0.6168509252143629\n",
      "Batch 34: loss 0.6146490240798277\n",
      "Batch 35: loss 0.6124486020633153\n",
      "Batch 36: loss 0.6102299392223358\n",
      "Batch 37: loss 0.6085198167208079\n",
      "Batch 38: loss 0.6061664976571736\n",
      "Batch 39: loss 0.6046091807194245\n",
      "Batch 40: loss 0.6020201191306114\n",
      "Batch 41: loss 0.599859101016347\n",
      "Batch 42: loss 0.5973570879016604\n",
      "Batch 43: loss 0.5956681918266208\n",
      "Batch 44: loss 0.5934918394142931\n",
      "Batch 45: loss 0.5917248176203833\n",
      "Batch 46: loss 0.5900422976068829\n",
      "Batch 47: loss 0.5882906463552029\n",
      "Batch 48: loss 0.5861628477772077\n",
      "Batch 49: loss 0.584035304736118\n",
      "Batch 50: loss 0.5820456457138061\n",
      "Batch 51: loss 0.5796313011178783\n",
      "Batch 52: loss 0.5772964988763516\n",
      "Batch 53: loss 0.5754651865869198\n",
      "Batch 54: loss 0.572834508838477\n",
      "Batch 55: loss 0.5713115903464231\n",
      "Batch 56: loss 0.568782274212156\n",
      "Batch 57: loss 0.5663177355339652\n",
      "Batch 58: loss 0.5642248109496873\n",
      "Batch 59: loss 0.5620224758730097\n",
      "Batch 60: loss 0.5597363511721293\n",
      "Batch 61: loss 0.5572769255911718\n",
      "Batch 62: loss 0.5565215565504567\n",
      "Batch 63: loss 0.5543250621311249\n",
      "Batch 64: loss 0.5521025904454291\n",
      "Batch 65: loss 0.5503290359790508\n",
      "Batch 66: loss 0.5481929327502395\n",
      "Batch 67: loss 0.5463291148641216\n",
      "Batch 68: loss 0.5441707683836713\n",
      "Batch 69: loss 0.5423957347006038\n",
      "Batch 70: loss 0.5402523547410965\n",
      "Batch 71: loss 0.5388048681574809\n",
      "Batch 72: loss 0.5374424983229902\n",
      "Batch 73: loss 0.5360180106881547\n",
      "Batch 74: loss 0.5345108186876452\n",
      "Batch 75: loss 0.5327034743626913\n",
      "Batch 76: loss 0.5310476336039995\n",
      "Batch 77: loss 0.5291458062537304\n",
      "Batch 78: loss 0.5271253853272169\n",
      "Batch 79: loss 0.5253142261806922\n",
      "Batch 80: loss 0.5232522983103991\n",
      "Batch 81: loss 0.5214518981951254\n",
      "Batch 82: loss 0.5196401665850383\n",
      "Batch 83: loss 0.5179045243435595\n",
      "Batch 84: loss 0.5160429435116904\n",
      "Batch 85: loss 0.5142133484868442\n",
      "Batch 86: loss 0.5123656392097473\n",
      "Batch 87: loss 0.5106275314572214\n",
      "Batch 88: loss 0.5091320838440548\n",
      "Batch 89: loss 0.5072503866774313\n",
      "Batch 90: loss 0.5054549038410187\n",
      "Batch 91: loss 0.5035542989825155\n",
      "Batch 92: loss 0.501681528661562\n",
      "Batch 93: loss 0.49962811893032444\n",
      "Batch 94: loss 0.4977384114519079\n",
      "Batch 95: loss 0.4961510360240936\n",
      "Batch 96: loss 0.49431579528997344\n",
      "Batch 97: loss 0.49307089337368604\n",
      "Batch 98: loss 0.49152907850790994\n",
      "Batch 99: loss 0.4896543796616371\n",
      "Batch 100: loss 0.48820981234312055\n",
      "Batch 101: loss 0.48617698296461953\n",
      "Batch 102: loss 0.4843711005706413\n",
      "Batch 103: loss 0.4827218961368487\n",
      "Batch 104: loss 0.4809944440539067\n",
      "Batch 105: loss 0.4794801303318569\n",
      "Batch 106: loss 0.47790700337796843\n",
      "Batch 107: loss 0.4762955650547955\n",
      "Batch 108: loss 0.47454467940109746\n",
      "Batch 109: loss 0.4728464847857799\n",
      "Batch 110: loss 0.47099953618916596\n",
      "Batch 111: loss 0.4691821680412636\n",
      "Batch 112: loss 0.46755660112415043\n",
      "Batch 113: loss 0.46588447874626226\n",
      "Batch 114: loss 0.46439649033964725\n",
      "Batch 115: loss 0.4628383330676867\n",
      "Batch 116: loss 0.4611124848497325\n",
      "Batch 117: loss 0.4595111585580386\n",
      "Batch 118: loss 0.4578940870903306\n",
      "Batch 119: loss 0.4564255661323291\n",
      "Batch 120: loss 0.45486893554528557\n",
      "Batch 121: loss 0.45333363275882627\n",
      "Batch 122: loss 0.4517213206310741\n",
      "Batch 123: loss 0.4500982422169631\n",
      "Batch 124: loss 0.4486268881347872\n",
      "Batch 125: loss 0.44756040287017823\n",
      "Batch 126: loss 0.4460098050416462\n",
      "Batch 127: loss 0.4445460424648495\n",
      "Batch 128: loss 0.4430736401118338\n",
      "Batch 129: loss 0.4416953205138214\n",
      "Batch 130: loss 0.4403984924921623\n",
      "Batch 131: loss 0.4388168451895241\n",
      "Batch 132: loss 0.4371685307811607\n",
      "Batch 133: loss 0.4358413363981964\n",
      "Batch 134: loss 0.4346085690073113\n",
      "Batch 135: loss 0.4334424450441643\n",
      "Batch 136: loss 0.43207249275463466\n",
      "Batch 137: loss 0.4307678834582767\n",
      "Batch 138: loss 0.4294682442062143\n",
      "Batch 139: loss 0.42816522149302116\n",
      "Batch 140: loss 0.4267650009265968\n",
      "Batch 141: loss 0.42535205840641727\n",
      "Batch 142: loss 0.42402067602100507\n",
      "Batch 143: loss 0.42275997246062\n",
      "Batch 144: loss 0.4214749860887726\n",
      "Batch 145: loss 0.4203092025796142\n",
      "Epoch 1: Training loss 0.4203092025796142\n",
      "Epoch 1: Validation loss 0.6857159852981567 | Accuracy 0.6688000000000001 | AUC 0.6491536748689871\n",
      "Batch 1: loss 0.2792271375656128\n",
      "Batch 2: loss 0.24789757281541824\n",
      "Batch 3: loss 0.23443768421808878\n",
      "Batch 4: loss 0.23431681841611862\n",
      "Batch 5: loss 0.23287765085697174\n",
      "Batch 6: loss 0.23559921234846115\n",
      "Batch 7: loss 0.2316494584083557\n",
      "Batch 8: loss 0.23419290035963058\n",
      "Batch 9: loss 0.2394271128707462\n",
      "Batch 10: loss 0.24146150350570678\n",
      "Batch 11: loss 0.2403238835659894\n",
      "Batch 12: loss 0.2405230738222599\n",
      "Batch 13: loss 0.24051823409704062\n",
      "Batch 14: loss 0.23796515592506953\n",
      "Batch 15: loss 0.23651503920555114\n",
      "Batch 16: loss 0.23672430869191885\n",
      "Batch 17: loss 0.23548985491780675\n",
      "Batch 18: loss 0.23422954231500626\n",
      "Batch 19: loss 0.23321800718182012\n",
      "Batch 20: loss 0.23419115766882898\n",
      "Batch 21: loss 0.23333787847132909\n",
      "Batch 22: loss 0.2344201457771388\n",
      "Batch 23: loss 0.23416401640228604\n",
      "Batch 24: loss 0.23284725782771906\n",
      "Batch 25: loss 0.2320902919769287\n",
      "Batch 26: loss 0.23128587236771217\n",
      "Batch 27: loss 0.23119262633500276\n",
      "Batch 28: loss 0.23076392763427325\n",
      "Batch 29: loss 0.23057060406125826\n",
      "Batch 30: loss 0.2300514315565427\n",
      "Batch 31: loss 0.2290689661618202\n",
      "Batch 32: loss 0.22835827432572842\n",
      "Batch 33: loss 0.22766959035035336\n",
      "Batch 34: loss 0.2275267103139092\n",
      "Batch 35: loss 0.2258859293801444\n",
      "Batch 36: loss 0.22511169728305605\n",
      "Batch 37: loss 0.22447869705187307\n",
      "Batch 38: loss 0.22385129528610329\n",
      "Batch 39: loss 0.22322037663215247\n",
      "Batch 40: loss 0.22260495610535144\n",
      "Batch 41: loss 0.22191100331341349\n",
      "Batch 42: loss 0.2214847473161561\n",
      "Batch 43: loss 0.2205404472905536\n",
      "Batch 44: loss 0.2197467667812651\n",
      "Batch 45: loss 0.22042539020379384\n",
      "Batch 46: loss 0.22000634475894595\n",
      "Batch 47: loss 0.21941088996034988\n",
      "Batch 48: loss 0.21904763858765364\n",
      "Batch 49: loss 0.2192081638744899\n",
      "Batch 50: loss 0.2187616455554962\n",
      "Batch 51: loss 0.2178088137332131\n",
      "Batch 52: loss 0.21725599496410444\n",
      "Batch 53: loss 0.21632545849062362\n",
      "Batch 54: loss 0.2157155587165444\n",
      "Batch 55: loss 0.2152305066585541\n",
      "Batch 56: loss 0.21486334449478559\n",
      "Batch 57: loss 0.21514913596605\n",
      "Batch 58: loss 0.21467403990441355\n",
      "Batch 59: loss 0.21362228792602733\n",
      "Batch 60: loss 0.21258462940653164\n",
      "Batch 61: loss 0.212528079992435\n",
      "Batch 62: loss 0.21201673077960168\n",
      "Batch 63: loss 0.211022731567186\n",
      "Batch 64: loss 0.21100796619430184\n",
      "Batch 65: loss 0.21071268228384166\n",
      "Batch 66: loss 0.20979914475571027\n",
      "Batch 67: loss 0.21009061585611372\n",
      "Batch 68: loss 0.21023616852129207\n",
      "Batch 69: loss 0.20946961749291074\n",
      "Batch 70: loss 0.20874759980610438\n",
      "Batch 71: loss 0.2082617224102289\n",
      "Batch 72: loss 0.20804698558317292\n",
      "Batch 73: loss 0.20780838066584442\n",
      "Batch 74: loss 0.20693966766466965\n",
      "Batch 75: loss 0.206092422803243\n",
      "Batch 76: loss 0.20565977124007126\n",
      "Batch 77: loss 0.20498761166999865\n",
      "Batch 78: loss 0.20485003884786215\n",
      "Batch 79: loss 0.2042039048068131\n",
      "Batch 80: loss 0.20388042759150266\n",
      "Batch 81: loss 0.203160121669004\n",
      "Batch 82: loss 0.2025781873522735\n",
      "Batch 83: loss 0.20190355540758156\n",
      "Batch 84: loss 0.20126880226390703\n",
      "Batch 85: loss 0.20035354530110078\n",
      "Batch 86: loss 0.20013265103794808\n",
      "Batch 87: loss 0.20002626653375297\n",
      "Batch 88: loss 0.19925897432999176\n",
      "Batch 89: loss 0.1995377555656969\n",
      "Batch 90: loss 0.19892872588502036\n",
      "Batch 91: loss 0.19846908977398506\n",
      "Batch 92: loss 0.19803552183768022\n",
      "Batch 93: loss 0.1974100467338357\n",
      "Batch 94: loss 0.19681192506501016\n",
      "Batch 95: loss 0.19633623079249735\n",
      "Batch 96: loss 0.19573600962758064\n",
      "Batch 97: loss 0.19511680182107946\n",
      "Batch 98: loss 0.19471261969634465\n",
      "Batch 99: loss 0.19445213208896944\n",
      "Batch 100: loss 0.19411194577813148\n",
      "Batch 101: loss 0.19365095059470375\n",
      "Batch 102: loss 0.193098247051239\n",
      "Batch 103: loss 0.19271389763910793\n",
      "Batch 104: loss 0.19197786033440095\n",
      "Batch 105: loss 0.1916092322695823\n",
      "Batch 106: loss 0.19127049149488504\n",
      "Batch 107: loss 0.19050427171114448\n",
      "Batch 108: loss 0.18991040852334765\n",
      "Batch 109: loss 0.18922336939551415\n",
      "Batch 110: loss 0.18855843618512153\n",
      "Batch 111: loss 0.1879555207115036\n",
      "Batch 112: loss 0.18740772056792462\n",
      "Batch 113: loss 0.18672870086357657\n",
      "Batch 114: loss 0.18618368893338924\n",
      "Batch 115: loss 0.18556367353252742\n",
      "Batch 116: loss 0.18524764837889834\n",
      "Batch 117: loss 0.18472249411110186\n",
      "Batch 118: loss 0.1842858980772859\n",
      "Batch 119: loss 0.18389902345272674\n",
      "Batch 120: loss 0.1832170205190778\n",
      "Batch 121: loss 0.1826704568729913\n",
      "Batch 122: loss 0.1821064327339657\n",
      "Batch 123: loss 0.18152076629846076\n",
      "Batch 124: loss 0.1811026770381197\n",
      "Batch 125: loss 0.18060476547479629\n",
      "Batch 126: loss 0.18043362993806128\n",
      "Batch 127: loss 0.17984779847888496\n",
      "Batch 128: loss 0.17918982211267576\n",
      "Batch 129: loss 0.17897902954687445\n",
      "Batch 130: loss 0.1783349599975806\n",
      "Batch 131: loss 0.1779505134993837\n",
      "Batch 132: loss 0.17751473335154128\n",
      "Batch 133: loss 0.1769884830914942\n",
      "Batch 134: loss 0.17638835722385948\n",
      "Batch 135: loss 0.1757966607257172\n",
      "Batch 136: loss 0.17543856268200805\n",
      "Batch 137: loss 0.17486874492716614\n",
      "Batch 138: loss 0.17425268391768137\n",
      "Batch 139: loss 0.1736986524016737\n",
      "Batch 140: loss 0.17324845269322395\n",
      "Batch 141: loss 0.17276568948588472\n",
      "Batch 142: loss 0.17224559880478282\n",
      "Batch 143: loss 0.17181711352163262\n",
      "Batch 144: loss 0.1712852185074654\n",
      "Batch 145: loss 0.17097628826034972\n",
      "Epoch 2: Training loss 0.17097628826034972\n",
      "Epoch 2: Validation loss 1.5301698684692382 | Accuracy 0.596 | AUC 0.6662415829960502\n",
      "Batch 1: loss 0.12172887474298477\n",
      "Batch 2: loss 0.12435834482312202\n",
      "Batch 3: loss 0.11061866581439972\n",
      "Batch 4: loss 0.11310555972158909\n",
      "Batch 5: loss 0.11100229173898697\n",
      "Batch 6: loss 0.11116744329531987\n",
      "Batch 7: loss 0.11385216883250646\n",
      "Batch 8: loss 0.11125778313726187\n",
      "Batch 9: loss 0.11842780063549678\n",
      "Batch 10: loss 0.11735658049583435\n",
      "Batch 11: loss 0.11411638354713266\n",
      "Batch 12: loss 0.11663822643458843\n",
      "Batch 13: loss 0.11622347854650937\n",
      "Batch 14: loss 0.11488724393504006\n",
      "Batch 15: loss 0.11318406611680984\n",
      "Batch 16: loss 0.11242279782891273\n",
      "Batch 17: loss 0.11206629521706525\n",
      "Batch 18: loss 0.11319540358251995\n",
      "Batch 19: loss 0.11186844541838295\n",
      "Batch 20: loss 0.11153777167201043\n",
      "Batch 21: loss 0.11065489727826346\n",
      "Batch 22: loss 0.11254671994935382\n",
      "Batch 23: loss 0.11378890632287315\n",
      "Batch 24: loss 0.11207852077980836\n",
      "Batch 25: loss 0.1120589941740036\n",
      "Batch 26: loss 0.11054152766099343\n",
      "Batch 27: loss 0.10960274244900103\n",
      "Batch 28: loss 0.10893873976809638\n",
      "Batch 29: loss 0.10887851386234679\n",
      "Batch 30: loss 0.10770622789859771\n",
      "Batch 31: loss 0.10709008238007946\n",
      "Batch 32: loss 0.10649027163162827\n",
      "Batch 33: loss 0.10535505955869501\n",
      "Batch 34: loss 0.1052985940785969\n",
      "Batch 35: loss 0.105319402047566\n",
      "Batch 36: loss 0.10565244385765658\n",
      "Batch 37: loss 0.1051009490683272\n",
      "Batch 38: loss 0.10448391461058666\n",
      "Batch 39: loss 0.10394007464249928\n",
      "Batch 40: loss 0.10364049319177866\n",
      "Batch 41: loss 0.10243195813240075\n",
      "Batch 42: loss 0.1022763785329603\n",
      "Batch 43: loss 0.10240751097715178\n",
      "Batch 44: loss 0.10239706704901023\n",
      "Batch 45: loss 0.10176854191554917\n",
      "Batch 46: loss 0.10179004583345808\n",
      "Batch 47: loss 0.10150578562566574\n",
      "Batch 48: loss 0.10101178808448215\n",
      "Batch 49: loss 0.1002028214992309\n",
      "Batch 50: loss 0.09968403473496437\n",
      "Batch 51: loss 0.09950779436850081\n",
      "Batch 52: loss 0.099498758952205\n",
      "Batch 53: loss 0.09915700189347537\n",
      "Batch 54: loss 0.09872136927313274\n",
      "Batch 55: loss 0.09893537366932088\n",
      "Batch 56: loss 0.0983630046248436\n",
      "Batch 57: loss 0.09814639141162236\n",
      "Batch 58: loss 0.09753467129736111\n",
      "Batch 59: loss 0.09708601905632827\n",
      "Batch 60: loss 0.09647919858495395\n",
      "Batch 61: loss 0.09627314021841424\n",
      "Batch 62: loss 0.09573615017917848\n",
      "Batch 63: loss 0.09512458995930732\n",
      "Batch 64: loss 0.09516920027090237\n",
      "Batch 65: loss 0.09502219139383389\n",
      "Batch 66: loss 0.09456000482719956\n",
      "Batch 67: loss 0.09469612849070065\n",
      "Batch 68: loss 0.09458642524174031\n",
      "Batch 69: loss 0.09429321713421655\n",
      "Batch 70: loss 0.0939432164920228\n",
      "Batch 71: loss 0.09356865313061526\n",
      "Batch 72: loss 0.0933236411979629\n",
      "Batch 73: loss 0.09297763479694929\n",
      "Batch 74: loss 0.09332707730700841\n",
      "Batch 75: loss 0.09321431155006091\n",
      "Batch 76: loss 0.09263875374668523\n",
      "Batch 77: loss 0.09227693100254257\n",
      "Batch 78: loss 0.0918432801293257\n",
      "Batch 79: loss 0.0913590476199796\n",
      "Batch 80: loss 0.0910709414165467\n",
      "Batch 81: loss 0.09087590868642301\n",
      "Batch 82: loss 0.0903183587531491\n",
      "Batch 83: loss 0.09038153079798422\n",
      "Batch 84: loss 0.09011604197855506\n",
      "Batch 85: loss 0.0896845462567666\n",
      "Batch 86: loss 0.08927784790826399\n",
      "Batch 87: loss 0.0891952543765649\n",
      "Batch 88: loss 0.08876492988995531\n",
      "Batch 89: loss 0.08855286851692735\n",
      "Batch 90: loss 0.08847565576434135\n",
      "Batch 91: loss 0.08801554626488424\n",
      "Batch 92: loss 0.0877431773621103\n",
      "Batch 93: loss 0.08733938682463861\n",
      "Batch 94: loss 0.08726486000925937\n",
      "Batch 95: loss 0.08732355213478991\n",
      "Batch 96: loss 0.0875488962046802\n",
      "Batch 97: loss 0.08734842197796733\n",
      "Batch 98: loss 0.08702722085373742\n",
      "Batch 99: loss 0.08696879363722271\n",
      "Batch 100: loss 0.0866626676917076\n",
      "Batch 101: loss 0.08645060599440395\n",
      "Batch 102: loss 0.08656194272871111\n",
      "Batch 103: loss 0.0863137485694538\n",
      "Batch 104: loss 0.08633892549774967\n",
      "Batch 105: loss 0.08615789757598014\n",
      "Batch 106: loss 0.0861595733758976\n",
      "Batch 107: loss 0.08585864726767362\n",
      "Batch 108: loss 0.08563326516499122\n",
      "Batch 109: loss 0.08526662918269087\n",
      "Batch 110: loss 0.08514699336480011\n",
      "Batch 111: loss 0.08498410501324379\n",
      "Batch 112: loss 0.08487217224735234\n",
      "Batch 113: loss 0.0846293726523893\n",
      "Batch 114: loss 0.08432343589109287\n",
      "Batch 115: loss 0.08412533191883045\n",
      "Batch 116: loss 0.08382155736587171\n",
      "Batch 117: loss 0.08359647414877884\n",
      "Batch 118: loss 0.0835235240727158\n",
      "Batch 119: loss 0.0832395721696505\n",
      "Batch 120: loss 0.08297121288875739\n",
      "Batch 121: loss 0.08295615460754426\n",
      "Batch 122: loss 0.08268469376642196\n",
      "Batch 123: loss 0.08241424845849596\n",
      "Batch 124: loss 0.08212338405991754\n",
      "Batch 125: loss 0.08201720404624939\n",
      "Batch 126: loss 0.08199385311158877\n",
      "Batch 127: loss 0.08171228395672295\n",
      "Batch 128: loss 0.08152208448154852\n",
      "Batch 129: loss 0.08119691700436348\n",
      "Batch 130: loss 0.08092779967074211\n",
      "Batch 131: loss 0.0807022424201474\n",
      "Batch 132: loss 0.0804414636257923\n",
      "Batch 133: loss 0.08033207114925958\n",
      "Batch 134: loss 0.0800532841804757\n",
      "Batch 135: loss 0.07983529736046438\n",
      "Batch 136: loss 0.07970935130930122\n",
      "Batch 137: loss 0.07957898281569029\n",
      "Batch 138: loss 0.07928834736779117\n",
      "Batch 139: loss 0.07903990749832537\n",
      "Batch 140: loss 0.07893101775220462\n",
      "Batch 141: loss 0.07881928383247226\n",
      "Batch 142: loss 0.0786769199644176\n",
      "Batch 143: loss 0.07850311594930562\n",
      "Batch 144: loss 0.07833407410523957\n",
      "Batch 145: loss 0.07808695098749106\n",
      "Epoch 3: Training loss 0.07808695098749106\n",
      "Epoch 3: Validation loss 0.9447278618812561 | Accuracy 0.6944 | AUC 0.6773402684919\n",
      "Batch 1: loss 0.06296105682849884\n",
      "Batch 2: loss 0.06926922500133514\n",
      "Batch 3: loss 0.06714459260304768\n",
      "Batch 4: loss 0.0721548330038786\n",
      "Batch 5: loss 0.06792469322681427\n",
      "Batch 6: loss 0.0760734813908736\n",
      "Batch 7: loss 0.07600908832890647\n",
      "Batch 8: loss 0.07281394675374031\n",
      "Batch 9: loss 0.06846331308285396\n",
      "Batch 10: loss 0.06709474772214889\n",
      "Batch 11: loss 0.06466589292342012\n",
      "Batch 12: loss 0.0621828588967522\n",
      "Batch 13: loss 0.060268319570101224\n",
      "Batch 14: loss 0.06078107495393072\n",
      "Batch 15: loss 0.06084598153829575\n",
      "Batch 16: loss 0.060006513725966215\n",
      "Batch 17: loss 0.061599928228294146\n",
      "Batch 18: loss 0.06155879588590728\n",
      "Batch 19: loss 0.06025426913248865\n",
      "Batch 20: loss 0.05971733331680298\n",
      "Batch 21: loss 0.05840041151358968\n",
      "Batch 22: loss 0.0580321421677416\n",
      "Batch 23: loss 0.05761615591852561\n",
      "Batch 24: loss 0.05732941860333085\n",
      "Batch 25: loss 0.05683256790041923\n",
      "Batch 26: loss 0.05660535796330525\n",
      "Batch 27: loss 0.0562310460265036\n",
      "Batch 28: loss 0.05529361164995602\n",
      "Batch 29: loss 0.054764572916359736\n",
      "Batch 30: loss 0.054468006392319995\n",
      "Batch 31: loss 0.053697081823502815\n",
      "Batch 32: loss 0.05302697641309351\n",
      "Batch 33: loss 0.05279670475107251\n",
      "Batch 34: loss 0.05277379821328556\n",
      "Batch 35: loss 0.05280688287956374\n",
      "Batch 36: loss 0.052470686121119395\n",
      "Batch 37: loss 0.05316544887987343\n",
      "Batch 38: loss 0.05304254660088765\n",
      "Batch 39: loss 0.05307270118441337\n",
      "Batch 40: loss 0.052601739019155505\n",
      "Batch 41: loss 0.05231025288017785\n",
      "Batch 42: loss 0.051754758338488284\n",
      "Batch 43: loss 0.05122388396845307\n",
      "Batch 44: loss 0.05100033034316518\n",
      "Batch 45: loss 0.0510916192498472\n",
      "Batch 46: loss 0.05118043331996254\n",
      "Batch 47: loss 0.050743368038154664\n",
      "Batch 48: loss 0.050774477305822074\n",
      "Batch 49: loss 0.05165714956820011\n",
      "Batch 50: loss 0.052061746157705785\n",
      "Batch 51: loss 0.05170172697627077\n",
      "Batch 52: loss 0.051760020630004316\n",
      "Batch 53: loss 0.05159445764180624\n",
      "Batch 54: loss 0.05122662808194205\n",
      "Batch 55: loss 0.050949040766466745\n",
      "Batch 56: loss 0.051595060321103246\n",
      "Batch 57: loss 0.05153551674856428\n",
      "Batch 58: loss 0.05171172221287571\n",
      "Batch 59: loss 0.051531337485727614\n",
      "Batch 60: loss 0.05159130043660601\n",
      "Batch 61: loss 0.05182692454364456\n",
      "Batch 62: loss 0.05172282508424213\n",
      "Batch 63: loss 0.05145946940377591\n",
      "Batch 64: loss 0.05155324048246257\n",
      "Batch 65: loss 0.051962445876919304\n",
      "Batch 66: loss 0.051697971214624966\n",
      "Batch 67: loss 0.051541583668162576\n",
      "Batch 68: loss 0.05121729303808773\n",
      "Batch 69: loss 0.05093490429546522\n",
      "Batch 70: loss 0.05126274313245501\n",
      "Batch 71: loss 0.051430811659551004\n",
      "Batch 72: loss 0.05102915924766825\n",
      "Batch 73: loss 0.05139025839122191\n",
      "Batch 74: loss 0.051709171560769145\n",
      "Batch 75: loss 0.05139904871582985\n",
      "Batch 76: loss 0.05109498287109952\n",
      "Batch 77: loss 0.050908950435650815\n",
      "Batch 78: loss 0.05079154455317901\n",
      "Batch 79: loss 0.05070343222233314\n",
      "Batch 80: loss 0.050330899306572974\n",
      "Batch 81: loss 0.05008784343523008\n",
      "Batch 82: loss 0.04979181746248065\n",
      "Batch 83: loss 0.04959344421793897\n",
      "Batch 84: loss 0.049435233941213005\n",
      "Batch 85: loss 0.04937537034206531\n",
      "Batch 86: loss 0.04925419032833604\n",
      "Batch 87: loss 0.048961924174907565\n",
      "Batch 88: loss 0.04875403712503612\n",
      "Batch 89: loss 0.04844954155720352\n",
      "Batch 90: loss 0.048358935510946645\n",
      "Batch 91: loss 0.048254002970489825\n",
      "Batch 92: loss 0.04810708479793823\n",
      "Batch 93: loss 0.048026224598288536\n",
      "Batch 94: loss 0.047861895444703866\n",
      "Batch 95: loss 0.04760305148206259\n",
      "Batch 96: loss 0.04739741680289929\n",
      "Batch 97: loss 0.04732292258785557\n",
      "Batch 98: loss 0.04714436772070369\n",
      "Batch 99: loss 0.04695787241287304\n",
      "Batch 100: loss 0.04694742472842336\n",
      "Batch 101: loss 0.04679450066299132\n",
      "Batch 102: loss 0.04663026155721323\n",
      "Batch 103: loss 0.04642201531497599\n",
      "Batch 104: loss 0.04618096650721362\n",
      "Batch 105: loss 0.0459611905650014\n",
      "Batch 106: loss 0.04579917209679788\n",
      "Batch 107: loss 0.04573943864588983\n",
      "Batch 108: loss 0.04584829186744712\n",
      "Batch 109: loss 0.045772079327101006\n",
      "Batch 110: loss 0.04553169045935978\n",
      "Batch 111: loss 0.04528453288314579\n",
      "Batch 112: loss 0.045027922556203394\n",
      "Batch 113: loss 0.0448370095408332\n",
      "Batch 114: loss 0.04470320072090417\n",
      "Batch 115: loss 0.04453824062062346\n",
      "Batch 116: loss 0.04443899079643447\n",
      "Batch 117: loss 0.044287975845683336\n",
      "Batch 118: loss 0.04411306465833874\n",
      "Batch 119: loss 0.04417387499403553\n",
      "Batch 120: loss 0.044066429541756706\n",
      "Batch 121: loss 0.04399648866008136\n",
      "Batch 122: loss 0.043809967146056596\n",
      "Batch 123: loss 0.04359892410476033\n",
      "Batch 124: loss 0.04343479702008828\n",
      "Batch 125: loss 0.0433179057687521\n",
      "Batch 126: loss 0.04318203840641275\n",
      "Batch 127: loss 0.043181787486972774\n",
      "Batch 128: loss 0.04299441094917711\n",
      "Batch 129: loss 0.042910742219681886\n",
      "Batch 130: loss 0.04289022762901508\n",
      "Batch 131: loss 0.042777159670723305\n",
      "Batch 132: loss 0.04263322540756428\n",
      "Batch 133: loss 0.042564056757697484\n",
      "Batch 134: loss 0.042460090369542146\n",
      "Batch 135: loss 0.042275669508510166\n",
      "Batch 136: loss 0.04218337725957527\n",
      "Batch 137: loss 0.042035956003696376\n",
      "Batch 138: loss 0.04188139076628115\n",
      "Batch 139: loss 0.0417235701499011\n",
      "Batch 140: loss 0.041746358373867615\n",
      "Batch 141: loss 0.04166153554163926\n",
      "Batch 142: loss 0.04160893356926005\n",
      "Batch 143: loss 0.041536951763229774\n",
      "Batch 144: loss 0.04142092429618868\n",
      "Batch 145: loss 0.041253710128857965\n",
      "Epoch 4: Training loss 0.041253710128857965\n",
      "Epoch 4: Validation loss 1.6947896480560303 | Accuracy 0.6062000000000001 | AUC 0.6868000175998634\n",
      "Batch 1: loss 0.019890137016773224\n",
      "Batch 2: loss 0.02117490116506815\n",
      "Batch 3: loss 0.022404024377465248\n",
      "Batch 4: loss 0.02122704405337572\n",
      "Batch 5: loss 0.021014757454395294\n",
      "Batch 6: loss 0.02588242602845033\n",
      "Batch 7: loss 0.027282473232064928\n",
      "Batch 8: loss 0.02603129157796502\n",
      "Batch 9: loss 0.02890888022051917\n",
      "Batch 10: loss 0.029225876554846764\n",
      "Batch 11: loss 0.02918207747015086\n",
      "Batch 12: loss 0.028911275633921225\n",
      "Batch 13: loss 0.027699255384504795\n",
      "Batch 14: loss 0.02796327675293599\n",
      "Batch 15: loss 0.027669894509017467\n",
      "Batch 16: loss 0.027360972308088094\n",
      "Batch 17: loss 0.028953540029332918\n",
      "Batch 18: loss 0.03118654138719042\n",
      "Batch 19: loss 0.03077276546115938\n",
      "Batch 20: loss 0.03101480477489531\n",
      "Batch 21: loss 0.030870641493016764\n",
      "Batch 22: loss 0.030780184209685434\n",
      "Batch 23: loss 0.030607764771127182\n",
      "Batch 24: loss 0.031224712224987645\n",
      "Batch 25: loss 0.031224562115967275\n",
      "Batch 26: loss 0.03148931282787369\n",
      "Batch 27: loss 0.03147708621152021\n",
      "Batch 28: loss 0.0311946754809469\n",
      "Batch 29: loss 0.030890921933640694\n",
      "Batch 30: loss 0.030694977038850386\n",
      "Batch 31: loss 0.03159088169735286\n",
      "Batch 32: loss 0.03184791546664201\n",
      "Batch 33: loss 0.03168267234595436\n",
      "Batch 34: loss 0.031778103261090374\n",
      "Batch 35: loss 0.03185622210481337\n",
      "Batch 36: loss 0.032213148221166596\n",
      "Batch 37: loss 0.032668961841311\n",
      "Batch 38: loss 0.03246006352434817\n",
      "Batch 39: loss 0.032569766068496764\n",
      "Batch 40: loss 0.03246576136443764\n",
      "Batch 41: loss 0.03283197898417711\n",
      "Batch 42: loss 0.03333590439121638\n",
      "Batch 43: loss 0.03390607084039339\n",
      "Batch 44: loss 0.03368460621938787\n",
      "Batch 45: loss 0.034165954238010776\n",
      "Batch 46: loss 0.03418463134490277\n",
      "Batch 47: loss 0.03400141478298192\n",
      "Batch 48: loss 0.03368919756030664\n",
      "Batch 49: loss 0.033839033213349023\n",
      "Batch 50: loss 0.033646369706839326\n",
      "Batch 51: loss 0.03350114174114138\n",
      "Batch 52: loss 0.033562864194839045\n",
      "Batch 53: loss 0.033533376993014\n",
      "Batch 54: loss 0.03347562324186718\n",
      "Batch 55: loss 0.033955103582279246\n",
      "Batch 56: loss 0.03388048187896077\n",
      "Batch 57: loss 0.033785904300186714\n",
      "Batch 58: loss 0.03360624478102244\n",
      "Batch 59: loss 0.03367248052825867\n",
      "Batch 60: loss 0.033696182522301873\n",
      "Batch 61: loss 0.033614687667396226\n",
      "Batch 62: loss 0.03333533178233812\n",
      "Batch 63: loss 0.03316136051915468\n",
      "Batch 64: loss 0.0329172673082212\n",
      "Batch 65: loss 0.032751726488081306\n",
      "Batch 66: loss 0.032567103153489756\n",
      "Batch 67: loss 0.03247583450388108\n",
      "Batch 68: loss 0.032501165083993006\n",
      "Batch 69: loss 0.0329239788343725\n",
      "Batch 70: loss 0.033152171317487954\n",
      "Batch 71: loss 0.03299983681350107\n",
      "Batch 72: loss 0.03288199730579638\n",
      "Batch 73: loss 0.03274320694936873\n",
      "Batch 74: loss 0.03258265717257116\n",
      "Batch 75: loss 0.03252201632906993\n",
      "Batch 76: loss 0.03295230724554705\n",
      "Batch 77: loss 0.032856995130997976\n",
      "Batch 78: loss 0.03296075361327101\n",
      "Batch 79: loss 0.03292251344110015\n",
      "Batch 80: loss 0.0327942369855009\n",
      "Batch 81: loss 0.03271172879792658\n",
      "Batch 82: loss 0.03279425829603541\n",
      "Batch 83: loss 0.03286788207534925\n",
      "Batch 84: loss 0.033321790563474805\n",
      "Batch 85: loss 0.03331310594563975\n",
      "Batch 86: loss 0.033187297291967066\n",
      "Batch 87: loss 0.03301610094334545\n",
      "Batch 88: loss 0.03302814209283414\n",
      "Batch 89: loss 0.03295412390712607\n",
      "Batch 90: loss 0.03280313403035204\n",
      "Batch 91: loss 0.03271963464730716\n",
      "Batch 92: loss 0.032628000238100474\n",
      "Batch 93: loss 0.032485382020553594\n",
      "Batch 94: loss 0.03236947806076484\n",
      "Batch 95: loss 0.032314668017390524\n",
      "Batch 96: loss 0.03243475392810069\n",
      "Batch 97: loss 0.032363168482390264\n",
      "Batch 98: loss 0.03227731599757562\n",
      "Batch 99: loss 0.03246744115357146\n",
      "Batch 100: loss 0.03270781307481229\n",
      "Batch 101: loss 0.03263187636206351\n",
      "Batch 102: loss 0.03245281086613735\n",
      "Batch 103: loss 0.032394871712454316\n",
      "Batch 104: loss 0.03227998506134519\n",
      "Batch 105: loss 0.03215443566441536\n",
      "Batch 106: loss 0.03217839815144269\n",
      "Batch 107: loss 0.0320870108623928\n",
      "Batch 108: loss 0.031937352398893344\n",
      "Batch 109: loss 0.03199468124145215\n",
      "Batch 110: loss 0.031934598460793495\n",
      "Batch 111: loss 0.031848380627395874\n",
      "Batch 112: loss 0.03192403320489185\n",
      "Batch 113: loss 0.031803819705295354\n",
      "Batch 114: loss 0.03179078952719768\n",
      "Batch 115: loss 0.031669164918687036\n",
      "Batch 116: loss 0.03161311281266911\n",
      "Batch 117: loss 0.03150533026673345\n",
      "Batch 118: loss 0.031455491748401676\n",
      "Batch 119: loss 0.03144325819962165\n",
      "Batch 120: loss 0.03133664163760841\n",
      "Batch 121: loss 0.03131700244016391\n",
      "Batch 122: loss 0.03123563154005125\n",
      "Batch 123: loss 0.031145178023877184\n",
      "Batch 124: loss 0.031148706472689105\n",
      "Batch 125: loss 0.031180312782526018\n",
      "Batch 126: loss 0.0311016734127724\n",
      "Batch 127: loss 0.031042253378573366\n",
      "Batch 128: loss 0.030968368431786075\n",
      "Batch 129: loss 0.030847290285335956\n",
      "Batch 130: loss 0.030924658964459714\n",
      "Batch 131: loss 0.03114014073410107\n",
      "Batch 132: loss 0.03105227533502109\n",
      "Batch 133: loss 0.030933899902983716\n",
      "Batch 134: loss 0.030814645248951753\n",
      "Batch 135: loss 0.030685824921561612\n",
      "Batch 136: loss 0.030638093779356602\n",
      "Batch 137: loss 0.03086701320334725\n",
      "Batch 138: loss 0.030815429371390222\n",
      "Batch 139: loss 0.030971780828059577\n",
      "Batch 140: loss 0.030878578718485577\n",
      "Batch 141: loss 0.030827368490714976\n",
      "Batch 142: loss 0.0307685408518243\n",
      "Batch 143: loss 0.030673676144357745\n",
      "Batch 144: loss 0.0305987911285936\n",
      "Batch 145: loss 0.030650811986221996\n",
      "Epoch 5: Training loss 0.030650811986221996\n",
      "Epoch 5: Validation loss 1.6522771835327148 | Accuracy 0.6182000000000001 | AUC 0.672199024105535\n",
      "Batch 1: loss 0.05781254917383194\n",
      "Batch 2: loss 0.04665685445070267\n",
      "Batch 3: loss 0.0395934438953797\n",
      "Batch 4: loss 0.04204474249854684\n",
      "Batch 5: loss 0.03930763453245163\n",
      "Batch 6: loss 0.03635734940568606\n",
      "Batch 7: loss 0.034295420827610154\n",
      "Batch 8: loss 0.03258782462216914\n",
      "Batch 9: loss 0.03165524577101072\n",
      "Batch 10: loss 0.03429761417210102\n",
      "Batch 11: loss 0.032739123329520226\n",
      "Batch 12: loss 0.033910545675704874\n",
      "Batch 13: loss 0.033785732463002205\n",
      "Batch 14: loss 0.03396127253238644\n",
      "Batch 15: loss 0.0342317180087169\n",
      "Batch 16: loss 0.03345563227776438\n",
      "Batch 17: loss 0.03290828558451989\n",
      "Batch 18: loss 0.03204108133084244\n",
      "Batch 19: loss 0.03107529942338404\n",
      "Batch 20: loss 0.03086080723442137\n",
      "Batch 21: loss 0.030602676483492058\n",
      "Batch 22: loss 0.030182487627660685\n",
      "Batch 23: loss 0.029863597136800705\n",
      "Batch 24: loss 0.030746844015084207\n",
      "Batch 25: loss 0.030043307021260263\n",
      "Batch 26: loss 0.02966771790614495\n",
      "Batch 27: loss 0.029823161799598624\n",
      "Batch 28: loss 0.029551518748381307\n",
      "Batch 29: loss 0.029408041644712973\n",
      "Batch 30: loss 0.02900482142964999\n",
      "Batch 31: loss 0.028633078620318445\n",
      "Batch 32: loss 0.02825463202316314\n",
      "Batch 33: loss 0.028034889054569332\n",
      "Batch 34: loss 0.027624859830693287\n",
      "Batch 35: loss 0.027370754070580004\n",
      "Batch 36: loss 0.02717032586224377\n",
      "Batch 37: loss 0.02705394524476818\n",
      "Batch 38: loss 0.027402551035935942\n",
      "Batch 39: loss 0.027627147901325654\n",
      "Batch 40: loss 0.02728247328195721\n",
      "Batch 41: loss 0.02699173457647969\n",
      "Batch 42: loss 0.026880449282803705\n",
      "Batch 43: loss 0.02659415683254253\n",
      "Batch 44: loss 0.0267882786098529\n",
      "Batch 45: loss 0.027004556068115766\n",
      "Batch 46: loss 0.026927096971675106\n",
      "Batch 47: loss 0.02668749687678002\n",
      "Batch 48: loss 0.026633926395637293\n",
      "Batch 49: loss 0.026500613561698368\n",
      "Batch 50: loss 0.02626709910109639\n",
      "Batch 51: loss 0.0260939692859264\n",
      "Batch 52: loss 0.025863060673985343\n",
      "Batch 53: loss 0.02582296382916986\n",
      "Batch 54: loss 0.026033420926304878\n",
      "Batch 55: loss 0.026220210353759203\n",
      "Batch 56: loss 0.02604314676552479\n",
      "Batch 57: loss 0.026037768003318394\n",
      "Batch 58: loss 0.02592506431492752\n",
      "Batch 59: loss 0.02575643775763653\n",
      "Batch 60: loss 0.02553750048391521\n",
      "Batch 61: loss 0.025279617822561107\n",
      "Batch 62: loss 0.025040898156622724\n",
      "Batch 63: loss 0.024917162200879483\n",
      "Batch 64: loss 0.024721664318349212\n",
      "Batch 65: loss 0.024715700201117075\n",
      "Batch 66: loss 0.024882007813589138\n",
      "Batch 67: loss 0.02518506395394233\n",
      "Batch 68: loss 0.024977179600254577\n",
      "Batch 69: loss 0.024801807248613972\n",
      "Batch 70: loss 0.024610506264226777\n",
      "Batch 71: loss 0.02449869479931576\n",
      "Batch 72: loss 0.024393309994290274\n",
      "Batch 73: loss 0.024577508591217538\n",
      "Batch 74: loss 0.024685957443875237\n",
      "Batch 75: loss 0.025518224438031513\n",
      "Batch 76: loss 0.026045840919802065\n",
      "Batch 77: loss 0.026133269023198586\n",
      "Batch 78: loss 0.025988907183114536\n",
      "Batch 79: loss 0.025917555158368393\n",
      "Batch 80: loss 0.025945862394291908\n",
      "Batch 81: loss 0.02588393372472054\n",
      "Batch 82: loss 0.025802172300201362\n",
      "Batch 83: loss 0.025676386801144445\n",
      "Batch 84: loss 0.025693995467874976\n",
      "Batch 85: loss 0.0258664518046905\n",
      "Batch 86: loss 0.02598725366514436\n",
      "Batch 87: loss 0.026377642024094344\n",
      "Batch 88: loss 0.026274186992933126\n",
      "Batch 89: loss 0.02622667533669914\n",
      "Batch 90: loss 0.02608487715737687\n",
      "Batch 91: loss 0.02618767638373506\n",
      "Batch 92: loss 0.02607100508580713\n",
      "Batch 93: loss 0.025960010576552603\n",
      "Batch 94: loss 0.026040914597267165\n",
      "Batch 95: loss 0.0259934263107808\n",
      "Batch 96: loss 0.025858792175616447\n",
      "Batch 97: loss 0.025726582355720482\n",
      "Batch 98: loss 0.025599990030560568\n",
      "Batch 99: loss 0.02553894349157509\n",
      "Batch 100: loss 0.025411369632929563\n",
      "Batch 101: loss 0.025279307747167527\n",
      "Batch 102: loss 0.025198999399721037\n",
      "Batch 103: loss 0.025241412611189975\n",
      "Batch 104: loss 0.025113239490355436\n",
      "Batch 105: loss 0.024964538500422524\n",
      "Batch 106: loss 0.024975420625985793\n",
      "Batch 107: loss 0.025130218190846043\n",
      "Batch 108: loss 0.02508257298419873\n",
      "Batch 109: loss 0.025133033923873112\n",
      "Batch 110: loss 0.025113882124423982\n",
      "Batch 111: loss 0.02541477407689567\n",
      "Batch 112: loss 0.025310386205092072\n",
      "Batch 113: loss 0.02549544583379695\n",
      "Batch 114: loss 0.025500892024291188\n",
      "Batch 115: loss 0.025438905280569326\n",
      "Batch 116: loss 0.025479244887186534\n",
      "Batch 117: loss 0.025695055452549558\n",
      "Batch 118: loss 0.025607775881002517\n",
      "Batch 119: loss 0.02563223278760409\n",
      "Batch 120: loss 0.025563639138514796\n",
      "Batch 121: loss 0.025520553889353412\n",
      "Batch 122: loss 0.0254330816526027\n",
      "Batch 123: loss 0.02530378033596326\n",
      "Batch 124: loss 0.02541999533892639\n",
      "Batch 125: loss 0.025325794331729413\n",
      "Batch 126: loss 0.025215350039717224\n",
      "Batch 127: loss 0.02521303700151171\n",
      "Batch 128: loss 0.025243989752198104\n",
      "Batch 129: loss 0.02515543528738641\n",
      "Batch 130: loss 0.02508327030361845\n",
      "Batch 131: loss 0.02513041692067876\n",
      "Batch 132: loss 0.025103259598836303\n",
      "Batch 133: loss 0.02510502016437905\n",
      "Batch 134: loss 0.02503161226857954\n",
      "Batch 135: loss 0.024969232758438147\n",
      "Batch 136: loss 0.02496046110001557\n",
      "Batch 137: loss 0.024934443556805595\n",
      "Batch 138: loss 0.024852515211787777\n",
      "Batch 139: loss 0.024748585957417385\n",
      "Batch 140: loss 0.024756981445742506\n",
      "Batch 141: loss 0.024655248751824208\n",
      "Batch 142: loss 0.02460443147812301\n",
      "Batch 143: loss 0.02469095366948343\n",
      "Batch 144: loss 0.02459873433690518\n",
      "Batch 145: loss 0.02469203272810898\n",
      "Epoch 6: Training loss 0.02469203272810898\n",
      "Epoch 6: Validation loss 1.1629847526550292 | Accuracy 0.7101999999999999 | AUC 0.6842659693120305\n",
      "Batch 1: loss 0.026518452912569046\n",
      "Batch 2: loss 0.023502327501773834\n",
      "Batch 3: loss 0.025864874323209126\n",
      "Batch 4: loss 0.024440347217023373\n",
      "Batch 5: loss 0.023715026676654816\n",
      "Batch 6: loss 0.02172406855970621\n",
      "Batch 7: loss 0.023992512641208514\n",
      "Batch 8: loss 0.025539080379530787\n",
      "Batch 9: loss 0.025770356464717124\n",
      "Batch 10: loss 0.025135564804077148\n",
      "Batch 11: loss 0.025382388213818722\n",
      "Batch 12: loss 0.025191632720331352\n",
      "Batch 13: loss 0.026297898819813363\n",
      "Batch 14: loss 0.025763671046921184\n",
      "Batch 15: loss 0.02783761074145635\n",
      "Batch 16: loss 0.02699194010347128\n",
      "Batch 17: loss 0.026095519883229452\n",
      "Batch 18: loss 0.02533167636849814\n",
      "Batch 19: loss 0.024667106176677504\n",
      "Batch 20: loss 0.024255192000418903\n",
      "Batch 21: loss 0.023705720990186648\n",
      "Batch 22: loss 0.023141203871504826\n",
      "Batch 23: loss 0.02267298446563275\n",
      "Batch 24: loss 0.022791519489449758\n",
      "Batch 25: loss 0.022574904598295687\n",
      "Batch 26: loss 0.022732541215820953\n",
      "Batch 27: loss 0.022831967410941918\n",
      "Batch 28: loss 0.022626639576628804\n",
      "Batch 29: loss 0.022728025110374236\n",
      "Batch 30: loss 0.022334528838594755\n",
      "Batch 31: loss 0.022627994898826845\n",
      "Batch 32: loss 0.022222979314392433\n",
      "Batch 33: loss 0.02221196898343888\n",
      "Batch 34: loss 0.02215853034902145\n",
      "Batch 35: loss 0.023040227671819073\n",
      "Batch 36: loss 0.022997441158319514\n",
      "Batch 37: loss 0.022620714017869654\n",
      "Batch 38: loss 0.02227233514483822\n",
      "Batch 39: loss 0.021991799561641157\n",
      "Batch 40: loss 0.021786081255413592\n",
      "Batch 41: loss 0.021797498937968802\n",
      "Batch 42: loss 0.0221508463977703\n",
      "Batch 43: loss 0.022210553570022416\n",
      "Batch 44: loss 0.02211023682982407\n",
      "Batch 45: loss 0.022748642104367413\n",
      "Batch 46: loss 0.02254034631440173\n",
      "Batch 47: loss 0.022541851677159046\n",
      "Batch 48: loss 0.022392807567181688\n",
      "Batch 49: loss 0.02222763961751242\n",
      "Batch 50: loss 0.022159376908093692\n",
      "Batch 51: loss 0.02233467910292686\n",
      "Batch 52: loss 0.02239524030413192\n",
      "Batch 53: loss 0.02220422771038874\n",
      "Batch 54: loss 0.02215081066996963\n",
      "Batch 55: loss 0.022057822380553593\n",
      "Batch 56: loss 0.02246680200499083\n",
      "Batch 57: loss 0.022307307802532848\n",
      "Batch 58: loss 0.022095701223688907\n",
      "Batch 59: loss 0.021852734640745792\n",
      "Batch 60: loss 0.021803512455274662\n",
      "Batch 61: loss 0.021612371730267026\n",
      "Batch 62: loss 0.021535006622152943\n",
      "Batch 63: loss 0.021583247811548294\n",
      "Batch 64: loss 0.021438179872347973\n",
      "Batch 65: loss 0.021404403663025453\n",
      "Batch 66: loss 0.02162081938744946\n",
      "Batch 67: loss 0.021542399412771657\n",
      "Batch 68: loss 0.021446813418365577\n",
      "Batch 69: loss 0.021379402150278507\n",
      "Batch 70: loss 0.02130259671913726\n",
      "Batch 71: loss 0.021193200827274526\n",
      "Batch 72: loss 0.021207393871413335\n",
      "Batch 73: loss 0.021111627708967417\n",
      "Batch 74: loss 0.0210784745427805\n",
      "Batch 75: loss 0.02103522260983785\n",
      "Batch 76: loss 0.020921067628813416\n",
      "Batch 77: loss 0.020991218360987576\n",
      "Batch 78: loss 0.020901023804281767\n",
      "Batch 79: loss 0.020751879196849805\n",
      "Batch 80: loss 0.020829704089555888\n",
      "Batch 81: loss 0.02070666951943695\n",
      "Batch 82: loss 0.020626356200564924\n",
      "Batch 83: loss 0.020795387148587818\n",
      "Batch 84: loss 0.02075138139272375\n",
      "Batch 85: loss 0.020766522702487075\n",
      "Batch 86: loss 0.020745846107168946\n",
      "Batch 87: loss 0.020633607674604176\n",
      "Batch 88: loss 0.02055792363402857\n",
      "Batch 89: loss 0.020505948860742404\n",
      "Batch 90: loss 0.020581716537061666\n",
      "Batch 91: loss 0.020697766208796056\n",
      "Batch 92: loss 0.020574158523231745\n",
      "Batch 93: loss 0.02074227055474635\n",
      "Batch 94: loss 0.020787941926020258\n",
      "Batch 95: loss 0.020962744323830856\n",
      "Batch 96: loss 0.020928557166674484\n",
      "Batch 97: loss 0.020808705349558407\n",
      "Batch 98: loss 0.02096386736600983\n",
      "Batch 99: loss 0.020866826233087166\n",
      "Batch 100: loss 0.020956527963280677\n",
      "Batch 101: loss 0.02097709973038423\n",
      "Batch 102: loss 0.020881376238357202\n",
      "Batch 103: loss 0.020830164536906098\n",
      "Batch 104: loss 0.020731455243479174\n",
      "Batch 105: loss 0.020608224382712727\n",
      "Batch 106: loss 0.020530971373378667\n",
      "Batch 107: loss 0.0204618150002767\n",
      "Batch 108: loss 0.02035120425366417\n",
      "Batch 109: loss 0.02026166413150249\n",
      "Batch 110: loss 0.020232929628003726\n",
      "Batch 111: loss 0.020152988742936303\n",
      "Batch 112: loss 0.02011542786411675\n",
      "Batch 113: loss 0.020052360214161135\n",
      "Batch 114: loss 0.020004192538755506\n",
      "Batch 115: loss 0.020168000338194162\n",
      "Batch 116: loss 0.020209618803948677\n",
      "Batch 117: loss 0.020214614315101735\n",
      "Batch 118: loss 0.02024361466736359\n",
      "Batch 119: loss 0.020327245151357993\n",
      "Batch 120: loss 0.02029138965687404\n",
      "Batch 121: loss 0.020205583846704525\n",
      "Batch 122: loss 0.02020233409234979\n",
      "Batch 123: loss 0.02013486558647175\n",
      "Batch 124: loss 0.020042896173113296\n",
      "Batch 125: loss 0.020114924110472204\n",
      "Batch 126: loss 0.020306007204843417\n",
      "Batch 127: loss 0.020252816503735508\n",
      "Batch 128: loss 0.020180556522973347\n",
      "Batch 129: loss 0.02026713660434466\n",
      "Batch 130: loss 0.020198076082250247\n",
      "Batch 131: loss 0.020135677952802817\n",
      "Batch 132: loss 0.02021059018531532\n",
      "Batch 133: loss 0.020166193660264623\n",
      "Batch 134: loss 0.02011313489568767\n",
      "Batch 135: loss 0.02010373478686368\n",
      "Batch 136: loss 0.020055540049832093\n",
      "Batch 137: loss 0.0199754523371693\n",
      "Batch 138: loss 0.019913481366213247\n",
      "Batch 139: loss 0.019960240929943623\n",
      "Batch 140: loss 0.01999310637558145\n",
      "Batch 141: loss 0.01996558749427398\n",
      "Batch 142: loss 0.019924633559101904\n",
      "Batch 143: loss 0.01985176045030772\n",
      "Batch 144: loss 0.019817884461695537\n",
      "Batch 145: loss 0.019901804288189964\n",
      "Epoch 7: Training loss 0.019901804288189964\n",
      "Epoch 7: Validation loss 1.0983128309249879 | Accuracy 0.6876 | AUC 0.6717470470274468\n",
      "Batch 1: loss 0.017387768253684044\n",
      "Batch 2: loss 0.021886910311877728\n",
      "Batch 3: loss 0.022691304485003155\n",
      "Batch 4: loss 0.019597425125539303\n",
      "Batch 5: loss 0.018325217254459857\n",
      "Batch 6: loss 0.018201446005453665\n",
      "Batch 7: loss 0.018374804806496416\n",
      "Batch 8: loss 0.018717662314884365\n",
      "Batch 9: loss 0.020125174584488075\n",
      "Batch 10: loss 0.019347280357033016\n",
      "Batch 11: loss 0.01947013855996457\n",
      "Batch 12: loss 0.018934128650774557\n",
      "Batch 13: loss 0.018323714558321696\n",
      "Batch 14: loss 0.017625113150903156\n",
      "Batch 15: loss 0.017231248691678046\n",
      "Batch 16: loss 0.01677476748591289\n",
      "Batch 17: loss 0.01689981565098552\n",
      "Batch 18: loss 0.01744108559149835\n",
      "Batch 19: loss 0.017237126582155104\n",
      "Batch 20: loss 0.017679285397753118\n",
      "Batch 21: loss 0.017314310052565167\n",
      "Batch 22: loss 0.016924169422550636\n",
      "Batch 23: loss 0.016522673586302477\n",
      "Batch 24: loss 0.01622648414922878\n",
      "Batch 25: loss 0.016121427100151776\n",
      "Batch 26: loss 0.015967254294082522\n",
      "Batch 27: loss 0.015890479346530303\n",
      "Batch 28: loss 0.016190820623056164\n",
      "Batch 29: loss 0.01617508155197419\n",
      "Batch 30: loss 0.016639642774437864\n",
      "Batch 31: loss 0.017012827564030886\n",
      "Batch 32: loss 0.01672143275209237\n",
      "Batch 33: loss 0.016873132733797484\n",
      "Batch 34: loss 0.01694348143578014\n",
      "Batch 35: loss 0.016837425569870643\n",
      "Batch 36: loss 0.016678385708170634\n",
      "Batch 37: loss 0.016585605508471664\n",
      "Batch 38: loss 0.016357594230947525\n",
      "Batch 39: loss 0.016179241228084534\n",
      "Batch 40: loss 0.016273682832252234\n",
      "Batch 41: loss 0.016625822033369687\n",
      "Batch 42: loss 0.01690164943491774\n",
      "Batch 43: loss 0.016855908850164608\n",
      "Batch 44: loss 0.01664644471285018\n",
      "Batch 45: loss 0.01650801973624362\n",
      "Batch 46: loss 0.016817960260516924\n",
      "Batch 47: loss 0.016727048467765463\n",
      "Batch 48: loss 0.016901108824337523\n",
      "Batch 49: loss 0.016769677818733817\n",
      "Batch 50: loss 0.016669503711163998\n",
      "Batch 51: loss 0.01655476043621699\n",
      "Batch 52: loss 0.016541916232269544\n",
      "Batch 53: loss 0.016724804697452852\n",
      "Batch 54: loss 0.01659690064412576\n",
      "Batch 55: loss 0.01717987758192149\n",
      "Batch 56: loss 0.017184729255469783\n",
      "Batch 57: loss 0.017353082467850885\n",
      "Batch 58: loss 0.017335927691952937\n",
      "Batch 59: loss 0.017316492526965627\n",
      "Batch 60: loss 0.017261078441515566\n",
      "Batch 61: loss 0.017436791928943065\n",
      "Batch 62: loss 0.017251377835148764\n",
      "Batch 63: loss 0.017515155028492685\n",
      "Batch 64: loss 0.01745240273885429\n",
      "Batch 65: loss 0.017312166639245473\n",
      "Batch 66: loss 0.017187964862607645\n",
      "Batch 67: loss 0.017118607814521042\n",
      "Batch 68: loss 0.017085012500448263\n",
      "Batch 69: loss 0.017366033978760242\n",
      "Batch 70: loss 0.017254020433340754\n",
      "Batch 71: loss 0.01743690024169398\n",
      "Batch 72: loss 0.01752339275036421\n",
      "Batch 73: loss 0.017773580443981574\n",
      "Batch 74: loss 0.017845102032092778\n",
      "Batch 75: loss 0.017808283356328804\n",
      "Batch 76: loss 0.018279512204523934\n",
      "Batch 77: loss 0.018421388134457074\n",
      "Batch 78: loss 0.0184306151424654\n",
      "Batch 79: loss 0.018518780758958076\n",
      "Batch 80: loss 0.01843250875826925\n",
      "Batch 81: loss 0.01833912858992447\n",
      "Batch 82: loss 0.018439316894949938\n",
      "Batch 83: loss 0.018360473092151695\n",
      "Batch 84: loss 0.01859042160434737\n",
      "Batch 85: loss 0.018489116068710298\n",
      "Batch 86: loss 0.018539221578299306\n",
      "Batch 87: loss 0.018461860266738926\n",
      "Batch 88: loss 0.018401974251239815\n",
      "Batch 89: loss 0.01843571640927805\n",
      "Batch 90: loss 0.018463164598991475\n",
      "Batch 91: loss 0.018433011403041228\n",
      "Batch 92: loss 0.018502390690390832\n",
      "Batch 93: loss 0.018400619898031476\n",
      "Batch 94: loss 0.018627938863999667\n",
      "Batch 95: loss 0.018560140021145345\n",
      "Batch 96: loss 0.018447714067103032\n",
      "Batch 97: loss 0.01837115964765862\n",
      "Batch 98: loss 0.018321873425334995\n",
      "Batch 99: loss 0.01829350683744056\n",
      "Batch 100: loss 0.018215636392123997\n",
      "Batch 101: loss 0.018243462183872367\n",
      "Batch 102: loss 0.018256463352408187\n",
      "Batch 103: loss 0.01821132779609665\n",
      "Batch 104: loss 0.01816391180573891\n",
      "Batch 105: loss 0.018119672947518883\n",
      "Batch 106: loss 0.018234621641173395\n",
      "Batch 107: loss 0.0183161226727904\n",
      "Batch 108: loss 0.018304066245306144\n",
      "Batch 109: loss 0.018296071622999163\n",
      "Batch 110: loss 0.018192432609132746\n",
      "Batch 111: loss 0.018120289897663636\n",
      "Batch 112: loss 0.018214128720241467\n",
      "Batch 113: loss 0.01819070309103854\n",
      "Batch 114: loss 0.018120623904427414\n",
      "Batch 115: loss 0.01802044976502657\n",
      "Batch 116: loss 0.01797047171516922\n",
      "Batch 117: loss 0.017882390313933037\n",
      "Batch 118: loss 0.01787948106431355\n",
      "Batch 119: loss 0.018174999213519217\n",
      "Batch 120: loss 0.01841859777147571\n",
      "Batch 121: loss 0.018422858096843908\n",
      "Batch 122: loss 0.018777365322972906\n",
      "Batch 123: loss 0.018801540499780237\n",
      "Batch 124: loss 0.01881046848551881\n",
      "Batch 125: loss 0.01879864850640297\n",
      "Batch 126: loss 0.0187101075869231\n",
      "Batch 127: loss 0.018700985941947915\n",
      "Batch 128: loss 0.018807985718012787\n",
      "Batch 129: loss 0.018735427719851334\n",
      "Batch 130: loss 0.01866259928792715\n",
      "Batch 131: loss 0.01867324342766336\n",
      "Batch 132: loss 0.018669260581108658\n",
      "Batch 133: loss 0.01865597590243906\n",
      "Batch 134: loss 0.018568418892239456\n",
      "Batch 135: loss 0.018472650026281675\n",
      "Batch 136: loss 0.018477757913334406\n",
      "Batch 137: loss 0.018588086617362762\n",
      "Batch 138: loss 0.018648863891544548\n",
      "Batch 139: loss 0.01858106383504413\n",
      "Batch 140: loss 0.01850615993275174\n",
      "Batch 141: loss 0.01851577592472024\n",
      "Batch 142: loss 0.018458475155467297\n",
      "Batch 143: loss 0.01846791051614743\n",
      "Batch 144: loss 0.018622678038405463\n",
      "Batch 145: loss 0.01856379355959166\n",
      "Epoch 8: Training loss 0.01856379355959166\n",
      "Epoch 8: Validation loss 1.8531980037689209 | Accuracy 0.6236 | AUC 0.6782128335003957\n",
      "Batch 1: loss 0.012110787443816662\n",
      "Batch 2: loss 0.00960318767465651\n",
      "Batch 3: loss 0.010315160422275463\n",
      "Batch 4: loss 0.010172419832088053\n",
      "Batch 5: loss 0.010547335352748632\n",
      "Batch 6: loss 0.010326132100696364\n",
      "Batch 7: loss 0.011672480497509241\n",
      "Batch 8: loss 0.012931291188579053\n",
      "Batch 9: loss 0.014400761729727188\n",
      "Batch 10: loss 0.014671514136716723\n",
      "Batch 11: loss 0.013968210117044773\n",
      "Batch 12: loss 0.013800120175195238\n",
      "Batch 13: loss 0.013670820850305833\n",
      "Batch 14: loss 0.013200357057420271\n",
      "Batch 15: loss 0.01279801931232214\n",
      "Batch 16: loss 0.013827146089170128\n",
      "Batch 17: loss 0.013855165637591305\n",
      "Batch 18: loss 0.013899972144928243\n",
      "Batch 19: loss 0.013839961225657086\n",
      "Batch 20: loss 0.013945072749629616\n",
      "Batch 21: loss 0.013949583816741194\n",
      "Batch 22: loss 0.013950030234726992\n",
      "Batch 23: loss 0.0147486865196539\n",
      "Batch 24: loss 0.014851263569047054\n",
      "Batch 25: loss 0.014758879020810127\n",
      "Batch 26: loss 0.015223871343410932\n",
      "Batch 27: loss 0.014872852868090073\n",
      "Batch 28: loss 0.014981809221873326\n",
      "Batch 29: loss 0.015860909290997118\n",
      "Batch 30: loss 0.01591939344070852\n",
      "Batch 31: loss 0.01567356118692025\n",
      "Batch 32: loss 0.01611667782708537\n",
      "Batch 33: loss 0.01601639614355835\n",
      "Batch 34: loss 0.016086155699346873\n",
      "Batch 35: loss 0.016523857348199403\n",
      "Batch 36: loss 0.01657049180681093\n",
      "Batch 37: loss 0.016565613587059685\n",
      "Batch 38: loss 0.016596169541835\n",
      "Batch 39: loss 0.016460907203742325\n",
      "Batch 40: loss 0.016673101263586433\n",
      "Batch 41: loss 0.01680814834856769\n",
      "Batch 42: loss 0.01664156541006551\n",
      "Batch 43: loss 0.016569674350754467\n",
      "Batch 44: loss 0.016846120008267462\n",
      "Batch 45: loss 0.016680654603987932\n",
      "Batch 46: loss 0.016518657024868804\n",
      "Batch 47: loss 0.01637359790781394\n",
      "Batch 48: loss 0.016461153737812612\n",
      "Batch 49: loss 0.016523112780509556\n",
      "Batch 50: loss 0.01649907753802836\n",
      "Batch 51: loss 0.01653312410538395\n",
      "Batch 52: loss 0.016607097036635075\n",
      "Batch 53: loss 0.01709282402616908\n",
      "Batch 54: loss 0.016991435110362038\n",
      "Batch 55: loss 0.017003341738811947\n",
      "Batch 56: loss 0.016843614742226367\n",
      "Batch 57: loss 0.016783115821645447\n",
      "Batch 58: loss 0.016620981258115376\n",
      "Batch 59: loss 0.016608374864178693\n",
      "Batch 60: loss 0.01658330901991576\n",
      "Batch 61: loss 0.016672978765468617\n",
      "Batch 62: loss 0.017051251973175714\n",
      "Batch 63: loss 0.017516872897330258\n",
      "Batch 64: loss 0.017465294637077022\n",
      "Batch 65: loss 0.01739339383605581\n",
      "Batch 66: loss 0.017259574671642797\n",
      "Batch 67: loss 0.017100773174871704\n",
      "Batch 68: loss 0.0172778111677069\n",
      "Batch 69: loss 0.0173444127380524\n",
      "Batch 70: loss 0.017597383027896284\n",
      "Batch 71: loss 0.017663363924800928\n",
      "Batch 72: loss 0.017636527753590297\n",
      "Batch 73: loss 0.01783235149764239\n",
      "Batch 74: loss 0.017889187123777496\n",
      "Batch 75: loss 0.017870254845668874\n",
      "Batch 76: loss 0.01830370781183439\n",
      "Batch 77: loss 0.018784624639317973\n",
      "Batch 78: loss 0.018658492206715237\n",
      "Batch 79: loss 0.01864287438154032\n",
      "Batch 80: loss 0.01858811604906805\n",
      "Batch 81: loss 0.018623975490760288\n",
      "Batch 82: loss 0.018517880887928898\n",
      "Batch 83: loss 0.01843014107568257\n",
      "Batch 84: loss 0.018385019413905128\n",
      "Batch 85: loss 0.018362111505120994\n",
      "Batch 86: loss 0.01831271495605104\n",
      "Batch 87: loss 0.018246342347742153\n",
      "Batch 88: loss 0.018321831853509964\n",
      "Batch 89: loss 0.01824611398715819\n",
      "Batch 90: loss 0.018175080320280458\n",
      "Batch 91: loss 0.018342281707081493\n",
      "Batch 92: loss 0.01844175342652623\n",
      "Batch 93: loss 0.018523290417888152\n",
      "Batch 94: loss 0.01841969820274476\n",
      "Batch 95: loss 0.01833337117476683\n",
      "Batch 96: loss 0.018253093716339208\n",
      "Batch 97: loss 0.01838053733628896\n",
      "Batch 98: loss 0.01848957991246514\n",
      "Batch 99: loss 0.018445002286685535\n",
      "Batch 100: loss 0.018447769372723997\n",
      "Batch 101: loss 0.01846564440794363\n",
      "Batch 102: loss 0.018431677944118194\n",
      "Batch 103: loss 0.01851939605729817\n",
      "Batch 104: loss 0.018601594891291685\n",
      "Batch 105: loss 0.01856211768463254\n",
      "Batch 106: loss 0.018521476330516755\n",
      "Batch 107: loss 0.018577664050438973\n",
      "Batch 108: loss 0.01852707469542683\n",
      "Batch 109: loss 0.0184320328936596\n",
      "Batch 110: loss 0.01836444119584154\n",
      "Batch 111: loss 0.018552959842087182\n",
      "Batch 112: loss 0.018603202857775614\n",
      "Batch 113: loss 0.018540324820452824\n",
      "Batch 114: loss 0.01848846448078882\n",
      "Batch 115: loss 0.018675208929926156\n",
      "Batch 116: loss 0.018588394637155378\n",
      "Batch 117: loss 0.01853376615426352\n",
      "Batch 118: loss 0.018491293244474266\n",
      "Batch 119: loss 0.018402727606448056\n",
      "Batch 120: loss 0.018400706121853242\n",
      "Batch 121: loss 0.01842049041701373\n",
      "Batch 122: loss 0.018359951365769643\n",
      "Batch 123: loss 0.018380015554678875\n",
      "Batch 124: loss 0.018449033681874075\n",
      "Batch 125: loss 0.018456969525665046\n",
      "Batch 126: loss 0.018466994059198195\n",
      "Batch 127: loss 0.018399450682308964\n",
      "Batch 128: loss 0.018379823701252462\n",
      "Batch 129: loss 0.01840214200621081\n",
      "Batch 130: loss 0.01854632646203614\n",
      "Batch 131: loss 0.01848489223474651\n",
      "Batch 132: loss 0.01843713614454662\n",
      "Batch 133: loss 0.018397779417525333\n",
      "Batch 134: loss 0.018572699525324043\n",
      "Batch 135: loss 0.018512047078736403\n",
      "Batch 136: loss 0.018526452124420115\n",
      "Batch 137: loss 0.018505860524537572\n",
      "Batch 138: loss 0.018435904469368034\n",
      "Batch 139: loss 0.018441028367031178\n",
      "Batch 140: loss 0.018506924605130086\n",
      "Batch 141: loss 0.01844232377153656\n",
      "Batch 142: loss 0.01835922721419221\n",
      "Batch 143: loss 0.01836096670257149\n",
      "Batch 144: loss 0.01830717003839608\n",
      "Batch 145: loss 0.018229236153468102\n",
      "Epoch 9: Training loss 0.018229236153468102\n",
      "Epoch 9: Validation loss 1.7796597480773926 | Accuracy 0.624 | AUC 0.6824106790595985\n",
      "Batch 1: loss 0.00581900542601943\n",
      "Batch 2: loss 0.010877216001972556\n",
      "Batch 3: loss 0.012280269681165615\n",
      "Batch 4: loss 0.011671534623019397\n",
      "Batch 5: loss 0.012343340646475553\n",
      "Batch 6: loss 0.01132407559392353\n",
      "Batch 7: loss 0.012387201921748263\n",
      "Batch 8: loss 0.01321145537076518\n",
      "Batch 9: loss 0.013180508019609584\n",
      "Batch 10: loss 0.015369321452453732\n",
      "Batch 11: loss 0.014731983611868187\n",
      "Batch 12: loss 0.01456983647464464\n",
      "Batch 13: loss 0.014175304319136418\n",
      "Batch 14: loss 0.014548931969329715\n",
      "Batch 15: loss 0.013977554099013408\n",
      "Batch 16: loss 0.013977969909319654\n",
      "Batch 17: loss 0.013699680155910113\n",
      "Batch 18: loss 0.013915249275871448\n",
      "Batch 19: loss 0.014556728916144684\n",
      "Batch 20: loss 0.014847779669798911\n",
      "Batch 21: loss 0.01465986937373167\n",
      "Batch 22: loss 0.014884795845401559\n",
      "Batch 23: loss 0.01516107633790892\n",
      "Batch 24: loss 0.015943438144555937\n",
      "Batch 25: loss 0.01611521365121007\n",
      "Batch 26: loss 0.01571358416157846\n",
      "Batch 27: loss 0.015375797188392392\n",
      "Batch 28: loss 0.015270039472462875\n",
      "Batch 29: loss 0.015870168194945515\n",
      "Batch 30: loss 0.015950393645713726\n",
      "Batch 31: loss 0.01575406054936109\n",
      "Batch 32: loss 0.01566900289617479\n",
      "Batch 33: loss 0.016360953788865696\n",
      "Batch 34: loss 0.0161193040856982\n",
      "Batch 35: loss 0.016022648901811667\n",
      "Batch 36: loss 0.015816541492111154\n",
      "Batch 37: loss 0.01591633099156457\n",
      "Batch 38: loss 0.01579659914990005\n",
      "Batch 39: loss 0.015638254415721465\n",
      "Batch 40: loss 0.015592765109613538\n",
      "Batch 41: loss 0.015596212728357896\n",
      "Batch 42: loss 0.01574795183149122\n",
      "Batch 43: loss 0.01568462067218714\n",
      "Batch 44: loss 0.015556728467345238\n",
      "Batch 45: loss 0.015325835295435455\n",
      "Batch 46: loss 0.01515744293473013\n",
      "Batch 47: loss 0.015037353853992325\n",
      "Batch 48: loss 0.015051780782717591\n",
      "Batch 49: loss 0.01508009579146699\n",
      "Batch 50: loss 0.014967538146302105\n",
      "Batch 51: loss 0.014898814270090238\n",
      "Batch 52: loss 0.015011264629160555\n",
      "Batch 53: loss 0.014961879867357466\n",
      "Batch 54: loss 0.01504247144933928\n",
      "Batch 55: loss 0.015111243614757602\n",
      "Batch 56: loss 0.015206451818812639\n",
      "Batch 57: loss 0.015109704080315535\n",
      "Batch 58: loss 0.015040229949779037\n",
      "Batch 59: loss 0.015024041196615514\n",
      "Batch 60: loss 0.014960460232881207\n",
      "Batch 61: loss 0.015116108695930634\n",
      "Batch 62: loss 0.015196442536468948\n",
      "Batch 63: loss 0.015102654688119416\n",
      "Batch 64: loss 0.014980657426349353\n",
      "Batch 65: loss 0.01498959969299344\n",
      "Batch 66: loss 0.014877113972254323\n",
      "Batch 67: loss 0.014951131438641851\n",
      "Batch 68: loss 0.014948876019950737\n",
      "Batch 69: loss 0.015038970068259083\n",
      "Batch 70: loss 0.015476947563833424\n",
      "Batch 71: loss 0.015379591261259687\n",
      "Batch 72: loss 0.015413331572845992\n",
      "Batch 73: loss 0.015439919955160928\n",
      "Batch 74: loss 0.015398874960688723\n",
      "Batch 75: loss 0.015369153562933207\n",
      "Batch 76: loss 0.015354163950848343\n",
      "Batch 77: loss 0.01525498056397229\n",
      "Batch 78: loss 0.015295316518929142\n",
      "Batch 79: loss 0.015430718902026928\n",
      "Batch 80: loss 0.015446155314566567\n",
      "Batch 81: loss 0.015467933810281533\n",
      "Batch 82: loss 0.015386356977836751\n",
      "Batch 83: loss 0.01529948687719473\n",
      "Batch 84: loss 0.01518992725981488\n",
      "Batch 85: loss 0.015062268719296244\n",
      "Batch 86: loss 0.014996581272287078\n",
      "Batch 87: loss 0.014988546789591682\n",
      "Batch 88: loss 0.014979112009644847\n",
      "Batch 89: loss 0.0149355026202674\n",
      "Batch 90: loss 0.015002407614762585\n",
      "Batch 91: loss 0.014929841482336377\n",
      "Batch 92: loss 0.014877010425350265\n",
      "Batch 93: loss 0.014822210871203933\n",
      "Batch 94: loss 0.014776470045499663\n",
      "Batch 95: loss 0.014830327744742758\n",
      "Batch 96: loss 0.0148823449514263\n",
      "Batch 97: loss 0.014987882173898601\n",
      "Batch 98: loss 0.015066846278591119\n",
      "Batch 99: loss 0.014954198180048756\n",
      "Batch 100: loss 0.01497701964341104\n",
      "Batch 101: loss 0.014878284495028824\n",
      "Batch 102: loss 0.01485122496937858\n",
      "Batch 103: loss 0.01478648463770458\n",
      "Batch 104: loss 0.014942619694361033\n",
      "Batch 105: loss 0.014909261132457427\n",
      "Batch 106: loss 0.014896650425211157\n",
      "Batch 107: loss 0.014939049567818363\n",
      "Batch 108: loss 0.014878760113518822\n",
      "Batch 109: loss 0.014843922462997906\n",
      "Batch 110: loss 0.014819588211619042\n",
      "Batch 111: loss 0.014919972873653646\n",
      "Batch 112: loss 0.014899683480117736\n",
      "Batch 113: loss 0.014832271745148222\n",
      "Batch 114: loss 0.014791507588438037\n",
      "Batch 115: loss 0.01478046418937004\n",
      "Batch 116: loss 0.014756847176989862\n",
      "Batch 117: loss 0.014756819579559259\n",
      "Batch 118: loss 0.01474461608597914\n",
      "Batch 119: loss 0.014688869511724269\n",
      "Batch 120: loss 0.014670865148461113\n",
      "Batch 121: loss 0.01469118610286146\n",
      "Batch 122: loss 0.014739595281082343\n",
      "Batch 123: loss 0.014757573790091082\n",
      "Batch 124: loss 0.014702057244346266\n",
      "Batch 125: loss 0.014735180113464594\n",
      "Batch 126: loss 0.014716413001426391\n",
      "Batch 127: loss 0.014744261672030988\n",
      "Batch 128: loss 0.014677511568152113\n",
      "Batch 129: loss 0.014622889827330445\n",
      "Batch 130: loss 0.01462131103930565\n",
      "Batch 131: loss 0.01460492426088748\n",
      "Batch 132: loss 0.014599076422834487\n",
      "Batch 133: loss 0.014532262683474928\n",
      "Batch 134: loss 0.014508936978153773\n",
      "Batch 135: loss 0.01459461247065553\n",
      "Batch 136: loss 0.014517114768215619\n",
      "Batch 137: loss 0.014516190914641115\n",
      "Batch 138: loss 0.014541666343997138\n",
      "Batch 139: loss 0.014535584222364555\n",
      "Batch 140: loss 0.014488905744760165\n",
      "Batch 141: loss 0.01446607782578109\n",
      "Batch 142: loss 0.014483508676893905\n",
      "Batch 143: loss 0.01446696468856889\n",
      "Batch 144: loss 0.014400700783072453\n",
      "Batch 145: loss 0.014400110386255965\n",
      "Epoch 10: Training loss 0.014400110386255965\n",
      "Epoch 10: Validation loss 1.27510507106781 | Accuracy 0.6958 | AUC 0.6773538058327172\n",
      "Best epoch:  6\n"
     ]
    }
   ],
   "source": [
    "mtrainer.run_train(10, support_loader, query_loader, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.best_model.state_dict(), 'models/backbone/pretrained/vindr2/trained-backbone-weights.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3af20c5b7ad2810593c71dd4ba5b7d473da7f58b181d32c1c7ac42846aa4b248"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
