{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import medmnist\n",
    "from medmnist import INFO\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "# chestmnist, retinamnist\n",
    "def get_image_mean_std(dataname):\n",
    "    info = INFO[dataname]\n",
    "    DataClass = getattr(medmnist, info['python_class'])\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Resize((224, 224))\n",
    "            ])\n",
    "\n",
    "    train_dataset = DataClass(split='train', transform=transform, download=True)\n",
    "\n",
    "    train_loader = data.DataLoader(dataset=train_dataset, batch_size=8192)\n",
    "    total = info['n_samples']['train']\n",
    "    mean = torch.zeros(info['n_channels'])\n",
    "    std = torch.zeros(info['n_channels'])\n",
    "    for images, _ in train_loader:\n",
    "        num_img = len(images)\n",
    "        m, s = images.mean([0,2,3]), images.std([0,2,3])\n",
    "        mean += num_img * m / total\n",
    "        std += np.sqrt(num_img/total) * s\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/naomileow/.medmnist/pathmnist.npz\n",
      "pathmnist tensor([0.7405, 0.5330, 0.7058]) tensor([0.3920, 0.5636, 0.3959])\n",
      "Downloading https://zenodo.org/record/6496656/files/chestmnist.npz?download=1 to /Users/naomileow/.medmnist/chestmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a4e26d9d65463885431a1112f1c12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82802576 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chestmnist tensor([0.4936]) tensor([0.7392])\n",
      "Downloading https://zenodo.org/record/6496656/files/dermamnist.npz?download=1 to /Users/naomileow/.medmnist/dermamnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a9f387e2264897bdff7ae73a39c3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19725078 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dermamnist tensor([0.7631, 0.5381, 0.5614]) tensor([0.1354, 0.1530, 0.1679])\n",
      "Downloading https://zenodo.org/record/6496656/files/octmnist.npz?download=1 to /Users/naomileow/.medmnist/octmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7097f8dabc6248908b4ff79621ca9174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54938180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "octmnist tensor([0.1889]) tensor([0.6606])\n",
      "Downloading https://zenodo.org/record/6496656/files/pneumoniamnist.npz?download=1 to /Users/naomileow/.medmnist/pneumoniamnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0ae89427524cf7806d3cbcd5289d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4170669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pneumoniamnist tensor([0.5719]) tensor([0.1651])\n",
      "Downloading https://zenodo.org/record/6496656/files/retinamnist.npz?download=1 to /Users/naomileow/.medmnist/retinamnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f60fc6b2d63460bb3a2196718b78112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3291041 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retinamnist tensor([0.3984, 0.2447, 0.1558]) tensor([0.2952, 0.1970, 0.1470])\n",
      "Downloading https://zenodo.org/record/6496656/files/breastmnist.npz?download=1 to /Users/naomileow/.medmnist/breastmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e47eaa870494073b514256c59621848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/559580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breastmnist tensor([0.3276]) tensor([0.2027])\n",
      "Downloading https://zenodo.org/record/6496656/files/bloodmnist.npz?download=1 to /Users/naomileow/.medmnist/bloodmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b123ec2919ca4a54ba8645f6c203c062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35461855 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bloodmnist tensor([0.7943, 0.6597, 0.6962]) tensor([0.2930, 0.3292, 0.1541])\n",
      "Downloading https://zenodo.org/record/6496656/files/tissuemnist.npz?download=1 to /Users/naomileow/.medmnist/tissuemnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f937abd55417400ca0f8ab5a41cc9cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124962739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tissuemnist tensor([0.1020]) tensor([0.4443])\n",
      "Downloading https://zenodo.org/record/6496656/files/organamnist.npz?download=1 to /Users/naomileow/.medmnist/organamnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd87d128acb4dabae5ec87edc43dc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/38247903 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organamnist tensor([0.4678]) tensor([0.6105])\n",
      "Downloading https://zenodo.org/record/6496656/files/organcmnist.npz?download=1 to /Users/naomileow/.medmnist/organcmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91efcfa71b4f495a9e83122e8df95ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15527535 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organcmnist tensor([0.4932]) tensor([0.3762])\n",
      "Downloading https://zenodo.org/record/6496656/files/organsmnist.npz?download=1 to /Users/naomileow/.medmnist/organsmnist.npz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a442275b7a495d81bd3d380182f0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16528536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organsmnist tensor([0.4950]) tensor([0.3779])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdecb9172087411d972fa4e036525a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32657407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_stds = {}\n",
    "\n",
    "for k in ['pathmnist', 'chestmnist', 'dermamnist', 'octmnist', 'pneumoniamnist', 'retinamnist', 'breastmnist', 'bloodmnist', 'tissuemnist', 'organamnist', 'organcmnist', 'organsmnist']:\n",
    "    mean, std = get_image_mean_std(k)\n",
    "    print(k, mean, std)\n",
    "    mean_stds[k] = {\n",
    "        'mean': mean,\n",
    "        'std': std\n",
    "    }\n",
    "print(mean_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.device import get_device\n",
    "from models.backbone.datasets import MEAN_STDS, DataSets\n",
    "from models.backbone.trainer import Trainer\n",
    "\n",
    "device = get_device()\n",
    "MODEL_SAVE_PATH = 'models/backbone/pretrained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/naomileow/.medmnist/retinamnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/retinamnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/retinamnist.npz\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torch import nn\n",
    "\n",
    "retina_ds = DataSets('retinamnist', MEAN_STDS) # 5 classes\n",
    "backbone = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "backbone.fc = nn.Linear(2048, len(retina_ds.info['label']))\n",
    "\n",
    "batch_size = 256\n",
    "rtrainer = Trainer(backbone, retina_ds, batch_size, device, balance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtrainer.run_train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.52, 0.7557038049792416, 1.3845270824432374)\n"
     ]
    }
   ],
   "source": [
    "print(rtrainer.run_eval(rtrainer.best_model, rtrainer.test_loader)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# (0.5125, 0.7247683647010932, 1.464751205444336) for non pretrained, non balanced\n",
    "print(rtrainer.run_eval(rtrainer.best_model, rtrainer.test_loader)) \n",
    "\n",
    "torch.save(rtrainer.best_model.state_dict(), os.path.join(MODEL_SAVE_PATH, 'retina_backbone_pretrained_bal.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "# 'chestmnist', 'pneumoniamnist', 'octmnist',  'retinamnist'\n",
    "chest_ds = DataSets('chestmnist', MEAN_STDS)\n",
    "\n",
    "backbone = torchvision.models.resnet50(num_classes=len(chest_ds.info['label']), pretrained=False)\n",
    "# patch for single channel\n",
    "backbone.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "batch_size = 128\n",
    "ctrainer = Trainer(backbone, chest_ds, batch_size, device)\n",
    "\n",
    "ctrainer.run_train(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n",
      "Using downloaded and verified file: /Users/naomileow/.medmnist/chestmnist.npz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# [7996,  1950,  9261, 13914,  3988,  4375,   978,  3705,  3263, 1690,  1799,  1158,  2279,   144]\n",
    "chest_ds = DataSets('chestmnist', mean_stds)\n",
    "\n",
    "backbone = torchvision.models.resnet50(num_classes=len(chest_ds.info['label']), weights=None)\n",
    "backbone.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "backbone.load_state_dict(torch.load(os.path.join(MODEL_SAVE_PATH, 'cxr_backbone.pkl')))\n",
    "\n",
    "batch_size = 256\n",
    "ctrainer = Trainer(backbone, chest_ds, batch_size, device, balance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrainer.run_train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9239799784755875, 0.6329840270919405, 0.7261211995067128)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctrainer.run_eval(ctrainer.model, ctrainer.test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and save pretrained resnet from medclip\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from medclip import MedCLIPModel, MedCLIPVisionModel\n",
    "\n",
    "# load MedCLIP-ResNet50\n",
    "model = MedCLIPModel(vision_cls=MedCLIPVisionModel)\n",
    "model.from_pretrained()\n",
    "\n",
    "bconv_weight = model.vision_model.model.conv1.weight.mean(dim=1).unsqueeze(1)\n",
    "\n",
    "# The resnet model was trained on CheXpert and MIMIC-CXR\n",
    "backbone = model.vision_model.model\n",
    "backbone.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "backbone.conv1.weight = nn.Parameter(bconv_weight)\n",
    "\n",
    "torch.save(backbone.state_dict(), 'medclip_resnet50.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.labels import VINDR_CXR_LABELS, VINDR_SPLIT, VINDR_SPLIT2\n",
    "from models.backbone.datasets import MEAN_STDS\n",
    "\n",
    "from utils.data import get_query_and_support_ids, DatasetConfig\n",
    "from utils.device import get_device\n",
    "from models.embedding.dataset import Dataset\n",
    "from utils.sampling import MultilabelBalancedRandomSampler\n",
    "\n",
    "configs = {\n",
    "    'vindr2': DatasetConfig('datasets/vindr-cxr-png', 'data/vindr_cxr_split_labels2.pkl', 'data/vindr_train_query_set2.pkl', VINDR_CXR_LABELS, VINDR_SPLIT2, MEAN_STDS['chestmnist'])\n",
    "}\n",
    "\n",
    "config = configs['vindr2']\n",
    "\n",
    "batch_size = 10*10\n",
    "query_image_ids, support_image_ids = get_query_and_support_ids(config.img_info, config.training_split_path)\n",
    "query_dataset = Dataset(config.img_path, config.img_info, query_image_ids, config.label_names_map, config.classes_split_map['train'], mean_std=config.mean_std)\n",
    "query_loader = DataLoader(dataset=query_dataset, batch_size=batch_size, shuffle=True)\n",
    "support_dataset = Dataset(config.img_path, config.img_info, support_image_ids, config.label_names_map, config.classes_split_map['train'], mean_std=config.mean_std)\n",
    "support_loader = DataLoader(dataset=support_dataset, batch_size=batch_size, sampler=MultilabelBalancedRandomSampler(support_dataset.get_class_indicators()))\n",
    "\n",
    "PROJ_SIZE = 512\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import os\n",
    "import torch\n",
    "from models.backbone.trainer import DSTrainer\n",
    "from utils.f1_loss import BalAccuracyLoss\n",
    "\n",
    "backbone = torchvision.models.resnet50(num_classes=len(config.classes_split_map['train']), weights=None)\n",
    "backbone.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "mtrainer = DSTrainer(backbone, query_dataset.class_labels(), criterion=BalAccuracyLoss(), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.8178021907806396 | F1 0.3351583778858185 | AUC 0.602855014089035 | Specificity 0.7272143363952637 | Recall 0.4031338691711426 | Bal Acc 0.5651741027832031\n",
      "Loss 0.8040915727615356 | F1 0.32222121953964233 | AUC 0.5738897543349903 | Specificity 0.7176041603088379 | Recall 0.3890199661254883 | Bal Acc 0.5533120632171631\n",
      "Loss 0.8150610327720642 | F1 0.32819268107414246 | AUC 0.5927963819758275 | Specificity 0.685369610786438 | Recall 0.41243231296539307 | Bal Acc 0.5489009618759155\n",
      "Loss 0.8141124248504639 | F1 0.31385940313339233 | AUC 0.6090060357042646 | Specificity 0.7192613482475281 | Recall 0.3877924680709839 | Bal Acc 0.5535268783569336\n",
      "Loss 0.8236879706382751 | F1 0.3152325451374054 | AUC 0.5647615005108276 | Specificity 0.7081069946289062 | Recall 0.37620580196380615 | Bal Acc 0.5421563982963562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8149510383605957,\n",
       " tensor(0.3229, device='mps:0'),\n",
       " 0.588661737322989,\n",
       " 0.7115112900733948,\n",
       " 0.3937168836593628)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.model, query_loader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.7730595469474792 | F1 0.4065409302711487 | AUC 0.6377244623793866 | Specificity 0.6742991209030151 | Recall 0.5058967471122742 | Bal Acc 0.5900979042053223\n",
      "Loss 0.792277455329895 | F1 0.38331103324890137 | AUC 0.6266736220474719 | Specificity 0.6769896745681763 | Recall 0.5041444301605225 | Bal Acc 0.5905670523643494\n",
      "Loss 0.8309187889099121 | F1 0.3456721305847168 | AUC 0.6039829723119572 | Specificity 0.656503438949585 | Recall 0.47546499967575073 | Bal Acc 0.5659842491149902\n",
      "Loss 0.7993811368942261 | F1 0.3438308537006378 | AUC 0.5979420461276035 | Specificity 0.6408977508544922 | Recall 0.469146728515625 | Bal Acc 0.5550222396850586\n",
      "Loss 0.7731005549430847 | F1 0.3598782420158386 | AUC 0.6240732889015435 | Specificity 0.6538779735565186 | Recall 0.48356109857559204 | Bal Acc 0.5687195062637329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7937474966049194,\n",
       " tensor(0.3678, device='mps:0'),\n",
       " 0.6180792783535926,\n",
       " 0.6605135917663574,\n",
       " 0.48764280080795286)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtrainer.run_eval(mtrainer.best_model, query_loader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: loss 0.8280101418495178\n",
      "Batch 2: loss 0.8292435109615326\n",
      "Batch 3: loss 0.8295965592066447\n",
      "Batch 4: loss 0.8288757503032684\n",
      "Batch 5: loss 0.8278212785720825\n",
      "Batch 6: loss 0.8269070088863373\n",
      "Batch 7: loss 0.8257842574800763\n",
      "Batch 8: loss 0.8247404843568802\n",
      "Batch 9: loss 0.8234745727645026\n",
      "Batch 10: loss 0.8221038401126861\n",
      "Batch 11: loss 0.8206689737059853\n",
      "Batch 12: loss 0.8191321740547816\n",
      "Batch 13: loss 0.81712773671517\n",
      "Batch 14: loss 0.8156357535294124\n",
      "Batch 15: loss 0.8143048286437988\n",
      "Batch 16: loss 0.8137635849416256\n",
      "Batch 17: loss 0.8123951484175289\n",
      "Batch 18: loss 0.811598002910614\n",
      "Batch 19: loss 0.8101978678452341\n",
      "Batch 20: loss 0.8089416712522507\n",
      "Batch 21: loss 0.8080876100630987\n",
      "Batch 22: loss 0.8072474815628745\n",
      "Batch 23: loss 0.806087763413139\n",
      "Batch 24: loss 0.8052229334910711\n",
      "Batch 25: loss 0.8045683622360229\n",
      "Batch 26: loss 0.8032535773057204\n",
      "Batch 27: loss 0.8025501524960553\n",
      "Batch 28: loss 0.8007806433098656\n",
      "Batch 29: loss 0.7994909224839046\n",
      "Batch 30: loss 0.7983721097310384\n",
      "Batch 31: loss 0.79701147156377\n",
      "Batch 32: loss 0.7958965953439474\n",
      "Batch 33: loss 0.7945986859726183\n",
      "Batch 34: loss 0.793084079728407\n",
      "Batch 35: loss 0.7916559934616089\n",
      "Batch 36: loss 0.7900184194246928\n",
      "Batch 37: loss 0.7889131017633386\n",
      "Batch 38: loss 0.7881304213875219\n",
      "Batch 39: loss 0.7868080689356878\n",
      "Batch 40: loss 0.7856034576892853\n",
      "Batch 41: loss 0.7842499395696129\n",
      "Batch 42: loss 0.783442097050803\n",
      "Batch 43: loss 0.7831817394079164\n",
      "Batch 44: loss 0.7821919132362712\n",
      "Batch 45: loss 0.7816567818323771\n",
      "Batch 46: loss 0.7805460963560187\n",
      "Batch 47: loss 0.779925119369588\n",
      "Batch 48: loss 0.7788156233727932\n",
      "Batch 49: loss 0.7781625827964471\n",
      "Batch 50: loss 0.7775459516048432\n",
      "Batch 51: loss 0.776460073742212\n",
      "Batch 52: loss 0.7754861288345777\n",
      "Batch 53: loss 0.7739950879564825\n",
      "Batch 54: loss 0.772862704815688\n",
      "Batch 55: loss 0.7718309976837852\n",
      "Batch 56: loss 0.7708466915147645\n",
      "Batch 57: loss 0.7695110984015883\n",
      "Batch 58: loss 0.7683622919279953\n",
      "Batch 59: loss 0.7674392946695877\n",
      "Batch 60: loss 0.7662882814804713\n",
      "Batch 61: loss 0.7652157965253611\n",
      "Batch 62: loss 0.7640267793209322\n",
      "Batch 63: loss 0.7630819735072908\n",
      "Batch 64: loss 0.7621604269370437\n",
      "Batch 65: loss 0.7610459263508137\n",
      "Batch 66: loss 0.7603938787272482\n",
      "Batch 67: loss 0.759382769243041\n",
      "Batch 68: loss 0.7584159830037285\n",
      "Batch 69: loss 0.7571869328402091\n",
      "Batch 70: loss 0.7563213501657758\n",
      "Batch 71: loss 0.7553062052793906\n",
      "Batch 72: loss 0.7542922298113505\n",
      "Batch 73: loss 0.7532480840813623\n",
      "Batch 74: loss 0.7521979623549694\n",
      "Batch 75: loss 0.7509410341580709\n",
      "Batch 76: loss 0.7498083044039575\n",
      "Batch 77: loss 0.7489334832538258\n",
      "Batch 78: loss 0.7475725603409302\n",
      "Batch 79: loss 0.7464825284631946\n",
      "Batch 80: loss 0.7453421972692013\n",
      "Batch 81: loss 0.7443059940397003\n",
      "Batch 82: loss 0.7433945245859099\n",
      "Batch 83: loss 0.742133551333324\n",
      "Batch 84: loss 0.7410687917754764\n",
      "Batch 85: loss 0.7403424319098978\n",
      "Batch 86: loss 0.7394853677860526\n",
      "Batch 87: loss 0.7381546319216147\n",
      "Batch 88: loss 0.7370725761760365\n",
      "Batch 89: loss 0.7361498414800408\n",
      "Batch 90: loss 0.7349580751525031\n",
      "Batch 91: loss 0.7342197056655045\n",
      "Batch 92: loss 0.7332195833973263\n",
      "Batch 93: loss 0.7321124397298341\n",
      "Batch 94: loss 0.7311245900519351\n",
      "Batch 95: loss 0.730227666152151\n",
      "Batch 96: loss 0.7292899073412021\n",
      "Batch 97: loss 0.7282657494250032\n",
      "Batch 98: loss 0.7274968922138214\n",
      "Batch 99: loss 0.7265430128935612\n",
      "Batch 100: loss 0.7256313294172287\n",
      "Batch 101: loss 0.7247646840492098\n",
      "Batch 102: loss 0.7239870177764519\n",
      "Batch 103: loss 0.7229946186241595\n",
      "Batch 104: loss 0.7221220748928877\n",
      "Batch 105: loss 0.7214254407655625\n",
      "Batch 106: loss 0.7205058679265796\n",
      "Batch 107: loss 0.7199303140150053\n",
      "Batch 108: loss 0.7191791407488011\n",
      "Batch 109: loss 0.7183212862102264\n",
      "Batch 110: loss 0.7174556071108038\n",
      "Batch 111: loss 0.716591612712757\n",
      "Batch 112: loss 0.7160257888691766\n",
      "Batch 113: loss 0.7151599911461889\n",
      "Batch 114: loss 0.7144057165112412\n",
      "Batch 115: loss 0.7138720688612564\n",
      "Batch 116: loss 0.7131521835409361\n",
      "Batch 117: loss 0.712290989028083\n",
      "Batch 118: loss 0.7115023449315863\n",
      "Batch 119: loss 0.7108082375606569\n",
      "Batch 120: loss 0.7098142758011818\n",
      "Batch 121: loss 0.7090970716200584\n",
      "Batch 122: loss 0.7085381296814465\n",
      "Batch 123: loss 0.7078744095515429\n",
      "Batch 124: loss 0.7073507376255528\n",
      "Batch 125: loss 0.7065953769683838\n",
      "Batch 126: loss 0.705964984401824\n",
      "Batch 127: loss 0.7051060448481342\n",
      "Batch 128: loss 0.7043604110367596\n",
      "Batch 129: loss 0.7037457278532575\n",
      "Batch 130: loss 0.7031135187699245\n",
      "Batch 131: loss 0.7024223190227538\n",
      "Batch 132: loss 0.7020187992038149\n",
      "Batch 133: loss 0.7014109012775851\n",
      "Batch 134: loss 0.7008831509903296\n",
      "Batch 135: loss 0.7002244763904147\n",
      "Batch 136: loss 0.6996202604735599\n",
      "Batch 137: loss 0.6989567867160713\n",
      "Batch 138: loss 0.6984256868777068\n",
      "Batch 139: loss 0.697809429906255\n",
      "Batch 140: loss 0.697237566113472\n",
      "Batch 141: loss 0.6966738409184395\n",
      "Batch 142: loss 0.6961367772498601\n",
      "Batch 143: loss 0.6955186682981211\n",
      "Batch 144: loss 0.6948863305151463\n",
      "Batch 145: loss 0.6941795321644796\n",
      "Epoch 1: Training loss 0.6941795321644796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torch/functional.py:799: UserWarning: MPS: _unique2 op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performace implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Unique.mm:354.)\n",
      "  output, inverse_indices, counts = torch._unique2(\n",
      "/opt/homebrew/anaconda3/envs/cs6240/lib/python3.9/site-packages/torchmetrics/utilities/compute.py:52: UserWarning: MPS: nonzero op is supported natively starting from macOS 13.0. Falling back on CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Indexing.mm:218.)\n",
      "  denom[denom == 0.0] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation loss 0.7927254676818848 | F1 0.43386778235435486 | AUC 0.5992680664118906 | Acc H-Mean 0.5316094630482393\n",
      "Batch 1: loss 0.6033168435096741\n",
      "Batch 2: loss 0.6056607067584991\n",
      "Batch 3: loss 0.6068258484204611\n",
      "Batch 4: loss 0.6138665080070496\n",
      "Batch 5: loss 0.6156848430633545\n",
      "Batch 6: loss 0.6145555178324381\n",
      "Batch 7: loss 0.6182503274508885\n",
      "Batch 8: loss 0.6162194386124611\n",
      "Batch 9: loss 0.6142563356293572\n",
      "Batch 10: loss 0.6154501378536225\n",
      "Batch 11: loss 0.6160964153029702\n",
      "Batch 12: loss 0.616858571767807\n",
      "Batch 13: loss 0.61766189795274\n",
      "Batch 14: loss 0.6189858402524676\n",
      "Batch 15: loss 0.6176013469696044\n",
      "Batch 16: loss 0.6182028129696846\n",
      "Batch 17: loss 0.6190365973640891\n",
      "Batch 18: loss 0.6184763179885017\n",
      "Batch 19: loss 0.6181828661968833\n",
      "Batch 20: loss 0.616915363073349\n",
      "Batch 21: loss 0.6158973035358247\n",
      "Batch 22: loss 0.6156895973465659\n",
      "Batch 23: loss 0.6152722550475079\n",
      "Batch 24: loss 0.6155804693698883\n",
      "Batch 25: loss 0.6153893566131592\n",
      "Batch 26: loss 0.6165638336768517\n",
      "Batch 27: loss 0.6167089188540423\n",
      "Batch 28: loss 0.6163827436310905\n",
      "Batch 29: loss 0.6158546702615146\n",
      "Batch 30: loss 0.6166962186495463\n",
      "Batch 31: loss 0.6159402497353093\n",
      "Batch 32: loss 0.6160445939749479\n",
      "Batch 33: loss 0.6153409065622272\n",
      "Batch 34: loss 0.6145552694797516\n",
      "Batch 35: loss 0.6143772942679269\n",
      "Batch 36: loss 0.6141821212238736\n",
      "Batch 37: loss 0.6142469483452875\n",
      "Batch 38: loss 0.6139443704956457\n",
      "Batch 39: loss 0.6140099672170786\n",
      "Batch 40: loss 0.6137259572744369\n",
      "Batch 41: loss 0.6136364587923375\n",
      "Batch 42: loss 0.6132870728061313\n",
      "Batch 43: loss 0.6127460418745528\n",
      "Batch 44: loss 0.6124380501833829\n",
      "Batch 45: loss 0.6122901863522\n",
      "Batch 46: loss 0.6122022232283717\n",
      "Batch 47: loss 0.6120198039298362\n",
      "Batch 48: loss 0.6119223982095718\n",
      "Batch 49: loss 0.6115072880472455\n",
      "Batch 50: loss 0.6112581026554108\n",
      "Batch 51: loss 0.6110852337351033\n",
      "Batch 52: loss 0.6110518288153869\n",
      "Batch 53: loss 0.6112280870383641\n",
      "Batch 54: loss 0.6108545075964045\n",
      "Batch 55: loss 0.6106871182268316\n",
      "Batch 56: loss 0.6104369418961662\n",
      "Batch 57: loss 0.6102544403912729\n",
      "Batch 58: loss 0.6101803461025501\n",
      "Batch 59: loss 0.610316705905785\n",
      "Batch 60: loss 0.6101861586173375\n",
      "Batch 61: loss 0.609382978228272\n",
      "Batch 62: loss 0.6092620174730977\n",
      "Batch 63: loss 0.6090044795520722\n",
      "Batch 64: loss 0.6089672045782208\n",
      "Batch 65: loss 0.6087158487393306\n",
      "Batch 66: loss 0.6081482867399851\n",
      "Batch 67: loss 0.6077255766783187\n",
      "Batch 68: loss 0.6073509042753893\n",
      "Batch 69: loss 0.6074316648469456\n",
      "Batch 70: loss 0.6074094542435238\n",
      "Batch 71: loss 0.6074229839821936\n",
      "Batch 72: loss 0.6072256192564964\n",
      "Batch 73: loss 0.6072777134098418\n",
      "Batch 74: loss 0.6071223858240489\n",
      "Batch 75: loss 0.6070591004689535\n",
      "Batch 76: loss 0.606757829063817\n",
      "Batch 77: loss 0.6064363578697304\n",
      "Batch 78: loss 0.606598898386344\n",
      "Batch 79: loss 0.606349065333982\n",
      "Batch 80: loss 0.6062847688794136\n",
      "Batch 81: loss 0.6062155152544563\n",
      "Batch 82: loss 0.6058670355052482\n",
      "Batch 83: loss 0.6058013913143112\n",
      "Batch 84: loss 0.6058537151132312\n",
      "Batch 85: loss 0.6057936556199017\n",
      "Batch 86: loss 0.6057777820631515\n",
      "Batch 87: loss 0.6057532792803885\n",
      "Batch 88: loss 0.6055132800882513\n",
      "Batch 89: loss 0.6056370132424859\n",
      "Batch 90: loss 0.6055738899442885\n",
      "Batch 91: loss 0.6055589861922211\n",
      "Batch 92: loss 0.6055885410827139\n",
      "Batch 93: loss 0.605368552669402\n",
      "Batch 94: loss 0.6053088618085739\n",
      "Batch 95: loss 0.6051599458644265\n",
      "Batch 96: loss 0.6050288385401169\n",
      "Batch 97: loss 0.6048582165511613\n",
      "Batch 98: loss 0.6047355398839834\n",
      "Batch 99: loss 0.6046669675846292\n",
      "Batch 100: loss 0.6045626282691956\n",
      "Batch 101: loss 0.6044707227461409\n",
      "Batch 102: loss 0.6043952095742319\n",
      "Batch 103: loss 0.6043861502582587\n",
      "Batch 104: loss 0.6041982047832929\n",
      "Batch 105: loss 0.6038554129146394\n",
      "Batch 106: loss 0.6037606973693056\n",
      "Batch 107: loss 0.6034893995133516\n",
      "Batch 108: loss 0.603306601996775\n",
      "Batch 109: loss 0.6032217440255191\n",
      "Batch 110: loss 0.6031609649007971\n",
      "Batch 111: loss 0.6031622585949598\n",
      "Batch 112: loss 0.602843909391335\n",
      "Batch 113: loss 0.6025879704846745\n",
      "Batch 114: loss 0.6025101071909854\n",
      "Batch 115: loss 0.6023183439088904\n",
      "Batch 116: loss 0.6021392371120124\n",
      "Batch 117: loss 0.6020229815417885\n",
      "Batch 118: loss 0.6018963511717521\n",
      "Batch 119: loss 0.601811791668419\n",
      "Batch 120: loss 0.6016766975323359\n",
      "Batch 121: loss 0.6016243646952731\n",
      "Batch 122: loss 0.6015564916563816\n",
      "Batch 123: loss 0.6015691582749529\n",
      "Batch 124: loss 0.6015779116461354\n",
      "Batch 125: loss 0.601408447265625\n",
      "Batch 126: loss 0.6013262309725322\n",
      "Batch 127: loss 0.6013032334057364\n",
      "Batch 128: loss 0.6010962673462927\n",
      "Batch 129: loss 0.6010794450146283\n",
      "Batch 130: loss 0.6009805188729213\n",
      "Batch 131: loss 0.6009461292783722\n",
      "Batch 132: loss 0.6009570915590633\n",
      "Batch 133: loss 0.6008246742693105\n",
      "Batch 134: loss 0.6008314143365888\n",
      "Batch 135: loss 0.6006005622722485\n",
      "Batch 136: loss 0.6004699345897225\n",
      "Batch 137: loss 0.6003803297551009\n",
      "Batch 138: loss 0.6001448393731877\n",
      "Batch 139: loss 0.6000200128383774\n",
      "Batch 140: loss 0.5998933485576085\n",
      "Batch 141: loss 0.5997442023973938\n",
      "Batch 142: loss 0.5996098585531745\n",
      "Batch 143: loss 0.5995583901038537\n",
      "Batch 144: loss 0.5994426980614662\n",
      "Batch 145: loss 0.599275989324671\n",
      "Epoch 2: Training loss 0.599275989324671\n",
      "Epoch 2: Validation loss 0.7720002651214599 | F1 0.4100073277950287 | AUC 0.59092091185928 | Acc H-Mean 0.5562976793971529\n",
      "Batch 1: loss 0.5884237289428711\n",
      "Batch 2: loss 0.5849114656448364\n",
      "Batch 3: loss 0.587498148282369\n",
      "Batch 4: loss 0.5789836347103119\n",
      "Batch 5: loss 0.5772708892822266\n",
      "Batch 6: loss 0.5793018142382304\n",
      "Batch 7: loss 0.5792764680726188\n",
      "Batch 8: loss 0.5814457833766937\n",
      "Batch 9: loss 0.5811653004752265\n",
      "Batch 10: loss 0.5816425800323486\n",
      "Batch 11: loss 0.5830387093804099\n",
      "Batch 12: loss 0.583669568101565\n",
      "Batch 13: loss 0.5839104606555059\n",
      "Batch 14: loss 0.5848661363124847\n",
      "Batch 15: loss 0.5854914704958598\n",
      "Batch 16: loss 0.5842514485120773\n",
      "Batch 17: loss 0.5850512630799237\n",
      "Batch 18: loss 0.5856286552217271\n",
      "Batch 19: loss 0.5858374143901625\n",
      "Batch 20: loss 0.5865491151809692\n",
      "Batch 21: loss 0.5856695487385705\n",
      "Batch 22: loss 0.5854784168980338\n",
      "Batch 23: loss 0.5851321764614271\n",
      "Batch 24: loss 0.5843411187330881\n",
      "Batch 25: loss 0.5845284557342529\n",
      "Batch 26: loss 0.5847169023293716\n",
      "Batch 27: loss 0.5851099976786861\n",
      "Batch 28: loss 0.5852951279708317\n",
      "Batch 29: loss 0.5852333944419335\n",
      "Batch 30: loss 0.5848879814147949\n",
      "Batch 31: loss 0.5847364241077054\n",
      "Batch 32: loss 0.5843822732567787\n",
      "Batch 33: loss 0.5842332153609304\n",
      "Batch 34: loss 0.5845827845966115\n",
      "Batch 35: loss 0.5850059883935111\n",
      "Batch 36: loss 0.5846028692192502\n",
      "Batch 37: loss 0.5843701040422594\n",
      "Batch 38: loss 0.5839860298131642\n",
      "Batch 39: loss 0.5844289996685126\n",
      "Batch 40: loss 0.5842131420969963\n",
      "Batch 41: loss 0.5842400033299516\n",
      "Batch 42: loss 0.5843090556916737\n",
      "Batch 43: loss 0.5844418281732604\n",
      "Batch 44: loss 0.5844370776956732\n",
      "Batch 45: loss 0.5850073522991605\n",
      "Batch 46: loss 0.5848366099855175\n",
      "Batch 47: loss 0.5846111470080436\n",
      "Batch 48: loss 0.5848638738195101\n",
      "Batch 49: loss 0.5847602048698737\n",
      "Batch 50: loss 0.5845051681995392\n",
      "Batch 51: loss 0.5845029412531385\n",
      "Batch 52: loss 0.5845408428173798\n",
      "Batch 53: loss 0.584382545273259\n",
      "Batch 54: loss 0.5840986658025671\n",
      "Batch 55: loss 0.5840516133741899\n",
      "Batch 56: loss 0.5838606261781284\n",
      "Batch 57: loss 0.5837364939221165\n",
      "Batch 58: loss 0.5838535088917305\n",
      "Batch 59: loss 0.5837495367405778\n",
      "Batch 60: loss 0.5836794753869374\n",
      "Batch 61: loss 0.5833802389316871\n",
      "Batch 62: loss 0.5834745562845661\n",
      "Batch 63: loss 0.583848656169952\n",
      "Batch 64: loss 0.5836926158517599\n",
      "Batch 65: loss 0.5833352538255545\n",
      "Batch 66: loss 0.5832747952504591\n",
      "Batch 67: loss 0.5831893637998781\n",
      "Batch 68: loss 0.5829553647952921\n",
      "Batch 69: loss 0.5832065473432126\n",
      "Batch 70: loss 0.5833161830902099\n",
      "Batch 71: loss 0.5833975429266272\n",
      "Batch 72: loss 0.5835111257102754\n",
      "Batch 73: loss 0.5832988954570195\n",
      "Batch 74: loss 0.5833365627237268\n",
      "Batch 75: loss 0.5833311494191488\n",
      "Batch 76: loss 0.5833217838877126\n",
      "Batch 77: loss 0.5831486226676347\n",
      "Batch 78: loss 0.582864693342111\n",
      "Batch 79: loss 0.5826900314681137\n",
      "Batch 80: loss 0.5824845679104328\n",
      "Batch 81: loss 0.5824207516364109\n",
      "Batch 82: loss 0.5823770143636843\n",
      "Batch 83: loss 0.5824453184403569\n",
      "Batch 84: loss 0.5823427665801275\n",
      "Batch 85: loss 0.5823389179566327\n",
      "Batch 86: loss 0.5824025100053742\n",
      "Batch 87: loss 0.5825570417546678\n",
      "Batch 88: loss 0.5826811370524493\n",
      "Batch 89: loss 0.5824937164113763\n",
      "Batch 90: loss 0.5822816961341434\n",
      "Batch 91: loss 0.5822615106027205\n",
      "Batch 92: loss 0.582079242752946\n",
      "Batch 93: loss 0.5818334849931861\n",
      "Batch 94: loss 0.5817745118699176\n",
      "Batch 95: loss 0.5817710424724378\n",
      "Batch 96: loss 0.5817169913401207\n",
      "Batch 97: loss 0.581551808057372\n",
      "Batch 98: loss 0.5815257399666066\n",
      "Batch 99: loss 0.581350200706058\n",
      "Batch 100: loss 0.5812388449907303\n",
      "Batch 101: loss 0.5811993509235949\n",
      "Batch 102: loss 0.5810103825494355\n",
      "Batch 103: loss 0.5808659444734888\n",
      "Batch 104: loss 0.5807456586223382\n",
      "Batch 105: loss 0.5808497128032503\n",
      "Batch 106: loss 0.5808976495041037\n",
      "Batch 107: loss 0.5808691276568119\n",
      "Batch 108: loss 0.5809774211159459\n",
      "Batch 109: loss 0.5808672708108884\n",
      "Batch 110: loss 0.5807512283325196\n",
      "Batch 111: loss 0.5807575857317125\n",
      "Batch 112: loss 0.5806970096060208\n",
      "Batch 113: loss 0.5806358472435875\n",
      "Batch 114: loss 0.5806248198475754\n",
      "Batch 115: loss 0.5806328752766485\n",
      "Batch 116: loss 0.5805981688458344\n",
      "Batch 117: loss 0.5805454289811289\n",
      "Batch 118: loss 0.5803620072744661\n",
      "Batch 119: loss 0.5802782278100983\n",
      "Batch 120: loss 0.5801663503050805\n",
      "Batch 121: loss 0.5802393752681322\n",
      "Batch 122: loss 0.5801655957933332\n",
      "Batch 123: loss 0.5799962563243338\n",
      "Batch 124: loss 0.5800037960852346\n",
      "Batch 125: loss 0.5800262565612793\n",
      "Batch 126: loss 0.5798672541739449\n",
      "Batch 127: loss 0.5798644146581334\n",
      "Batch 128: loss 0.5799429845064878\n",
      "Batch 129: loss 0.5799227102782375\n",
      "Batch 130: loss 0.5799040335875291\n",
      "Batch 131: loss 0.5799393926868002\n",
      "Batch 132: loss 0.5797970475572528\n",
      "Batch 133: loss 0.5797971361561826\n",
      "Batch 134: loss 0.5797267529501844\n",
      "Batch 135: loss 0.579617879125807\n",
      "Batch 136: loss 0.5797075475840008\n",
      "Batch 137: loss 0.579734786148489\n",
      "Batch 138: loss 0.5796422137730364\n",
      "Batch 139: loss 0.579652929048744\n",
      "Batch 140: loss 0.5796994026218142\n",
      "Batch 141: loss 0.5796258927237058\n",
      "Batch 142: loss 0.5795270607505046\n",
      "Batch 143: loss 0.5795807721731546\n",
      "Batch 144: loss 0.5794878080487251\n",
      "Batch 145: loss 0.5794494262385603\n",
      "Epoch 3: Training loss 0.5794494262385603\n",
      "Epoch 3: Validation loss 0.8211418628692627 | F1 0.37576228380203247 | AUC 0.6045405732322808 | Acc H-Mean 0.5352099344065009\n",
      "Batch 1: loss 0.5659241676330566\n",
      "Batch 2: loss 0.5637920796871185\n",
      "Batch 3: loss 0.5645185708999634\n",
      "Batch 4: loss 0.5716875493526459\n",
      "Batch 5: loss 0.572589886188507\n",
      "Batch 6: loss 0.5729900300502777\n",
      "Batch 7: loss 0.5725285070283073\n",
      "Batch 8: loss 0.5722314044833183\n",
      "Batch 9: loss 0.5724506510628594\n",
      "Batch 10: loss 0.5717029571533203\n",
      "Batch 11: loss 0.5706278735941107\n",
      "Batch 12: loss 0.5704824427763621\n",
      "Batch 13: loss 0.5694939585832449\n",
      "Batch 14: loss 0.5696497687271663\n",
      "Batch 15: loss 0.5697989424069723\n",
      "Batch 16: loss 0.5710104182362556\n",
      "Batch 17: loss 0.5711567016208873\n",
      "Batch 18: loss 0.5712457531028323\n",
      "Batch 19: loss 0.5717926903774864\n",
      "Batch 20: loss 0.5721125304698944\n",
      "Batch 21: loss 0.5714361383801415\n",
      "Batch 22: loss 0.5718027028170499\n",
      "Batch 23: loss 0.5725500765054122\n",
      "Batch 24: loss 0.5724508836865425\n",
      "Batch 25: loss 0.572338936328888\n",
      "Batch 26: loss 0.5728153746861678\n",
      "Batch 27: loss 0.5727938700605322\n",
      "Batch 28: loss 0.5731772908142635\n",
      "Batch 29: loss 0.5731722342556921\n",
      "Batch 30: loss 0.5735218346118927\n",
      "Batch 31: loss 0.5738531062679906\n",
      "Batch 32: loss 0.5738005675375462\n",
      "Batch 33: loss 0.573071812138413\n",
      "Batch 34: loss 0.5735517862965079\n",
      "Batch 35: loss 0.5735705052103315\n",
      "Batch 36: loss 0.5736715975734923\n",
      "Batch 37: loss 0.5735224469287975\n",
      "Batch 38: loss 0.5735409902898889\n",
      "Batch 39: loss 0.5734648475280175\n",
      "Batch 40: loss 0.5730565801262856\n",
      "Batch 41: loss 0.5733792956282453\n",
      "Batch 42: loss 0.573393912542434\n",
      "Batch 43: loss 0.5735776396684868\n",
      "Batch 44: loss 0.5738449801098217\n",
      "Batch 45: loss 0.5739360160297817\n",
      "Batch 46: loss 0.5736552658288375\n",
      "Batch 47: loss 0.5738259832909767\n",
      "Batch 48: loss 0.573645698527495\n",
      "Batch 49: loss 0.5738527835631857\n",
      "Batch 50: loss 0.5739354574680329\n",
      "Batch 51: loss 0.5738178035792183\n",
      "Batch 52: loss 0.5735789273793881\n",
      "Batch 53: loss 0.5733735032801358\n",
      "Batch 54: loss 0.5733405031539776\n",
      "Batch 55: loss 0.5731740290468389\n",
      "Batch 56: loss 0.5732027132596288\n",
      "Batch 57: loss 0.5728105777188351\n",
      "Batch 58: loss 0.5725988992329302\n",
      "Batch 59: loss 0.5726884825754974\n",
      "Batch 60: loss 0.5729940632979075\n",
      "Batch 61: loss 0.5729426243266121\n",
      "Batch 62: loss 0.5730163685737117\n",
      "Batch 63: loss 0.5728564111013261\n",
      "Batch 64: loss 0.5729602705687284\n",
      "Batch 65: loss 0.5730287625239445\n",
      "Batch 66: loss 0.5729818488612319\n",
      "Batch 67: loss 0.5730513654538055\n",
      "Batch 68: loss 0.5730857498505536\n",
      "Batch 69: loss 0.5729279984598574\n",
      "Batch 70: loss 0.5732648304530552\n",
      "Batch 71: loss 0.5735758056103344\n",
      "Batch 72: loss 0.5733920170201195\n",
      "Batch 73: loss 0.5736016492321067\n",
      "Batch 74: loss 0.5736978899788212\n",
      "Batch 75: loss 0.5735501837730408\n",
      "Batch 76: loss 0.5735955175600553\n",
      "Batch 77: loss 0.5734525059724783\n",
      "Batch 78: loss 0.5734568597414554\n",
      "Batch 79: loss 0.5734827737264996\n",
      "Batch 80: loss 0.573437912017107\n",
      "Batch 81: loss 0.5734374471652655\n",
      "Batch 82: loss 0.5731189999638534\n",
      "Batch 83: loss 0.5730190585894757\n",
      "Batch 84: loss 0.5729442217520305\n",
      "Batch 85: loss 0.5729004979133606\n",
      "Batch 86: loss 0.5728465041448904\n",
      "Batch 87: loss 0.57282035378204\n",
      "Batch 88: loss 0.5730063833973624\n",
      "Batch 89: loss 0.5731437407182843\n",
      "Batch 90: loss 0.5730252769258287\n",
      "Batch 91: loss 0.573081781575968\n",
      "Batch 92: loss 0.573275229205256\n",
      "Batch 93: loss 0.5730839967727661\n",
      "Batch 94: loss 0.5729243970931844\n",
      "Batch 95: loss 0.572868862277583\n",
      "Batch 96: loss 0.5728954176108042\n",
      "Batch 97: loss 0.5729687349083498\n",
      "Batch 98: loss 0.5730241792542594\n",
      "Batch 99: loss 0.5730156013459871\n",
      "Batch 100: loss 0.5732363599538803\n",
      "Batch 101: loss 0.573444393011603\n",
      "Batch 102: loss 0.5735851789222044\n",
      "Batch 103: loss 0.5736718247237714\n",
      "Batch 104: loss 0.5737276025689565\n",
      "Batch 105: loss 0.573835848058973\n",
      "Batch 106: loss 0.5738682431994744\n",
      "Batch 107: loss 0.5738944444700936\n",
      "Batch 108: loss 0.5737759763443911\n",
      "Batch 109: loss 0.5737758694438759\n",
      "Batch 110: loss 0.5735254108905792\n",
      "Batch 111: loss 0.573525211295566\n",
      "Batch 112: loss 0.5735737197101116\n",
      "Batch 113: loss 0.5734545672889304\n",
      "Batch 114: loss 0.5733781096182371\n",
      "Batch 115: loss 0.573385052577309\n",
      "Batch 116: loss 0.573376736250417\n",
      "Batch 117: loss 0.5732944526224055\n",
      "Batch 118: loss 0.5733070979684086\n",
      "Batch 119: loss 0.5732022168255654\n",
      "Batch 120: loss 0.5730908001462619\n",
      "Batch 121: loss 0.5730698773683596\n",
      "Batch 122: loss 0.5730411884237508\n",
      "Batch 123: loss 0.5730922246366982\n",
      "Batch 124: loss 0.5731199460644876\n",
      "Batch 125: loss 0.5729248380661011\n",
      "Batch 126: loss 0.5729442967308892\n",
      "Batch 127: loss 0.5728937622130387\n",
      "Batch 128: loss 0.5728598218411207\n",
      "Batch 129: loss 0.5727820340977159\n",
      "Batch 130: loss 0.5727704121516302\n",
      "Batch 131: loss 0.5727612130514538\n",
      "Batch 132: loss 0.5726899870417335\n",
      "Batch 133: loss 0.5726824833038158\n",
      "Batch 134: loss 0.5726538024731537\n",
      "Batch 135: loss 0.5727584812376234\n",
      "Batch 136: loss 0.5727954182554694\n",
      "Batch 137: loss 0.5728097066391994\n",
      "Batch 138: loss 0.5727436175380928\n",
      "Batch 139: loss 0.5726977925506427\n",
      "Batch 140: loss 0.5726009645632335\n",
      "Batch 141: loss 0.5727610668392046\n",
      "Batch 142: loss 0.572710008268625\n",
      "Batch 143: loss 0.5726333960786566\n",
      "Batch 144: loss 0.572603404108021\n",
      "Batch 145: loss 0.5726499894696744\n",
      "Epoch 4: Training loss 0.5726499894696744\n",
      "Epoch 4: Validation loss 0.8223626255989075 | F1 0.3325158953666687 | AUC 0.6048675124856415 | Acc H-Mean 0.5114072288765371\n",
      "Batch 1: loss 0.5795871019363403\n",
      "Batch 2: loss 0.5737550854682922\n",
      "Batch 3: loss 0.5716900825500488\n",
      "Batch 4: loss 0.5699563473463058\n",
      "Batch 5: loss 0.5702655911445618\n",
      "Batch 6: loss 0.5673551062742869\n",
      "Batch 7: loss 0.5673397353717259\n",
      "Batch 8: loss 0.5674723014235497\n",
      "Batch 9: loss 0.5686581532160441\n",
      "Batch 10: loss 0.5681586503982544\n",
      "Batch 11: loss 0.5667654817754572\n",
      "Batch 12: loss 0.5678381472826004\n",
      "Batch 13: loss 0.565291744012099\n",
      "Batch 14: loss 0.5647056358201163\n",
      "Batch 15: loss 0.5646183570226033\n",
      "Batch 16: loss 0.5656688846647739\n",
      "Batch 17: loss 0.5669855510487276\n",
      "Batch 18: loss 0.5660708745320638\n",
      "Batch 19: loss 0.5659302284843043\n",
      "Batch 20: loss 0.5663364142179489\n",
      "Batch 21: loss 0.5664828476451692\n",
      "Batch 22: loss 0.5671741122549231\n",
      "Batch 23: loss 0.5679005669510883\n",
      "Batch 24: loss 0.568219356238842\n",
      "Batch 25: loss 0.568047616481781\n",
      "Batch 26: loss 0.5680914360743302\n",
      "Batch 27: loss 0.5673908017299794\n",
      "Batch 28: loss 0.5673293194600514\n",
      "Batch 29: loss 0.5664789615006283\n",
      "Batch 30: loss 0.5662324885527293\n",
      "Batch 31: loss 0.5661409689534095\n",
      "Batch 32: loss 0.5657172948122025\n",
      "Batch 33: loss 0.5651210257501313\n",
      "Batch 34: loss 0.5645196507958805\n",
      "Batch 35: loss 0.5647407327379499\n",
      "Batch 36: loss 0.5654604865445031\n",
      "Batch 37: loss 0.5661209660607416\n",
      "Batch 38: loss 0.5656333189261588\n",
      "Batch 39: loss 0.5663918715256911\n",
      "Batch 40: loss 0.5660803467035294\n",
      "Batch 41: loss 0.5663792418270577\n",
      "Batch 42: loss 0.566580403418768\n",
      "Batch 43: loss 0.566994592200878\n",
      "Batch 44: loss 0.5665986023165963\n",
      "Batch 45: loss 0.5664330985811021\n",
      "Batch 46: loss 0.5661299539648968\n",
      "Batch 47: loss 0.5660477369389636\n",
      "Batch 48: loss 0.5662255262335142\n",
      "Batch 49: loss 0.5663821818877239\n",
      "Batch 50: loss 0.5664650750160217\n",
      "Batch 51: loss 0.5667064353531482\n",
      "Batch 52: loss 0.5670771873914279\n",
      "Batch 53: loss 0.5673824469998198\n",
      "Batch 54: loss 0.5672226029413717\n",
      "Batch 55: loss 0.5672936287793247\n",
      "Batch 56: loss 0.5674131949033056\n",
      "Batch 57: loss 0.5674601782832229\n",
      "Batch 58: loss 0.5675717458642763\n",
      "Batch 59: loss 0.5674002443329763\n",
      "Batch 60: loss 0.5674972762664159\n",
      "Batch 61: loss 0.5677057504653931\n",
      "Batch 62: loss 0.5675559889885687\n",
      "Batch 63: loss 0.5673592204139346\n",
      "Batch 64: loss 0.5674210842698812\n",
      "Batch 65: loss 0.567521570279048\n",
      "Batch 66: loss 0.5676346403179746\n",
      "Batch 67: loss 0.5676027082685214\n",
      "Batch 68: loss 0.5674585849046707\n",
      "Batch 69: loss 0.5675239658010178\n",
      "Batch 70: loss 0.5674130312034062\n",
      "Batch 71: loss 0.5674142610858863\n",
      "Batch 72: loss 0.5674945190548897\n",
      "Batch 73: loss 0.5673768528520244\n",
      "Batch 74: loss 0.5673022342694772\n",
      "Batch 75: loss 0.5675646869341532\n",
      "Batch 76: loss 0.5677439342988165\n",
      "Batch 77: loss 0.5679272250695662\n",
      "Batch 78: loss 0.5679807418431991\n",
      "Batch 79: loss 0.5681796647325347\n",
      "Batch 80: loss 0.5682204365730286\n",
      "Batch 81: loss 0.5680587814177995\n",
      "Batch 82: loss 0.5682546496391296\n",
      "Batch 83: loss 0.5683344056807369\n",
      "Batch 84: loss 0.5682968710150037\n",
      "Batch 85: loss 0.5684167728704564\n",
      "Batch 86: loss 0.5686252228049344\n",
      "Batch 87: loss 0.5686535807861679\n",
      "Batch 88: loss 0.5686595053835348\n",
      "Batch 89: loss 0.5685034782698984\n",
      "Batch 90: loss 0.5683891514937083\n",
      "Batch 91: loss 0.5682080021271338\n",
      "Batch 92: loss 0.5680693498124247\n",
      "Batch 93: loss 0.5681124662840238\n",
      "Batch 94: loss 0.5679179461712532\n",
      "Batch 95: loss 0.5680334812716434\n",
      "Batch 96: loss 0.5682611310233673\n",
      "Batch 97: loss 0.5682876687688926\n",
      "Batch 98: loss 0.5683920164497531\n",
      "Batch 99: loss 0.5685487082510283\n",
      "Batch 100: loss 0.5684709358215332\n",
      "Batch 101: loss 0.5686737818293052\n",
      "Batch 102: loss 0.5690875252087911\n",
      "Batch 103: loss 0.5687824719160506\n",
      "Batch 104: loss 0.5689767014521819\n",
      "Batch 105: loss 0.5688226001603263\n",
      "Batch 106: loss 0.5689399934039926\n",
      "Batch 107: loss 0.5689040422439575\n",
      "Batch 108: loss 0.5690899701030167\n",
      "Batch 109: loss 0.5689669103797422\n",
      "Batch 110: loss 0.5687568035992709\n",
      "Batch 111: loss 0.5687682113131961\n",
      "Batch 112: loss 0.5686959070818765\n",
      "Batch 113: loss 0.5687036218896376\n",
      "Batch 114: loss 0.5685360703551978\n",
      "Batch 115: loss 0.5685309363448101\n",
      "Batch 116: loss 0.5685662488485205\n",
      "Batch 117: loss 0.5685122843481537\n",
      "Batch 118: loss 0.5684466993404647\n",
      "Batch 119: loss 0.5684174830172243\n",
      "Batch 120: loss 0.568502954641978\n",
      "Batch 121: loss 0.5683722328548589\n",
      "Batch 122: loss 0.5683148894153658\n",
      "Batch 123: loss 0.5683412711794783\n",
      "Batch 124: loss 0.5683021184898192\n",
      "Batch 125: loss 0.5683152899742127\n",
      "Batch 126: loss 0.5683585187745472\n",
      "Batch 127: loss 0.5682849442864967\n",
      "Batch 128: loss 0.568307900801301\n",
      "Batch 129: loss 0.5684844255447388\n",
      "Batch 130: loss 0.5685108221494235\n",
      "Batch 131: loss 0.5683968048969298\n",
      "Batch 132: loss 0.5684176656332883\n",
      "Batch 133: loss 0.5683812440786147\n",
      "Batch 134: loss 0.5684677420267418\n",
      "Batch 135: loss 0.5685404031365006\n",
      "Batch 136: loss 0.5685366361456758\n",
      "Batch 137: loss 0.5684818960454342\n",
      "Batch 138: loss 0.5684378764767578\n",
      "Batch 139: loss 0.5685089243401726\n",
      "Batch 140: loss 0.5685597802911486\n",
      "Batch 141: loss 0.5685698411143418\n",
      "Batch 142: loss 0.568583111947691\n",
      "Batch 143: loss 0.5686193437843056\n",
      "Batch 144: loss 0.5685105588701036\n",
      "Batch 145: loss 0.5685120394458419\n",
      "Epoch 5: Training loss 0.5685120394458419\n",
      "Epoch 5: Validation loss 0.7949973583221436 | F1 0.3645921051502228 | AUC 0.611632257211361 | Acc H-Mean 0.5361870366483419\n",
      "Batch 1: loss 0.5762497186660767\n",
      "Batch 2: loss 0.5650600492954254\n",
      "Batch 3: loss 0.5658166408538818\n",
      "Batch 4: loss 0.5657927691936493\n",
      "Batch 5: loss 0.569123101234436\n",
      "Batch 6: loss 0.5652871131896973\n",
      "Batch 7: loss 0.5694532224110195\n",
      "Batch 8: loss 0.5701740086078644\n",
      "Batch 9: loss 0.5722547504636977\n",
      "Batch 10: loss 0.5728851675987243\n",
      "Batch 11: loss 0.5704839121211659\n",
      "Batch 12: loss 0.5712510943412781\n",
      "Batch 13: loss 0.5711777210235596\n",
      "Batch 14: loss 0.5713127340589251\n",
      "Batch 15: loss 0.570868444442749\n",
      "Batch 16: loss 0.5702133923768997\n",
      "Batch 17: loss 0.5705418165992288\n",
      "Batch 18: loss 0.5708545313941108\n",
      "Batch 19: loss 0.5712191054695531\n",
      "Batch 20: loss 0.5706112861633301\n",
      "Batch 21: loss 0.5702440114248366\n",
      "Batch 22: loss 0.5701824914325367\n",
      "Batch 23: loss 0.5693674968636554\n",
      "Batch 24: loss 0.5699032098054886\n",
      "Batch 25: loss 0.5697727775573731\n",
      "Batch 26: loss 0.5695365117146418\n",
      "Batch 27: loss 0.5696690612369113\n",
      "Batch 28: loss 0.5701596822057452\n",
      "Batch 29: loss 0.5700945648653754\n",
      "Batch 30: loss 0.5693508724371592\n",
      "Batch 31: loss 0.5698167374057155\n",
      "Batch 32: loss 0.5689147543162107\n",
      "Batch 33: loss 0.5683806827574065\n",
      "Batch 34: loss 0.5683968347661635\n",
      "Batch 35: loss 0.5679965632302421\n",
      "Batch 36: loss 0.5685805976390839\n",
      "Batch 37: loss 0.5689323029002628\n",
      "Batch 38: loss 0.5687369437594163\n",
      "Batch 39: loss 0.5685657553183727\n",
      "Batch 40: loss 0.5682294324040413\n",
      "Batch 41: loss 0.5683711142074771\n",
      "Batch 42: loss 0.5681988440808796\n",
      "Batch 43: loss 0.5678267562112143\n",
      "Batch 44: loss 0.5683083534240723\n",
      "Batch 45: loss 0.5684158153004116\n",
      "Batch 46: loss 0.568243893592254\n",
      "Batch 47: loss 0.5686443359293836\n",
      "Batch 48: loss 0.5687621633211771\n",
      "Batch 49: loss 0.568560386190609\n",
      "Batch 50: loss 0.5688677620887757\n",
      "Batch 51: loss 0.5691675169795167\n",
      "Batch 52: loss 0.5689794914080546\n",
      "Batch 53: loss 0.568806602145141\n",
      "Batch 54: loss 0.5687068678714611\n",
      "Batch 55: loss 0.5685019319707697\n",
      "Batch 56: loss 0.5686175142015729\n",
      "Batch 57: loss 0.5685462637951499\n",
      "Batch 58: loss 0.56864958487708\n",
      "Batch 59: loss 0.5685721241821677\n",
      "Batch 60: loss 0.5682197540998459\n",
      "Batch 61: loss 0.5682905042757753\n",
      "Batch 62: loss 0.5680530811509779\n",
      "Batch 63: loss 0.5678111333695669\n",
      "Batch 64: loss 0.5677806157618761\n",
      "Batch 65: loss 0.5676734667557937\n",
      "Batch 66: loss 0.5678018241217642\n",
      "Batch 67: loss 0.5679345202090135\n",
      "Batch 68: loss 0.5678659887874827\n",
      "Batch 69: loss 0.5676309500915416\n",
      "Batch 70: loss 0.5675300879137857\n",
      "Batch 71: loss 0.5674436428177525\n",
      "Batch 72: loss 0.567624161640803\n",
      "Batch 73: loss 0.5675249026246267\n",
      "Batch 74: loss 0.567453869291254\n",
      "Batch 75: loss 0.5677402448654175\n",
      "Batch 76: loss 0.5675882085373527\n",
      "Batch 77: loss 0.5674400453443651\n",
      "Batch 78: loss 0.5675139045104002\n",
      "Batch 79: loss 0.5676121870173684\n",
      "Batch 80: loss 0.5675204582512379\n",
      "Batch 81: loss 0.5673687082749826\n",
      "Batch 82: loss 0.5674178898334503\n",
      "Batch 83: loss 0.5675830589719566\n",
      "Batch 84: loss 0.5672581280980792\n",
      "Batch 85: loss 0.5671183333677404\n",
      "Batch 86: loss 0.5673061068667922\n",
      "Batch 87: loss 0.5672078392971521\n",
      "Batch 88: loss 0.5672344267368317\n",
      "Batch 89: loss 0.5671735606836469\n",
      "Batch 90: loss 0.5672021581066979\n",
      "Batch 91: loss 0.5673493480944372\n",
      "Batch 92: loss 0.5672952429107998\n",
      "Batch 93: loss 0.56724638067266\n",
      "Batch 94: loss 0.5671862611111175\n",
      "Batch 95: loss 0.5673901614389921\n",
      "Batch 96: loss 0.5673663690686226\n",
      "Batch 97: loss 0.567349881241002\n",
      "Batch 98: loss 0.5674633140466652\n",
      "Batch 99: loss 0.5675337386853767\n",
      "Batch 100: loss 0.5673853647708893\n",
      "Batch 101: loss 0.567347766149162\n",
      "Batch 102: loss 0.5672179752705144\n",
      "Batch 103: loss 0.5675559495259257\n",
      "Batch 104: loss 0.5676675324256604\n",
      "Batch 105: loss 0.5679583197548276\n",
      "Batch 106: loss 0.5677885587485332\n",
      "Batch 107: loss 0.5679996565123585\n",
      "Batch 108: loss 0.5680781398658399\n",
      "Batch 109: loss 0.56829714282937\n",
      "Batch 110: loss 0.5682684557004408\n",
      "Batch 111: loss 0.5682227906880079\n",
      "Batch 112: loss 0.5680800266563892\n",
      "Batch 113: loss 0.5681233611782041\n",
      "Batch 114: loss 0.5681047716684509\n",
      "Batch 115: loss 0.5680997148804042\n",
      "Batch 116: loss 0.5680566712700087\n",
      "Batch 117: loss 0.5681253472963969\n",
      "Batch 118: loss 0.5680731802673663\n",
      "Batch 119: loss 0.5681957782817488\n",
      "Batch 120: loss 0.5682878558834393\n",
      "Batch 121: loss 0.5685011980947384\n",
      "Batch 122: loss 0.5686353049317344\n",
      "Batch 123: loss 0.5687352239601011\n",
      "Batch 124: loss 0.5687107323638855\n",
      "Batch 125: loss 0.5687033848762513\n",
      "Batch 126: loss 0.5686446466143169\n",
      "Batch 127: loss 0.5686567913828873\n",
      "Batch 128: loss 0.5686261462979019\n",
      "Batch 129: loss 0.5685213471567908\n",
      "Batch 130: loss 0.5683859164898212\n",
      "Batch 131: loss 0.5682083805098789\n",
      "Batch 132: loss 0.56833098061157\n",
      "Batch 133: loss 0.568384361446352\n",
      "Batch 134: loss 0.5684286267010134\n",
      "Batch 135: loss 0.5683519310421414\n",
      "Batch 136: loss 0.5685480692807365\n",
      "Batch 137: loss 0.5684121403380902\n",
      "Batch 138: loss 0.5683704606000928\n",
      "Batch 139: loss 0.5683467975623316\n",
      "Batch 140: loss 0.5682943774121149\n",
      "Batch 141: loss 0.5683022627593778\n",
      "Batch 142: loss 0.5682535268051524\n",
      "Batch 143: loss 0.5682735989143798\n",
      "Batch 144: loss 0.5682750696109401\n",
      "Batch 145: loss 0.5682977274390335\n",
      "Epoch 6: Training loss 0.5682977274390335\n",
      "Epoch 6: Validation loss 0.7925847887992858 | F1 0.3686351180076599 | AUC 0.6182164527675943 | Acc H-Mean 0.5615926980333646\n",
      "Batch 1: loss 0.5583071112632751\n",
      "Batch 2: loss 0.5616156160831451\n",
      "Batch 3: loss 0.5671136577924093\n",
      "Batch 4: loss 0.567359447479248\n",
      "Batch 5: loss 0.5715965032577515\n",
      "Batch 6: loss 0.5711429516474406\n",
      "Batch 7: loss 0.5700574091502598\n",
      "Batch 8: loss 0.5696593075990677\n",
      "Batch 9: loss 0.5705342557695177\n",
      "Batch 10: loss 0.5712957322597504\n",
      "Batch 11: loss 0.5696952939033508\n",
      "Batch 12: loss 0.5702591687440872\n",
      "Batch 13: loss 0.5713487267494202\n",
      "Batch 14: loss 0.5715965330600739\n",
      "Batch 15: loss 0.5715594967206319\n",
      "Batch 16: loss 0.5722778402268887\n",
      "Batch 17: loss 0.5717562542242163\n",
      "Batch 18: loss 0.5706374413437314\n",
      "Batch 19: loss 0.5703063669957613\n",
      "Batch 20: loss 0.5694225639104843\n",
      "Batch 21: loss 0.5681146071070716\n",
      "Batch 22: loss 0.5675506889820099\n",
      "Batch 23: loss 0.568046201830325\n",
      "Batch 24: loss 0.5681432485580444\n",
      "Batch 25: loss 0.5680517077445983\n",
      "Batch 26: loss 0.5679515393880697\n",
      "Batch 27: loss 0.5673599022406118\n",
      "Batch 28: loss 0.567639389208385\n",
      "Batch 29: loss 0.5681046535228861\n",
      "Batch 30: loss 0.5677152454853058\n",
      "Batch 31: loss 0.5679987380581517\n",
      "Batch 32: loss 0.5676199905574322\n",
      "Batch 33: loss 0.5674705505371094\n",
      "Batch 34: loss 0.5671506033224218\n",
      "Batch 35: loss 0.5676337480545044\n",
      "Batch 36: loss 0.5677633252408769\n",
      "Batch 37: loss 0.5679180428788468\n",
      "Batch 38: loss 0.5675782027997469\n",
      "Batch 39: loss 0.5673299447084085\n",
      "Batch 40: loss 0.5668722271919251\n",
      "Batch 41: loss 0.5660259171230037\n",
      "Batch 42: loss 0.5660016394796825\n",
      "Batch 43: loss 0.5660702988158824\n",
      "Batch 44: loss 0.5664018771865151\n",
      "Batch 45: loss 0.565799249543084\n",
      "Batch 46: loss 0.5654148925905642\n",
      "Batch 47: loss 0.5652705836803356\n",
      "Batch 48: loss 0.56526963164409\n",
      "Batch 49: loss 0.5653338894552115\n",
      "Batch 50: loss 0.5653757917881012\n",
      "Batch 51: loss 0.5651374178774217\n",
      "Batch 52: loss 0.5650680443415275\n",
      "Batch 53: loss 0.564673716167234\n",
      "Batch 54: loss 0.5646881218309756\n",
      "Batch 55: loss 0.5646339438178323\n",
      "Batch 56: loss 0.5644791041101728\n",
      "Batch 57: loss 0.5644654600243819\n",
      "Batch 58: loss 0.5645845141904108\n",
      "Batch 59: loss 0.5649556143809173\n",
      "Batch 60: loss 0.5653008341789245\n",
      "Batch 61: loss 0.5652506488268493\n",
      "Batch 62: loss 0.5651514741682238\n",
      "Batch 63: loss 0.5646397613343739\n",
      "Batch 64: loss 0.5647257184609771\n",
      "Batch 65: loss 0.5646219226030204\n",
      "Batch 66: loss 0.5644118812951174\n",
      "Batch 67: loss 0.5645701582752057\n",
      "Batch 68: loss 0.5643526683835423\n",
      "Batch 69: loss 0.5645629754964856\n",
      "Batch 70: loss 0.5647175107683454\n",
      "Batch 71: loss 0.5647996672442261\n",
      "Batch 72: loss 0.5648369507657157\n",
      "Batch 73: loss 0.5650786292063047\n",
      "Batch 74: loss 0.5648769567141662\n",
      "Batch 75: loss 0.5651221346855163\n",
      "Batch 76: loss 0.5651142401130576\n",
      "Batch 77: loss 0.5651875773033539\n",
      "Batch 78: loss 0.5652547425184494\n",
      "Batch 79: loss 0.5652887330779547\n",
      "Batch 80: loss 0.5649842210114002\n",
      "Batch 81: loss 0.5649481916133269\n",
      "Batch 82: loss 0.5649516182701763\n",
      "Batch 83: loss 0.5648315268826772\n",
      "Batch 84: loss 0.5649532718317849\n",
      "Batch 85: loss 0.564739411017474\n",
      "Batch 86: loss 0.5645358354546303\n",
      "Batch 87: loss 0.5645223444905775\n",
      "Batch 88: loss 0.5646676021543416\n",
      "Batch 89: loss 0.5644856256045653\n",
      "Batch 90: loss 0.564349788427353\n",
      "Batch 91: loss 0.5643856649870401\n",
      "Batch 92: loss 0.5643076546814131\n",
      "Batch 93: loss 0.5643236598660869\n",
      "Batch 94: loss 0.5644080537430783\n",
      "Batch 95: loss 0.564374596194217\n",
      "Batch 96: loss 0.5643054433166981\n",
      "Batch 97: loss 0.5643587382798342\n",
      "Batch 98: loss 0.5643627108359823\n",
      "Batch 99: loss 0.5643398472757051\n",
      "Batch 100: loss 0.5641859483718872\n",
      "Batch 101: loss 0.5641207128468126\n",
      "Batch 102: loss 0.5640111431187275\n",
      "Batch 103: loss 0.563829806823175\n",
      "Batch 104: loss 0.5638026050650157\n",
      "Batch 105: loss 0.5638836060251509\n",
      "Batch 106: loss 0.5639267513212168\n",
      "Batch 107: loss 0.5638901496601996\n",
      "Batch 108: loss 0.5639313658078512\n",
      "Batch 109: loss 0.5638550235590803\n",
      "Batch 110: loss 0.5638323074037378\n",
      "Batch 111: loss 0.5637756473309284\n",
      "Batch 112: loss 0.5639667580170291\n",
      "Batch 113: loss 0.5639603470279052\n",
      "Batch 114: loss 0.5638650713259714\n",
      "Batch 115: loss 0.5640457790830861\n",
      "Batch 116: loss 0.5639784752294935\n",
      "Batch 117: loss 0.5639481590344355\n",
      "Batch 118: loss 0.5638908698397168\n",
      "Batch 119: loss 0.5639135516992136\n",
      "Batch 120: loss 0.563965838154157\n",
      "Batch 121: loss 0.5639712130727846\n",
      "Batch 122: loss 0.5641682861281223\n",
      "Batch 123: loss 0.5642595523741187\n",
      "Batch 124: loss 0.5642087531666602\n",
      "Batch 125: loss 0.5641755137443543\n",
      "Batch 126: loss 0.5641624260516394\n",
      "Batch 127: loss 0.5642177077728933\n",
      "Batch 128: loss 0.5641363831236959\n",
      "Batch 129: loss 0.564156547997349\n",
      "Batch 130: loss 0.5642208768771245\n",
      "Batch 131: loss 0.5641951160576507\n",
      "Batch 132: loss 0.5642258497801694\n",
      "Batch 133: loss 0.5643167459875121\n",
      "Batch 134: loss 0.5643573412254675\n",
      "Batch 135: loss 0.5643336914203785\n",
      "Batch 136: loss 0.5643864799948299\n",
      "Batch 137: loss 0.5645656820631375\n",
      "Batch 138: loss 0.5643876410912776\n",
      "Batch 139: loss 0.564395396829509\n",
      "Batch 140: loss 0.5643126534564155\n",
      "Batch 141: loss 0.5641874975346505\n",
      "Batch 142: loss 0.5642823270508941\n",
      "Batch 143: loss 0.5642312419164431\n",
      "Batch 144: loss 0.56420181484686\n",
      "Batch 145: loss 0.5643541651987632\n",
      "Epoch 7: Training loss 0.5643541651987632\n",
      "Epoch 7: Validation loss 0.8095019578933715 | F1 0.34159398078918457 | AUC 0.6109130798895067 | Acc H-Mean 0.5282772374824677\n",
      "Batch 1: loss 0.5645632743835449\n",
      "Batch 2: loss 0.5619915723800659\n",
      "Batch 3: loss 0.5696820020675659\n",
      "Batch 4: loss 0.5663317739963531\n",
      "Batch 5: loss 0.5659589052200318\n",
      "Batch 6: loss 0.5627308289210001\n",
      "Batch 7: loss 0.5626505953924996\n",
      "Batch 8: loss 0.562247559428215\n",
      "Batch 9: loss 0.5608287122514513\n",
      "Batch 10: loss 0.5602651000022888\n",
      "Batch 11: loss 0.5618908080187711\n",
      "Batch 12: loss 0.5619825025399526\n",
      "Batch 13: loss 0.5632730539028461\n",
      "Batch 14: loss 0.5616394281387329\n",
      "Batch 15: loss 0.5625202973683675\n",
      "Batch 16: loss 0.5631637871265411\n",
      "Batch 17: loss 0.5633044768782223\n",
      "Batch 18: loss 0.562795470158259\n",
      "Batch 19: loss 0.5636066229719865\n",
      "Batch 20: loss 0.5635982245206833\n",
      "Batch 21: loss 0.564507680279868\n",
      "Batch 22: loss 0.5646883818236265\n",
      "Batch 23: loss 0.5650779553081678\n",
      "Batch 24: loss 0.5644510760903358\n",
      "Batch 25: loss 0.5646453547477722\n",
      "Batch 26: loss 0.5645933013695937\n",
      "Batch 27: loss 0.5654600019808169\n",
      "Batch 28: loss 0.5652960155691419\n",
      "Batch 29: loss 0.5648911328151308\n",
      "Batch 30: loss 0.5650895317395528\n",
      "Batch 31: loss 0.5651064226704259\n",
      "Batch 32: loss 0.565055001527071\n",
      "Batch 33: loss 0.5651788079377377\n",
      "Batch 34: loss 0.5656402391545913\n",
      "Batch 35: loss 0.5653031008584158\n",
      "Batch 36: loss 0.5650112215015624\n",
      "Batch 37: loss 0.5653473444887109\n",
      "Batch 38: loss 0.5653392280402937\n",
      "Batch 39: loss 0.5651684113037891\n",
      "Batch 40: loss 0.5654261216521264\n",
      "Batch 41: loss 0.5653204409087577\n",
      "Batch 42: loss 0.5654369578475044\n",
      "Batch 43: loss 0.5658512586771056\n",
      "Batch 44: loss 0.5661230331117456\n",
      "Batch 45: loss 0.566713105307685\n",
      "Batch 46: loss 0.5667431458182957\n",
      "Batch 47: loss 0.5665232348949352\n",
      "Batch 48: loss 0.5672106618682543\n",
      "Batch 49: loss 0.5670001628447552\n",
      "Batch 50: loss 0.5673661637306213\n",
      "Batch 51: loss 0.5674100787031884\n",
      "Batch 52: loss 0.567323638842656\n",
      "Batch 53: loss 0.5676209814143631\n",
      "Batch 54: loss 0.5678020550145043\n",
      "Batch 55: loss 0.5682019613005899\n",
      "Batch 56: loss 0.5684014377849442\n",
      "Batch 57: loss 0.5682483336381745\n",
      "Batch 58: loss 0.5682878956712526\n",
      "Batch 59: loss 0.5682759436510377\n",
      "Batch 60: loss 0.5683314492305119\n",
      "Batch 61: loss 0.5679957133824708\n",
      "Batch 62: loss 0.5679565216264417\n",
      "Batch 63: loss 0.5679476024612548\n",
      "Batch 64: loss 0.5678228521719575\n",
      "Batch 65: loss 0.5678175843678988\n",
      "Batch 66: loss 0.5678121655276327\n",
      "Batch 67: loss 0.5679691843132475\n",
      "Batch 68: loss 0.5677802352344289\n",
      "Batch 69: loss 0.5676042757172516\n",
      "Batch 70: loss 0.567814199413572\n",
      "Batch 71: loss 0.5678170247816704\n",
      "Batch 72: loss 0.5676862481567595\n",
      "Batch 73: loss 0.5678191903519304\n",
      "Batch 74: loss 0.5680351047902494\n",
      "Batch 75: loss 0.5679312483469645\n",
      "Batch 76: loss 0.5679083513586145\n",
      "Batch 77: loss 0.5677736873750563\n",
      "Batch 78: loss 0.5678325838003403\n",
      "Batch 79: loss 0.5676118490062182\n",
      "Batch 80: loss 0.5674535758793354\n",
      "Batch 81: loss 0.5670642315605541\n",
      "Batch 82: loss 0.5669995132015972\n",
      "Batch 83: loss 0.5667749248355268\n",
      "Batch 84: loss 0.5668542378005528\n",
      "Batch 85: loss 0.567014870222877\n",
      "Batch 86: loss 0.5669412578261176\n",
      "Batch 87: loss 0.5669776509548056\n",
      "Batch 88: loss 0.5669322210279378\n",
      "Batch 89: loss 0.5670876348956247\n",
      "Batch 90: loss 0.5670259316762288\n",
      "Batch 91: loss 0.5672016844644652\n",
      "Batch 92: loss 0.5670644375293151\n",
      "Batch 93: loss 0.5671029968928265\n",
      "Batch 94: loss 0.5670286917940099\n",
      "Batch 95: loss 0.5671388946081463\n",
      "Batch 96: loss 0.5670444555580616\n",
      "Batch 97: loss 0.567121113698507\n",
      "Batch 98: loss 0.5671242694465481\n",
      "Batch 99: loss 0.5670439257766261\n",
      "Batch 100: loss 0.5671272432804108\n",
      "Batch 101: loss 0.5671329120598217\n",
      "Batch 102: loss 0.5670859241018108\n",
      "Batch 103: loss 0.5673426030908973\n",
      "Batch 104: loss 0.567258032468649\n",
      "Batch 105: loss 0.5674079367092677\n",
      "Batch 106: loss 0.5674711797597274\n",
      "Batch 107: loss 0.5675183751872767\n",
      "Batch 108: loss 0.5673537723444126\n",
      "Batch 109: loss 0.5673282343313235\n",
      "Batch 110: loss 0.5674194498495622\n",
      "Batch 111: loss 0.5673786036603086\n",
      "Batch 112: loss 0.5674666602696691\n",
      "Batch 113: loss 0.5673656115489724\n",
      "Batch 114: loss 0.567480999126769\n",
      "Batch 115: loss 0.567345460601475\n",
      "Batch 116: loss 0.5675266508398384\n",
      "Batch 117: loss 0.5676288462092733\n",
      "Batch 118: loss 0.5676974846144854\n",
      "Batch 119: loss 0.5675753835870438\n",
      "Batch 120: loss 0.567523310581843\n",
      "Batch 121: loss 0.5676232655186298\n",
      "Batch 122: loss 0.5675247135709544\n",
      "Batch 123: loss 0.5675116195911314\n",
      "Batch 124: loss 0.5676092659273455\n",
      "Batch 125: loss 0.5675198168754577\n",
      "Batch 126: loss 0.5674623075931792\n",
      "Batch 127: loss 0.5674069401786084\n",
      "Batch 128: loss 0.5672237197868526\n",
      "Batch 129: loss 0.5671854578247366\n",
      "Batch 130: loss 0.567314960864874\n",
      "Batch 131: loss 0.5672907083089115\n",
      "Batch 132: loss 0.5672627719062747\n",
      "Batch 133: loss 0.5670880370570305\n",
      "Batch 134: loss 0.5671175518142644\n",
      "Batch 135: loss 0.567049656090913\n",
      "Batch 136: loss 0.5670063539462931\n",
      "Batch 137: loss 0.5670776941480428\n",
      "Batch 138: loss 0.5668835760890574\n",
      "Batch 139: loss 0.5668238530056082\n",
      "Batch 140: loss 0.5668134553091867\n",
      "Batch 141: loss 0.5667515642254065\n",
      "Batch 142: loss 0.5668721866439765\n",
      "Batch 143: loss 0.5668522707232229\n",
      "Batch 144: loss 0.5667321176992522\n",
      "Batch 145: loss 0.566727033486385\n",
      "Epoch 8: Training loss 0.566727033486385\n",
      "Epoch 8: Validation loss 0.852888309955597 | F1 0.31805890798568726 | AUC 0.6113536088033098 | Acc H-Mean 0.4962146800104487\n",
      "Batch 1: loss 0.563285231590271\n",
      "Batch 2: loss 0.5544443726539612\n",
      "Batch 3: loss 0.5576345523198446\n",
      "Batch 4: loss 0.5586793720722198\n",
      "Batch 5: loss 0.5586822271347046\n",
      "Batch 6: loss 0.5583512783050537\n",
      "Batch 7: loss 0.5566471644810268\n",
      "Batch 8: loss 0.5591680556535721\n",
      "Batch 9: loss 0.5603696770138211\n",
      "Batch 10: loss 0.5615763068199158\n",
      "Batch 11: loss 0.5619377114556052\n",
      "Batch 12: loss 0.5626347064971924\n",
      "Batch 13: loss 0.5636152304135836\n",
      "Batch 14: loss 0.5653665363788605\n",
      "Batch 15: loss 0.5650577743848165\n",
      "Batch 16: loss 0.5643387921154499\n",
      "Batch 17: loss 0.5641063942628748\n",
      "Batch 18: loss 0.5653821229934692\n",
      "Batch 19: loss 0.5649204442375585\n",
      "Batch 20: loss 0.564822518825531\n",
      "Batch 21: loss 0.5653755097162156\n",
      "Batch 22: loss 0.5656094415621324\n",
      "Batch 23: loss 0.5665336536324542\n",
      "Batch 24: loss 0.567361185948054\n",
      "Batch 25: loss 0.5673561263084411\n",
      "Batch 26: loss 0.5666306706575247\n",
      "Batch 27: loss 0.5662828772156326\n",
      "Batch 28: loss 0.5658279529639653\n",
      "Batch 29: loss 0.5658379197120667\n",
      "Batch 30: loss 0.565622615814209\n",
      "Batch 31: loss 0.5652407907670544\n",
      "Batch 32: loss 0.5652710981667042\n",
      "Batch 33: loss 0.5652543631466952\n",
      "Batch 34: loss 0.5659355398486642\n",
      "Batch 35: loss 0.5658861756324768\n",
      "Batch 36: loss 0.5658477445443472\n",
      "Batch 37: loss 0.565521613971607\n",
      "Batch 38: loss 0.565439931656185\n",
      "Batch 39: loss 0.5652970121457026\n",
      "Batch 40: loss 0.5653904810547828\n",
      "Batch 41: loss 0.5650875146796064\n",
      "Batch 42: loss 0.5647875609852019\n",
      "Batch 43: loss 0.5641793178957563\n",
      "Batch 44: loss 0.5640776211565192\n",
      "Batch 45: loss 0.5639622886975606\n",
      "Batch 46: loss 0.5637054780255193\n",
      "Batch 47: loss 0.5639256974484058\n",
      "Batch 48: loss 0.5637373961508274\n",
      "Batch 49: loss 0.5638325591476596\n",
      "Batch 50: loss 0.5637913715839385\n",
      "Batch 51: loss 0.5639081854446262\n",
      "Batch 52: loss 0.5637783316465524\n",
      "Batch 53: loss 0.5635640801123853\n",
      "Batch 54: loss 0.5634401970439487\n",
      "Batch 55: loss 0.5635578632354736\n",
      "Batch 56: loss 0.5637698503477233\n",
      "Batch 57: loss 0.5634983202867341\n",
      "Batch 58: loss 0.563491957968679\n",
      "Batch 59: loss 0.5635672050007319\n",
      "Batch 60: loss 0.5636689096689225\n",
      "Batch 61: loss 0.5636368550238062\n",
      "Batch 62: loss 0.5637564264958904\n",
      "Batch 63: loss 0.5636098063181317\n",
      "Batch 64: loss 0.563683969900012\n",
      "Batch 65: loss 0.5636103850144606\n",
      "Batch 66: loss 0.5635526541507605\n",
      "Batch 67: loss 0.5634633249311305\n",
      "Batch 68: loss 0.563501659561606\n",
      "Batch 69: loss 0.5631792355274808\n",
      "Batch 70: loss 0.5634206439767565\n",
      "Batch 71: loss 0.5634769889670359\n",
      "Batch 72: loss 0.5635367433230082\n",
      "Batch 73: loss 0.5634537788286601\n",
      "Batch 74: loss 0.563367486000061\n",
      "Batch 75: loss 0.5636137231191\n",
      "Batch 76: loss 0.5633203026495481\n",
      "Batch 77: loss 0.5633508292111483\n",
      "Batch 78: loss 0.5636322177373446\n",
      "Batch 79: loss 0.5637475252151489\n",
      "Batch 80: loss 0.563563397526741\n",
      "Batch 81: loss 0.5639675752616223\n",
      "Batch 82: loss 0.5639178738361452\n",
      "Batch 83: loss 0.5640321898173137\n",
      "Batch 84: loss 0.5640893166973477\n",
      "Batch 85: loss 0.5642176712260527\n",
      "Batch 86: loss 0.5639977746231611\n",
      "Batch 87: loss 0.563892248033107\n",
      "Batch 88: loss 0.5640217973427339\n",
      "Batch 89: loss 0.5638999115215259\n",
      "Batch 90: loss 0.5638788984881506\n",
      "Batch 91: loss 0.5638453921118936\n",
      "Batch 92: loss 0.5641070280386054\n",
      "Batch 93: loss 0.564140191642187\n",
      "Batch 94: loss 0.5642086863517761\n",
      "Batch 95: loss 0.5644005838193391\n",
      "Batch 96: loss 0.5642022571216027\n",
      "Batch 97: loss 0.5641684464572632\n",
      "Batch 98: loss 0.5642174841189871\n",
      "Batch 99: loss 0.5641618298761772\n",
      "Batch 100: loss 0.5640375983715057\n",
      "Batch 101: loss 0.5639989258039115\n",
      "Batch 102: loss 0.5638788225604039\n",
      "Batch 103: loss 0.5636237581956733\n",
      "Batch 104: loss 0.5636431230948522\n",
      "Batch 105: loss 0.5636142605826968\n",
      "Batch 106: loss 0.5636541607244959\n",
      "Batch 107: loss 0.5637967948601624\n",
      "Batch 108: loss 0.5637563792643724\n",
      "Batch 109: loss 0.5637035506580947\n",
      "Batch 110: loss 0.5636636219241402\n",
      "Batch 111: loss 0.5634861803269601\n",
      "Batch 112: loss 0.5635308144348008\n",
      "Batch 113: loss 0.5637397470727431\n",
      "Batch 114: loss 0.5638865640288905\n",
      "Batch 115: loss 0.5638138527455537\n",
      "Batch 116: loss 0.5635323868743305\n",
      "Batch 117: loss 0.5635810851031898\n",
      "Batch 118: loss 0.5635450739981764\n",
      "Batch 119: loss 0.5635516989130934\n",
      "Batch 120: loss 0.5633933410048485\n",
      "Batch 121: loss 0.5634523625216208\n",
      "Batch 122: loss 0.5634835181666202\n",
      "Batch 123: loss 0.5635046004279842\n",
      "Batch 124: loss 0.5634501792730824\n",
      "Batch 125: loss 0.5633457036018371\n",
      "Batch 126: loss 0.5633371235832335\n",
      "Batch 127: loss 0.5633421840630178\n",
      "Batch 128: loss 0.5631758444942534\n",
      "Batch 129: loss 0.5631213811940925\n",
      "Batch 130: loss 0.5631745219230652\n",
      "Batch 131: loss 0.5630556572484606\n",
      "Batch 132: loss 0.5631346386490446\n",
      "Batch 133: loss 0.5632310980244687\n",
      "Batch 134: loss 0.5631747548259906\n",
      "Batch 135: loss 0.5630487225673817\n",
      "Batch 136: loss 0.5631082522518495\n",
      "Batch 137: loss 0.5630866628493706\n",
      "Batch 138: loss 0.5629973670710688\n",
      "Batch 139: loss 0.5629741476594116\n",
      "Batch 140: loss 0.562999848808561\n",
      "Batch 141: loss 0.5629124882373404\n",
      "Batch 142: loss 0.5628075486337635\n",
      "Batch 143: loss 0.5628840556511512\n",
      "Batch 144: loss 0.5628025763564639\n",
      "Batch 145: loss 0.5627894104822733\n",
      "Epoch 9: Training loss 0.5627894104822733\n",
      "Epoch 9: Validation loss 0.8250082015991211 | F1 0.3353516757488251 | AUC 0.6134808968765565 | Acc H-Mean 0.5158208870049533\n",
      "Batch 1: loss 0.5478538274765015\n",
      "Batch 2: loss 0.5645248889923096\n",
      "Batch 3: loss 0.5708848436673483\n",
      "Batch 4: loss 0.571009024977684\n",
      "Batch 5: loss 0.5683768153190613\n",
      "Batch 6: loss 0.5629483362038931\n",
      "Batch 7: loss 0.5611316221100944\n",
      "Batch 8: loss 0.5616392567753792\n",
      "Batch 9: loss 0.5615198612213135\n",
      "Batch 10: loss 0.5618012547492981\n",
      "Batch 11: loss 0.561940919269215\n",
      "Batch 12: loss 0.5634868144989014\n",
      "Batch 13: loss 0.5636232724556556\n",
      "Batch 14: loss 0.5637259909084865\n",
      "Batch 15: loss 0.564707080523173\n",
      "Batch 16: loss 0.5645459890365601\n",
      "Batch 17: loss 0.5639315598151263\n",
      "Batch 18: loss 0.5645137826601664\n",
      "Batch 19: loss 0.5645980615364877\n",
      "Batch 20: loss 0.5628631472587585\n",
      "Batch 21: loss 0.5629736752737136\n",
      "Batch 22: loss 0.5634149800647389\n",
      "Batch 23: loss 0.5632128508194633\n",
      "Batch 24: loss 0.5633159751693407\n",
      "Batch 25: loss 0.5624148321151733\n",
      "Batch 26: loss 0.5621520143288833\n",
      "Batch 27: loss 0.5627356988412363\n",
      "Batch 28: loss 0.5625706038304737\n",
      "Batch 29: loss 0.5626324641293493\n",
      "Batch 30: loss 0.5624169568220775\n",
      "Batch 31: loss 0.5630934103842704\n",
      "Batch 32: loss 0.5635663811117411\n",
      "Batch 33: loss 0.5636922084923947\n",
      "Batch 34: loss 0.5631350489223704\n",
      "Batch 35: loss 0.5636122226715088\n",
      "Batch 36: loss 0.563490104344156\n",
      "Batch 37: loss 0.5633287784215566\n",
      "Batch 38: loss 0.5632623904629758\n",
      "Batch 39: loss 0.5633851748246413\n",
      "Batch 40: loss 0.5631357103586196\n",
      "Batch 41: loss 0.5631620040753993\n",
      "Batch 42: loss 0.5625175209272475\n",
      "Batch 43: loss 0.5629019875859105\n",
      "Batch 44: loss 0.5630439912730997\n",
      "Batch 45: loss 0.5636009057362874\n",
      "Batch 46: loss 0.5639214878496916\n",
      "Batch 47: loss 0.5638290390055231\n",
      "Batch 48: loss 0.564286674062411\n",
      "Batch 49: loss 0.5643891169100391\n",
      "Batch 50: loss 0.5643044042587281\n",
      "Batch 51: loss 0.564478335427303\n",
      "Batch 52: loss 0.5646414561913564\n",
      "Batch 53: loss 0.5647079843395161\n",
      "Batch 54: loss 0.5649424405009659\n",
      "Batch 55: loss 0.5647519079121677\n",
      "Batch 56: loss 0.564914057297366\n",
      "Batch 57: loss 0.565294553313339\n",
      "Batch 58: loss 0.5651857205506029\n",
      "Batch 59: loss 0.5649006285909879\n",
      "Batch 60: loss 0.5653916796048483\n",
      "Batch 61: loss 0.5652064954648253\n",
      "Batch 62: loss 0.5648637519728753\n",
      "Batch 63: loss 0.5648947093221877\n",
      "Batch 64: loss 0.5650105187669396\n",
      "Batch 65: loss 0.5650360740148104\n",
      "Batch 66: loss 0.5651234848932787\n",
      "Batch 67: loss 0.564851499315518\n",
      "Batch 68: loss 0.5648405867464402\n",
      "Batch 69: loss 0.5648128969081934\n",
      "Batch 70: loss 0.5648782900401524\n",
      "Batch 71: loss 0.564850607388456\n",
      "Batch 72: loss 0.5649570988284217\n",
      "Batch 73: loss 0.5648148957997152\n",
      "Batch 74: loss 0.5649017691612244\n",
      "Batch 75: loss 0.5649912269910177\n",
      "Batch 76: loss 0.5650033472399962\n",
      "Batch 77: loss 0.5647371488732177\n",
      "Batch 78: loss 0.564702740846536\n",
      "Batch 79: loss 0.564759170707268\n",
      "Batch 80: loss 0.5646536342799664\n",
      "Batch 81: loss 0.5647093697830483\n",
      "Batch 82: loss 0.564611397138456\n",
      "Batch 83: loss 0.5646724442401564\n",
      "Batch 84: loss 0.5648689099720546\n",
      "Batch 85: loss 0.564803446040434\n",
      "Batch 86: loss 0.5648117058498915\n",
      "Batch 87: loss 0.5649028132701742\n",
      "Batch 88: loss 0.5650077563795176\n",
      "Batch 89: loss 0.5649652708782239\n",
      "Batch 90: loss 0.564945662021637\n",
      "Batch 91: loss 0.5649672661508832\n",
      "Batch 92: loss 0.5647934435502343\n",
      "Batch 93: loss 0.5648926675960582\n",
      "Batch 94: loss 0.5650420049403576\n",
      "Batch 95: loss 0.5650781964000903\n",
      "Batch 96: loss 0.5650293808430433\n",
      "Batch 97: loss 0.5648091349405112\n",
      "Batch 98: loss 0.5647222357136863\n",
      "Batch 99: loss 0.564901701729707\n",
      "Batch 100: loss 0.5649178749322892\n",
      "Batch 101: loss 0.5649052297714914\n",
      "Batch 102: loss 0.5651502761186338\n",
      "Batch 103: loss 0.5651764024808569\n",
      "Batch 104: loss 0.5651586961287719\n",
      "Batch 105: loss 0.5650221222922915\n",
      "Batch 106: loss 0.5651225964978056\n",
      "Batch 107: loss 0.5650367786951154\n",
      "Batch 108: loss 0.5649516140973126\n",
      "Batch 109: loss 0.5650271510859148\n",
      "Batch 110: loss 0.5649946424094113\n",
      "Batch 111: loss 0.5650138033402933\n",
      "Batch 112: loss 0.5649227356272084\n",
      "Batch 113: loss 0.564848131838098\n",
      "Batch 114: loss 0.5649663837332475\n",
      "Batch 115: loss 0.5649566951005355\n",
      "Batch 116: loss 0.5648398466151336\n",
      "Batch 117: loss 0.5647298337047936\n",
      "Batch 118: loss 0.5646784452058501\n",
      "Batch 119: loss 0.5645919742704439\n",
      "Batch 120: loss 0.564526592195034\n",
      "Batch 121: loss 0.5644470758674559\n",
      "Batch 122: loss 0.5644274612919229\n",
      "Batch 123: loss 0.5642824332888533\n",
      "Batch 124: loss 0.5642833897183018\n",
      "Batch 125: loss 0.5642108798027039\n",
      "Batch 126: loss 0.5642659829722511\n",
      "Batch 127: loss 0.5641814043202739\n",
      "Batch 128: loss 0.5639431206509471\n",
      "Batch 129: loss 0.5638433667116387\n",
      "Batch 130: loss 0.5639389028915992\n",
      "Batch 131: loss 0.5641450517960177\n",
      "Batch 132: loss 0.5641088192210053\n",
      "Batch 133: loss 0.5639103503155529\n",
      "Batch 134: loss 0.563890533215964\n",
      "Batch 135: loss 0.563707317687847\n",
      "Batch 136: loss 0.563756974742693\n",
      "Batch 137: loss 0.5638187797400203\n",
      "Batch 138: loss 0.5639951362989951\n",
      "Batch 139: loss 0.5640021352459201\n",
      "Batch 140: loss 0.5640375873872212\n",
      "Batch 141: loss 0.5640881061553955\n",
      "Batch 142: loss 0.5638957204113544\n",
      "Batch 143: loss 0.5638999084492663\n",
      "Batch 144: loss 0.5638338016966978\n",
      "Batch 145: loss 0.5637679309522824\n",
      "Epoch 10: Training loss 0.5637679309522824\n",
      "Epoch 10: Validation loss 0.8146942973136901 | F1 0.3232361674308777 | AUC 0.5907090239888075 | Acc H-Mean 0.5085974066032377\n",
      "Best epoch:  6\n"
     ]
    }
   ],
   "source": [
    "mtrainer.run_train(10, support_loader, query_loader, lr=1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mtrainer.best_model, 'models/backbone/pretrained/vindr2/trained-backbone-balacc.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs6240",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3af20c5b7ad2810593c71dd4ba5b7d473da7f58b181d32c1c7ac42846aa4b248"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
